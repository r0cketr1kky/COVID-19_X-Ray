{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "smooth_scorecam.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOCpMn3MUCtG8udypceOLpp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/r0cketr1kky/COVID-19_X-Ray/blob/master/smooth_score1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlmDweVD0Tw-",
        "colab_type": "code",
        "outputId": "04c81f75-fc81-41d3-9bb8-873215d7187e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSiiyfeo1521",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import copy\n",
        "import numpy as np\n",
        "from PIL import Image, ImageFilter\n",
        "import matplotlib.cm as mpl_color_map\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torchvision import models\n",
        "\n",
        "\n",
        "def convert_to_grayscale(im_as_arr):\n",
        "    \"\"\"\n",
        "        Converts 3d image to grayscale\n",
        "    Args:\n",
        "        im_as_arr (numpy arr): RGB image with shape (D,W,H)\n",
        "    returns:\n",
        "        grayscale_im (numpy_arr): Grayscale image with shape (1,W,D)\n",
        "    \"\"\"\n",
        "    grayscale_im = np.sum(np.abs(im_as_arr), axis=0)\n",
        "    im_max = np.percentile(grayscale_im, 99)\n",
        "    im_min = np.min(grayscale_im)\n",
        "    grayscale_im = (np.clip((grayscale_im - im_min) / (im_max - im_min), 0, 1))\n",
        "    grayscale_im = np.expand_dims(grayscale_im, axis=0)\n",
        "    return grayscale_im\n",
        "\n",
        "\n",
        "def save_gradient_images(gradient, file_name):\n",
        "    \"\"\"\n",
        "        Exports the original gradient image\n",
        "    Args:\n",
        "        gradient (np arr): Numpy array of the gradient with shape (3, 224, 224)\n",
        "        file_name (str): File name to be exported\n",
        "    \"\"\"\n",
        "    if not os.path.exists('../results'):\n",
        "        os.makedirs('../results')\n",
        "    # Normalize\n",
        "    gradient = gradient - gradient.min()\n",
        "    gradient /= gradient.max()\n",
        "    # Save image\n",
        "    path_to_file = os.path.join('../results', file_name + '.jpg')\n",
        "    save_image(gradient, path_to_file)\n",
        "\n",
        "\n",
        "def save_class_activation_images(org_img, activation_map):\n",
        "    \"\"\"\n",
        "        Saves cam activation map and activation map on the original image\n",
        "    Args:\n",
        "        org_img (PIL img): Original image\n",
        "        activation_map (numpy arr): Activation map (grayscale) 0-255\n",
        "    \"\"\"\n",
        "    if not os.path.exists('../results'):\n",
        "        os.makedirs('../results')\n",
        "    # Grayscale activation map\n",
        "    heatmap, heatmap_on_image = apply_colormap_on_image(org_img, activation_map, 'hsv')\n",
        "    # Save colored heatmap\n",
        "    path_to_file = os.path.join('../results', '/_Cam_Heatmap.png')\n",
        "    save_image(heatmap, path_to_file)\n",
        "    # Save heatmap on iamge\n",
        "    path_to_file = os.path.join('../results', '/_Cam_On_Image.png')\n",
        "    save_image(heatmap_on_image, path_to_file)\n",
        "    # SAve grayscale heatmap\n",
        "    path_to_file = os.path.join('../results', '/_Cam_Grayscale.png')\n",
        "    save_image(activation_map, path_to_file)\n",
        "\n",
        "\n",
        "def apply_colormap_on_image(org_im, activation, colormap_name):\n",
        "    \"\"\"\n",
        "        Apply heatmap on image\n",
        "    Args:\n",
        "        org_img (PIL img): Original image\n",
        "        activation_map (numpy arr): Activation map (grayscale) 0-255\n",
        "        colormap_name (str): Name of the colormap\n",
        "    \"\"\"\n",
        "    # Get colormap\n",
        "    color_map = mpl_color_map.get_cmap(colormap_name)\n",
        "    no_trans_heatmap = color_map(activation)\n",
        "    # Change alpha channel in colormap to make sure original image is displayed\n",
        "    heatmap = copy.copy(no_trans_heatmap)\n",
        "    heatmap[:, :, 3] = 0.4\n",
        "    heatmap = Image.fromarray((heatmap*255).astype(np.uint8))\n",
        "    no_trans_heatmap = Image.fromarray((no_trans_heatmap*255).astype(np.uint8))\n",
        "\n",
        "    # Apply heatmap on iamge\n",
        "    heatmap_on_image = Image.new(\"RGBA\", org_im.size)\n",
        "    heatmap_on_image = Image.alpha_composite(heatmap_on_image, org_im.convert('RGBA'))\n",
        "    heatmap_on_image = Image.alpha_composite(heatmap_on_image, heatmap)\n",
        "    return no_trans_heatmap, heatmap_on_image\n",
        "\n",
        "\n",
        "def format_np_output(np_arr):\n",
        "    \"\"\"\n",
        "        This is a (kind of) bandaid fix to streamline saving procedure.\n",
        "        It converts all the outputs to the same format which is 3xWxH\n",
        "        with using sucecssive if clauses.\n",
        "    Args:\n",
        "        im_as_arr (Numpy array): Matrix of shape 1xWxH or WxH or 3xWxH\n",
        "    \"\"\"\n",
        "    # Phase/Case 1: The np arr only has 2 dimensions\n",
        "    # Result: Add a dimension at the beginning\n",
        "    if len(np_arr.shape) == 2:\n",
        "        np_arr = np.expand_dims(np_arr, axis=0)\n",
        "    # Phase/Case 2: Np arr has only 1 channel (assuming first dim is channel)\n",
        "    # Result: Repeat first channel and convert 1xWxH to 3xWxH\n",
        "    if np_arr.shape[0] == 1:\n",
        "        np_arr = np.repeat(np_arr, 3, axis=0)\n",
        "    # Phase/Case 3: Np arr is of shape 3xWxH\n",
        "    # Result: Convert it to WxHx3 in order to make it saveable by PIL\n",
        "    if np_arr.shape[0] == 3:\n",
        "        np_arr = np_arr.transpose(1, 2, 0)\n",
        "    # Phase/Case 4: NP arr is normalized between 0-1\n",
        "    # Result: Multiply with 255 and change type to make it saveable by PIL\n",
        "    if np.max(np_arr) <= 1:\n",
        "        np_arr = (np_arr*255).astype(np.uint8)\n",
        "    return np_arr\n",
        "\n",
        "\n",
        "def save_image(im, path):\n",
        "    \"\"\"\n",
        "        Saves a numpy matrix or PIL image as an image\n",
        "    Args:\n",
        "        im_as_arr (Numpy array): Matrix of shape DxWxH\n",
        "        path (str): Path to the image\n",
        "    \"\"\"\n",
        "    if isinstance(im, (np.ndarray, np.generic)):\n",
        "        im = format_np_output(im)\n",
        "        im = Image.fromarray(im)\n",
        "    im.save(path)\n",
        "\n",
        "\n",
        "def preprocess_image(pil_im, resize_im=True):\n",
        "    \"\"\"\n",
        "        Processes image for CNNs\n",
        "    Args:\n",
        "        PIL_img (PIL_img): PIL Image or numpy array to process\n",
        "        resize_im (bool): Resize to 224 or not\n",
        "    returns:\n",
        "        im_as_var (torch variable): Variable that contains processed float tensor\n",
        "    \"\"\"\n",
        "    # mean and std list for channels (Imagenet)\n",
        "    mean = [0.485, 0.456, 0.406]\n",
        "    std = [0.229, 0.224, 0.225]\n",
        "\n",
        "    #ensure or transform incoming image to PIL image\n",
        "    if type(pil_im) != Image.Image:\n",
        "        try:\n",
        "            pil_im = Image.fromarray(pil_im)\n",
        "        except Exception as e:\n",
        "            print(\"could not transform PIL_img to a PIL Image object. Please check input.\")\n",
        "\n",
        "    # Resize image\n",
        "    if resize_im:\n",
        "        pil_im = pil_im.resize((224, 224), Image.ANTIALIAS)\n",
        "\n",
        "    im_as_arr = np.float32(pil_im)\n",
        "    im_as_arr = im_as_arr.transpose(2, 0, 1)  # Convert array to D,W,H\n",
        "    # Normalize the channels\n",
        "    for channel, _ in enumerate(im_as_arr):\n",
        "        im_as_arr[channel] /= 255\n",
        "        im_as_arr[channel] -= mean[channel]\n",
        "        im_as_arr[channel] /= std[channel]\n",
        "    # Convert to float tensor\n",
        "    im_as_ten = torch.from_numpy(im_as_arr).float()\n",
        "    # Add one more channel to the beginning. Tensor shape = 1,3,224,224\n",
        "    im_as_ten.unsqueeze_(0)\n",
        "    # Convert to Pytorch variable\n",
        "    im_as_var = Variable(im_as_ten, requires_grad=True)\n",
        "    return im_as_var\n",
        "\n",
        "\n",
        "def recreate_image(im_as_var):\n",
        "    \"\"\"\n",
        "        Recreates images from a torch variable, sort of reverse preprocessing\n",
        "    Args:\n",
        "        im_as_var (torch variable): Image to recreate\n",
        "    returns:\n",
        "        recreated_im (numpy arr): Recreated image in array\n",
        "    \"\"\"\n",
        "    reverse_mean = [-0.485, -0.456, -0.406]\n",
        "    reverse_std = [1/0.229, 1/0.224, 1/0.225]\n",
        "    recreated_im = copy.copy(im_as_var.data.numpy()[0])\n",
        "    for c in range(3):\n",
        "        recreated_im[c] /= reverse_std[c]\n",
        "        recreated_im[c] -= reverse_mean[c]\n",
        "    recreated_im[recreated_im > 1] = 1\n",
        "    recreated_im[recreated_im < 0] = 0\n",
        "    recreated_im = np.round(recreated_im * 255)\n",
        "\n",
        "    recreated_im = np.uint8(recreated_im).transpose(1, 2, 0)\n",
        "    return recreated_im\n",
        "\n",
        "\n",
        "def get_positive_negative_saliency(gradient):\n",
        "    \"\"\"\n",
        "        Generates positive and negative saliency maps based on the gradient\n",
        "    Args:\n",
        "        gradient (numpy arr): Gradient of the operation to visualize\n",
        "    returns:\n",
        "        pos_saliency ( )\n",
        "    \"\"\"\n",
        "    pos_saliency = (np.maximum(0, gradient) / gradient.max())\n",
        "    neg_saliency = (np.maximum(0, -gradient) / -gradient.min())\n",
        "    return pos_saliency, neg_saliency\n",
        "\n",
        "\n",
        "def get_example_params(example_index):\n",
        "    \"\"\"\n",
        "        Gets used variables for almost all visualizations, like the image, model etc.\n",
        "    Args:\n",
        "        example_index (int): Image id to use from examples\n",
        "    returns:\n",
        "        original_image (numpy arr): Original image read from the file\n",
        "        prep_img (numpy_arr): Processed image\n",
        "        target_class (int): Target class for the image\n",
        "        pretrained_model(Pytorch model): Model to use for the operations\n",
        "    \"\"\"\n",
        "    # Pick one of the examples\n",
        "    \n",
        "    # Read image\n",
        "    original_image = Image.open(img_path).convert('RGB')\n",
        "    # Process image\n",
        "    prep_img = preprocess_image(original_image)\n",
        "    # Define model\n",
        "    #pretrained_model = model\n",
        "    return (original_image,\n",
        "            prep_img,\n",
        "            target_class)\n",
        "    \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class BaseProp(object):\n",
        "    \"\"\"\n",
        "        Base class for backpropagation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model):\n",
        "        \"\"\"Init\n",
        "        # Arguments:\n",
        "            model: torchvision.models. A pretrained model.\n",
        "            handle: list. Handle list that register a hook function.\n",
        "            relu_outputs: list. Forward output after relu.\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.handle = []\n",
        "        self.relu_outputs = []\n",
        "\n",
        "    def _register_conv_hook(self):\n",
        "\n",
        "        \"\"\"\n",
        "            Register hook function to save gradient w.r.t input image.\n",
        "        \"\"\"\n",
        "\n",
        "        def _record_gradients(module, grad_in, grad_out):\n",
        "                self.gradients = grad_in[0]\n",
        "\n",
        "        for _, module in self.model.named_modules():\n",
        "            if isinstance(module, nn.modules.conv.Conv2d) and module.in_channels == 3:\n",
        "                backward_handle = module.register_backward_hook(_record_gradients)\n",
        "                self.handle.append(backward_handle)\n",
        "\n",
        "    def _register_relu_hooks(self):\n",
        "\n",
        "        \"\"\"\n",
        "            Register hook function to save forward and backward relu result.\n",
        "        \"\"\"\n",
        "\n",
        "        # Save forward propagation output of the ReLU layer\n",
        "        def _record_output(module, input_, output):\n",
        "            self.relu_outputs.append(output)\n",
        "\n",
        "        def _clip_gradients(module, grad_in, grad_out):\n",
        "            # keep positive forward propagation output\n",
        "            relu_output = self.relu_outputs.pop()\n",
        "            relu_output[relu_output > 0] = 1\n",
        "\n",
        "            # keep positive backward propagation gradient\n",
        "            positive_grad_out = torch.clamp(grad_out[0], min=0.0)\n",
        "\n",
        "            # generate modified guided gradient\n",
        "            modified_grad_out = positive_grad_out * relu_output\n",
        "\n",
        "            return (modified_grad_out, )\n",
        "\n",
        "        for _, module in self.model.named_modules():\n",
        "            if isinstance(module, nn.ReLU):\n",
        "                forward_handle = module.register_forward_hook(_record_output)\n",
        "                backward_handle = module.register_backward_hook(_clip_gradients)\n",
        "                self.handle.append(forward_handle)\n",
        "                self.handle.append(backward_handle)\n",
        "\n",
        "class VanillaBackprop():\n",
        "    \"\"\"\n",
        "        Produces gradients generated with vanilla back propagation from the image\n",
        "    \"\"\"\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.gradients = None\n",
        "        # Put model in evaluation mode\n",
        "        self.model.eval()\n",
        "        # Hook the first layer to get the gradient\n",
        "        self.hook_layers()\n",
        "\n",
        "    def hook_layers(self):\n",
        "        def hook_function(module, grad_in, grad_out):\n",
        "            self.gradients = grad_in[0]\n",
        "\n",
        "        # Register hook to the first layer\n",
        "        first_layer = list(self.model.features._modules.items())[0][1]\n",
        "        first_layer.register_backward_hook(hook_function)\n",
        "\n",
        "    def generate_gradients(self, input_image, target_class):\n",
        "        # Forward\n",
        "        model_output = self.model(input_image)\n",
        "        # Zero grads\n",
        "        self.model.zero_grad()\n",
        "        # Target for backprop\n",
        "        one_hot_output = torch.FloatTensor(1, model_output.size()[-1]).zero_()\n",
        "        one_hot_output[0][target_class] = 1\n",
        "        # Backward pass\n",
        "        model_output.backward(gradient=one_hot_output)\n",
        "        # Convert Pytorch variable to numpy array\n",
        "        # [0] to get rid of the first channel (1,3,224,224)\n",
        "        gradients_as_arr = self.gradients.data.numpy()[0]\n",
        "        return gradients_as_arr\n",
        "\n",
        "class Backprop(BaseProp):\n",
        "\n",
        "    \"\"\" Generates vanilla or guided backprop gradients of a target class output w.r.t. an input image.\n",
        "        # Arguments:\n",
        "            model: torchvision.models. A pretrained model.\n",
        "            guided: bool. If True, perform guided backpropagation. Defaults to False.\n",
        "        # Return:\n",
        "            Backprop Class.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, guided=False):\n",
        "        super().__init__(model)\n",
        "        self.model.eval()\n",
        "        self.guided = guided\n",
        "        self.gradients = None\n",
        "        self._register_conv_hook()\n",
        "\n",
        "    def calculate_gradients(self,\n",
        "                            input_,\n",
        "                            target_class=None,\n",
        "                            take_max=False,\n",
        "                            use_gpu=False):\n",
        "\n",
        "        \"\"\" Calculate gradient.\n",
        "            # Arguments\n",
        "                input_: torch.Tensor. Preprocessed image with shape (1, C, H, W).\n",
        "                target_class: int. Index of target class. Default to None and use the prediction result as target class.\n",
        "                take_max: bool. Take the maximum across colour channels. Defaults to False.\n",
        "                use_gpu. bool. Use GPU or not. Defaults to False.\n",
        "            # Return:\n",
        "                Gradient (torch.Tensor) with shape (C, H, W). If take max is True, with shape (1, H, W).\n",
        "        \"\"\"\n",
        "\n",
        "        if self.guided:\n",
        "            self._register_relu_hooks()\n",
        "\n",
        "        # Create a empty tensor to save gradients\n",
        "        self.gradients = torch.zeros(input_.shape)\n",
        "\n",
        "        output = self.model(input_)\n",
        "\n",
        "        self.model.zero_grad()\n",
        "\n",
        "        if output.shape == torch.Size([1]):\n",
        "            target = None\n",
        "        else:\n",
        "            pred_class = output.argmax().item()\n",
        "\n",
        "            # Create a Tensor with zero elements, set the element at pred class index to be 1\n",
        "            target = torch.zeros(output.shape)\n",
        "\n",
        "            # If target class is None, calculate gradient of predicted class.\n",
        "            if target_class is None:\n",
        "                target[0][pred_class] = 1\n",
        "            else:\n",
        "                target[0][target_class] = 1\n",
        "\n",
        "            \n",
        "        # Calculate gradients w.r.t. input image\n",
        "        output.backward(gradient=target)\n",
        "\n",
        "        gradients = self.gradients.detach().cpu()[0]\n",
        "\n",
        "        if take_max:\n",
        "            gradients = gradients.max(dim=0, keepdim=True)[0]\n",
        "\n",
        "        for module in self.handle:\n",
        "            module.remove()\n",
        "        gradients = gradients.numpy()\n",
        "        return gradients\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "# from guided_backprop import GuidedBackprop  # To use with guided backprop\n",
        "\n",
        "\n",
        "def generate_smooth_grad(Backprop, prep_img, target_class, param_n, param_sigma_multiplier):\n",
        "    \"\"\"\n",
        "        Generates smooth gradients of given Backprop type. You can use this with both vanilla\n",
        "        and guided backprop\n",
        "    Args:\n",
        "        Backprop (class): Backprop type\n",
        "        prep_img (torch Variable): preprocessed image\n",
        "        target_class (int): target class of imagenet\n",
        "        param_n (int): Amount of images used to smooth gradient\n",
        "        param_sigma_multiplier (int): Sigma multiplier when calculating std of noise\n",
        "    \"\"\"\n",
        "    # Generate an empty image/matrix\n",
        "    smooth_grad = np.zeros(prep_img.size()[1:])\n",
        "\n",
        "    mean = 0\n",
        "    sigma = param_sigma_multiplier / (torch.max(prep_img) - torch.min(prep_img)).item()\n",
        "    \n",
        "    for x in range(param_n):\n",
        "        # Generate noise\n",
        "        noise = Variable(prep_img.data.new(prep_img.size()).normal_(mean, sigma**2))\n",
        "        # Add noise to the image\n",
        "        noisy_img = prep_img + noise\n",
        "        # Calculate gradients\n",
        "        vanilla_grads = Backprop.calculate_gradients(noisy_img, target_class)\n",
        "        # Add gradients to smooth_grad\n",
        "        smooth_grad = smooth_grad + vanilla_grads\n",
        "    # Average it out\n",
        "    smooth_grad = smooth_grad / param_n\n",
        "    return smooth_grad\n",
        "\n",
        "def process_image(image_path):\n",
        "    \"\"\"Process an image path into a PyTorch tensor\"\"\"\n",
        "\n",
        "    image = Image.open(image_path)\n",
        "    # Resize\n",
        "    img = image.resize((256, 256))\n",
        "\n",
        "    # Center crop\n",
        "    width = 256\n",
        "    height = 256\n",
        "    new_width = 224\n",
        "    new_height = 224\n",
        "\n",
        "    left = (width - new_width) / 2\n",
        "    top = (height - new_height) / 2\n",
        "    right = (width + new_width) / 2\n",
        "    bottom = (height + new_height) / 2\n",
        "    img = img.crop((left, top, right, bottom))\n",
        "\n",
        "    # Convert to numpy, transpose color dimension and normalize\n",
        "    img = np.array(img).transpose((2, 0, 1)) / 256\n",
        "\n",
        "    # Standardization\n",
        "    means = np.array([0.485, 0.456, 0.406]).reshape((3, 1, 1))\n",
        "    stds = np.array([0.229, 0.224, 0.225]).reshape((3, 1, 1))\n",
        "\n",
        "    img = img - means\n",
        "    img = img / stds\n",
        "\n",
        "    img_tensor = torch.Tensor(img)\n",
        "\n",
        "    return img_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UtUGPvt0ceD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python\n",
        "\"\"\"flashtorch.utils\n",
        "\n",
        "This module provides utility functions for image handling and tensor\n",
        "transformation.\n",
        "\n",
        "\"\"\"\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as F\n",
        "import torchvision.models\n",
        "\n",
        "def load_image(image_path):\n",
        "    \"\"\"Loads image as a PIL RGB image.\n",
        "\n",
        "        Args:\n",
        "            - **image_path (str) - **: A path to the image\n",
        "\n",
        "        Returns:\n",
        "            An instance of PIL.Image.Image in RGB\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    return Image.open(image_path).convert('RGB')\n",
        "\n",
        "\n",
        "def apply_transforms(image, size=224):\n",
        "    \"\"\"Transforms a PIL image to torch.Tensor.\n",
        "\n",
        "    Applies a series of tranformations on PIL image including a conversion\n",
        "    to a tensor. The returned tensor has a shape of :math:`(N, C, H, W)` and\n",
        "    is ready to be used as an input to neural networks.\n",
        "\n",
        "    First the image is resized to 256, then cropped to 224. The `means` and\n",
        "    `stds` for normalisation are taken from numbers used in ImageNet, as\n",
        "    currently developing the package for visualizing pre-trained models.\n",
        "\n",
        "    The plan is to to expand this to handle custom size/mean/std.\n",
        "\n",
        "    Args:\n",
        "        image (PIL.Image.Image or numpy array)\n",
        "        size (int, optional, default=224): Desired size (width/height) of the\n",
        "            output tensor\n",
        "\n",
        "    Shape:\n",
        "        Input: :math:`(C, H, W)` for numpy array\n",
        "        Output: :math:`(N, C, H, W)`\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor (torch.float32): Transformed image tensor\n",
        "\n",
        "    Note:\n",
        "        Symbols used to describe dimensions:\n",
        "            - N: number of images in a batch\n",
        "            - C: number of channels\n",
        "            - H: height of the image\n",
        "            - W: width of the image\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if not isinstance(image, Image.Image):\n",
        "        image = F.to_pil_image(image)\n",
        "\n",
        "    means = [0.485, 0.456, 0.406]\n",
        "    stds = [0.229, 0.224, 0.225]\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(size),\n",
        "        transforms.CenterCrop(size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(means, stds)\n",
        "    ])\n",
        "\n",
        "    tensor = transform(image).unsqueeze(0)\n",
        "\n",
        "    tensor.requires_grad = True\n",
        "\n",
        "    return tensor\n",
        "\n",
        "def apply_transforms_v0(image, size=224):\n",
        "    \"\"\"Transforms a PIL image to torch.Tensor.\n",
        "\n",
        "    Applies a series of tranformations on PIL image including a conversion\n",
        "    to a tensor. The returned tensor has a shape of :math:`(N, C, H, W)` and\n",
        "    is ready to be used as an input to neural networks.\n",
        "\n",
        "    First the image is resized to 256, then cropped to 224. The `means` and\n",
        "    `stds` for normalisation are taken from numbers used in ImageNet, as\n",
        "    currently developing the package for visualizing pre-trained models.\n",
        "\n",
        "    The plan is to to expand this to handle custom size/mean/std.\n",
        "\n",
        "    Args:\n",
        "        image (PIL.Image.Image or numpy array)\n",
        "        size (int, optional, default=224): Desired size (width/height) of the\n",
        "            output tensor\n",
        "\n",
        "    Shape:\n",
        "        Input: :math:`(C, H, W)` for numpy array\n",
        "        Output: :math:`(N, C, H, W)`\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor (torch.float32): Transformed image tensor\n",
        "\n",
        "    Note:\n",
        "        Symbols used to describe dimensions:\n",
        "            - N: number of images in a batch\n",
        "            - C: number of channels\n",
        "            - H: height of the image\n",
        "            - W: width of the image\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if not isinstance(image, Image.Image):\n",
        "        image = F.to_pil_image(image)\n",
        "\n",
        "    means = [0.485, 0.456, 0.406]\n",
        "    stds = [0.229, 0.224, 0.225]\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(size),\n",
        "        transforms.CenterCrop(size),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    tensor = transform(image).unsqueeze(0)\n",
        "\n",
        "    tensor.requires_grad = True\n",
        "\n",
        "    return tensor\n",
        "\n",
        "\n",
        "def denormalize(tensor):\n",
        "    \"\"\"Reverses the normalisation on a tensor.\n",
        "\n",
        "    Performs a reverse operation on a tensor, so the pixel value range is\n",
        "    between 0 and 1. Useful for when plotting a tensor into an image.\n",
        "\n",
        "    Normalisation: (image - mean) / std\n",
        "    Denormalisation: image * std + mean\n",
        "\n",
        "    Args:\n",
        "        tensor (torch.Tensor, dtype=torch.float32): Normalized image tensor\n",
        "\n",
        "    Shape:\n",
        "        Input: :math:`(N, C, H, W)`\n",
        "        Output: :math:`(N, C, H, W)` (same shape as input)\n",
        "\n",
        "    Return:\n",
        "        torch.Tensor (torch.float32): Demornalised image tensor with pixel\n",
        "            values between [0, 1]\n",
        "\n",
        "    Note:\n",
        "        Symbols used to describe dimensions:\n",
        "            - N: number of images in a batch\n",
        "            - C: number of channels\n",
        "            - H: height of the image\n",
        "            - W: width of the image\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    means = [0.485, 0.456, 0.406]\n",
        "    stds = [0.229, 0.224, 0.225]\n",
        "\n",
        "    denormalized = tensor.clone()\n",
        "\n",
        "    for channel, mean, std in zip(denormalized[0], means, stds):\n",
        "        channel.mul_(std).add_(mean)\n",
        "\n",
        "    return denormalized\n",
        "\n",
        "\n",
        "def standardize_and_clip(tensor, min_value=0.0, max_value=1.0):\n",
        "    \"\"\"Standardizes and clips input tensor.\n",
        "\n",
        "    Standardize the input tensor (mean = 0.0, std = 1.0), ensures std is 0.1\n",
        "    and clips it to values between min/max (default: 0.0/1.0).\n",
        "\n",
        "    Args:\n",
        "        tensor (torch.Tensor):\n",
        "        min_value (float, optional, default=0.0)\n",
        "        max_value (float, optional, default=1.0)\n",
        "\n",
        "    Shape:\n",
        "        Input: :math:`(C, H, W)`\n",
        "        Output: Same as the input\n",
        "\n",
        "    Return:\n",
        "        torch.Tensor (torch.float32): Normalised tensor with values between\n",
        "            [min_value, max_value]\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    tensor = tensor.detach().cpu()\n",
        "\n",
        "    mean = tensor.mean()\n",
        "    std = tensor.std()\n",
        "    if std == 0:\n",
        "        std += 1e-7\n",
        "\n",
        "    standardized = tensor.sub(mean).div(std).mul(0.1)\n",
        "    clipped = standardized.add(0.5).clamp(min_value, max_value)\n",
        "\n",
        "    return clipped\n",
        "\n",
        "\n",
        "def format_for_plotting(tensor):\n",
        "    \"\"\"Formats the shape of tensor for plotting.\n",
        "\n",
        "    Tensors typically have a shape of :math:`(N, C, H, W)` or :math:`(C, H, W)`\n",
        "    which is not suitable for plotting as images. This function formats an\n",
        "    input tensor :math:`(H, W, C)` for RGB and :math:`(H, W)` for mono-channel\n",
        "    data.\n",
        "\n",
        "    Args:\n",
        "        tensor (torch.Tensor, torch.float32): Image tensor\n",
        "\n",
        "    Shape:\n",
        "        Input: :math:`(N, C, H, W)` or :math:`(C, H, W)`\n",
        "        Output: :math:`(H, W, C)` or :math:`(H, W)`, respectively\n",
        "\n",
        "    Return:\n",
        "        torch.Tensor (torch.float32): Formatted image tensor (detached)\n",
        "\n",
        "    Note:\n",
        "        Symbols used to describe dimensions:\n",
        "            - N: number of images in a batch\n",
        "            - C: number of channels\n",
        "            - H: height of the image\n",
        "            - W: width of the image\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    has_batch_dimension = len(tensor.shape) == 4\n",
        "    formatted = tensor.clone()\n",
        "\n",
        "    if has_batch_dimension:\n",
        "        formatted = tensor.squeeze(0)\n",
        "\n",
        "    if formatted.shape[0] == 1:\n",
        "        return formatted.squeeze(0).detach()\n",
        "    else:\n",
        "        return formatted.permute(1, 2, 0).detach()\n",
        "\n",
        "\n",
        "def visualize(input_, gradients, save_path=None, cmap='viridis', alpha=0.7):\n",
        "\n",
        "    \"\"\" Method to plot the explanation.\n",
        "\n",
        "        # Arguments\n",
        "            input_: Tensor. Original image.\n",
        "            gradients: Tensor. Saliency map result.\n",
        "            save_path: String. Defaults to None.\n",
        "            cmap: Defaults to be 'viridis'.\n",
        "            alpha: Defaults to be 0.7.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    input_ = format_for_plotting(denormalize(input_))\n",
        "    gradients = format_for_plotting(standardize_and_clip(gradients))\n",
        "\n",
        "    subplots = [\n",
        "        ('Input image', [(input_, None, None)]),\n",
        "        ('Saliency map across RGB channels', [(gradients, None, None)]),\n",
        "        ('Overlay', [(input_, None, None), (gradients, cmap, alpha)])\n",
        "    ]\n",
        "\n",
        "    num_subplots = len(subplots)\n",
        "\n",
        "    fig = plt.figure(figsize=(16, 3))\n",
        "\n",
        "    for i, (title, images) in enumerate(subplots):\n",
        "        ax = fig.add_subplot(1, num_subplots, i + 1)\n",
        "        ax.set_axis_off()\n",
        "\n",
        "        for image, cmap, alpha in images:\n",
        "            ax.imshow(image, cmap=cmap, alpha=alpha)\n",
        "\n",
        "        ax.set_title(title)\n",
        "    if save_path is not None:\n",
        "        plt.savefig(save_path)\n",
        "\n",
        "\n",
        "def basic_visualize(input_, gradients, save_path=None, weight=None, cmap='viridis', alpha=0.7):\n",
        "\n",
        "    \"\"\" Method to plot the explanation.\n",
        "\n",
        "        # Arguments\n",
        "            input_: Tensor. Original image.\n",
        "            gradients: Tensor. Saliency map result.\n",
        "            save_path: String. Defaults to None.\n",
        "            cmap: Defaults to be 'viridis'.\n",
        "            alpha: Defaults to be 0.7.\n",
        "\n",
        "    \"\"\"\n",
        "    input_ = format_for_plotting(denormalize(input_))\n",
        "    gradients = format_for_plotting(standardize_and_clip(gradients))\n",
        "\n",
        "    subplots = [\n",
        "        ('Saliency map across RGB channels', [(gradients, None, None)]),\n",
        "        ('Overlay', [(input_, None, None), (gradients, cmap, alpha)])\n",
        "    ]\n",
        "\n",
        "    num_subplots = len(subplots)\n",
        "\n",
        "    fig = plt.figure(figsize=(4, 4))\n",
        "    count=0\n",
        "    for i, (title, images) in enumerate(subplots):\n",
        "        ax = fig.add_subplot(1, num_subplots, i + 1)\n",
        "        ax.set_axis_off()\n",
        "\n",
        "        for image, cmap, alpha in images:\n",
        "            print('hello {}'.format(count))\n",
        "            count+=1\n",
        "            ax.imshow(image, cmap=cmap, alpha=alpha)\n",
        "\n",
        "    if save_path is not None:\n",
        "        plt.savefig(save_path)\n",
        "\n",
        "    return images\n",
        "\n",
        "\n",
        "def find_resnet_layer(arch, target_layer_name):\n",
        "    \"\"\"Find resnet layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "    Args:\n",
        "        arch: default torchvision densenet models\n",
        "        target_layer_name (str): the name of layer with its hierarchical information. please refer to usages below.\n",
        "            target_layer_name = 'conv1'\n",
        "            target_layer_name = 'layer1'\n",
        "            target_layer_name = 'layer1_basicblock0'\n",
        "            target_layer_name = 'layer1_basicblock0_relu'\n",
        "            target_layer_name = 'layer1_bottleneck0'\n",
        "            target_layer_name = 'layer1_bottleneck0_conv1'\n",
        "            target_layer_name = 'layer1_bottleneck0_downsample'\n",
        "            target_layer_name = 'layer1_bottleneck0_downsample_0'\n",
        "            target_layer_name = 'avgpool'\n",
        "            target_layer_name = 'fc'\n",
        "\n",
        "    Return:\n",
        "        target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "    if target_layer_name is None:\n",
        "        target_layer_name = 'layer4'\n",
        "\n",
        "    if 'layer' in target_layer_name:\n",
        "        hierarchy = target_layer_name.split('_')\n",
        "        layer_num = int(hierarchy[0].lstrip('layer'))\n",
        "        if layer_num == 1:\n",
        "            target_layer = arch.layer1\n",
        "        elif layer_num == 2:\n",
        "            target_layer = arch.layer2\n",
        "        elif layer_num == 3:\n",
        "            target_layer = arch.layer3\n",
        "        elif layer_num == 4:\n",
        "            target_layer = arch.layer4\n",
        "        else:\n",
        "            raise ValueError('unknown layer : {}'.format(target_layer_name))\n",
        "\n",
        "        if len(hierarchy) >= 2:\n",
        "            bottleneck_num = int(hierarchy[1].lower().lstrip('bottleneck').lstrip('basicblock'))\n",
        "            target_layer = target_layer[bottleneck_num]\n",
        "\n",
        "        if len(hierarchy) >= 3:\n",
        "            target_layer = target_layer._modules[hierarchy[2]]\n",
        "\n",
        "        if len(hierarchy) == 4:\n",
        "            target_layer = target_layer._modules[hierarchy[3]]\n",
        "\n",
        "    else:\n",
        "        target_layer = arch._modules[target_layer_name]\n",
        "\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "def find_densenet_layer(arch, target_layer_name):\n",
        "    \"\"\"Find densenet layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "    Args:\n",
        "        arch: default torchvision densenet models\n",
        "        target_layer_name (str): the name of layer with its hierarchical information. please refer to usages below.\n",
        "            target_layer_name = 'features'\n",
        "            target_layer_name = 'features_transition1'\n",
        "            target_layer_name = 'features_transition1_norm'\n",
        "            target_layer_name = 'features_denseblock2_denselayer12'\n",
        "            target_layer_name = 'features_denseblock2_denselayer12_norm1'\n",
        "            target_layer_name = 'features_denseblock2_denselayer12_norm1'\n",
        "            target_layer_name = 'classifier'\n",
        "\n",
        "    Return:\n",
        "        target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "\n",
        "    if target_layer_name is None:\n",
        "        target_layer_name = 'features'\n",
        "\n",
        "    hierarchy = target_layer_name.split('_')\n",
        "    target_layer = arch._modules[hierarchy[0]]\n",
        "\n",
        "    if len(hierarchy) >= 2:\n",
        "        target_layer = target_layer._modules[hierarchy[1]]\n",
        "\n",
        "    if len(hierarchy) >= 3:\n",
        "        target_layer = target_layer._modules[hierarchy[2]]\n",
        "\n",
        "    if len(hierarchy) == 4:\n",
        "        target_layer = target_layer._modules[hierarchy[3]]\n",
        "\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "def find_vgg_layer(arch, target_layer_name):\n",
        "    \"\"\"Find vgg layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "    Args:\n",
        "        arch: default torchvision densenet models\n",
        "        target_layer_name (str): the name of layer with its hierarchical information. please refer to usages below.\n",
        "            target_layer_name = 'features'\n",
        "            target_layer_name = 'features_42'\n",
        "            target_layer_name = 'classifier'\n",
        "            target_layer_name = 'classifier_0'\n",
        "\n",
        "    Return:\n",
        "        target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "    if target_layer_name is None:\n",
        "        target_layer_name = 'features'\n",
        "\n",
        "    hierarchy = target_layer_name.split('_')\n",
        "\n",
        "    if len(hierarchy) >= 1:\n",
        "        target_layer = arch.features\n",
        "\n",
        "    if len(hierarchy) == 2:\n",
        "        target_layer = target_layer[int(hierarchy[1])]\n",
        "\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "def find_alexnet_layer(arch, target_layer_name):\n",
        "    \"\"\"Find alexnet layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "    Args:\n",
        "        arch: default torchvision densenet models\n",
        "        target_layer_name (str): the name of layer with its hierarchical information. please refer to usages below.\n",
        "            target_layer_name = 'features'\n",
        "            target_layer_name = 'features_0'\n",
        "            target_layer_name = 'classifier'\n",
        "            target_layer_name = 'classifier_0'\n",
        "\n",
        "    Return:\n",
        "        target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "    if target_layer_name is None:\n",
        "        target_layer_name = 'features_29'\n",
        "\n",
        "    hierarchy = target_layer_name.split('_')\n",
        "\n",
        "    if len(hierarchy) >= 1:\n",
        "        target_layer = arch.features\n",
        "\n",
        "    if len(hierarchy) == 2:\n",
        "        target_layer = target_layer[int(hierarchy[1])]\n",
        "\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "def find_squeezenet_layer(arch, target_layer_name):\n",
        "    \"\"\"Find squeezenet layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "        Args:\n",
        "            - **arch - **: default torchvision densenet models\n",
        "            - **target_layer_name (str) - **: the name of layer with its hierarchical information. please refer to usages below.\n",
        "                target_layer_name = 'features_12'\n",
        "                target_layer_name = 'features_12_expand3x3'\n",
        "                target_layer_name = 'features_12_expand3x3_activation'\n",
        "\n",
        "        Return:\n",
        "            target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "    if target_layer_name is None:\n",
        "        target_layer_name = 'features'\n",
        "\n",
        "    hierarchy = target_layer_name.split('_')\n",
        "    target_layer = arch._modules[hierarchy[0]]\n",
        "\n",
        "    if len(hierarchy) >= 2:\n",
        "        target_layer = target_layer._modules[hierarchy[1]]\n",
        "\n",
        "    if len(hierarchy) == 3:\n",
        "        target_layer = target_layer._modules[hierarchy[2]]\n",
        "\n",
        "    elif len(hierarchy) == 4:\n",
        "        target_layer = target_layer._modules[hierarchy[2] + '_' + hierarchy[3]]\n",
        "\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "def find_googlenet_layer(arch, target_layer_name):\n",
        "    \"\"\"Find squeezenet layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "        Args:\n",
        "            - **arch - **: default torchvision googlenet models\n",
        "            - **target_layer_name (str) - **: the name of layer with its hierarchical information. please refer to usages below.\n",
        "                target_layer_name = 'inception5b'\n",
        "\n",
        "        Return:\n",
        "            target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "    if target_layer_name is None:\n",
        "        target_layer_name = 'features'\n",
        "\n",
        "    hierarchy = target_layer_name.split('_')\n",
        "    target_layer = arch._modules[hierarchy[0]]\n",
        "\n",
        "    if len(hierarchy) >= 2:\n",
        "        target_layer = target_layer._modules[hierarchy[1]]\n",
        "\n",
        "    if len(hierarchy) == 3:\n",
        "        target_layer = target_layer._modules[hierarchy[2]]\n",
        "\n",
        "    elif len(hierarchy) == 4:\n",
        "        target_layer = target_layer._modules[hierarchy[2] + '_' + hierarchy[3]]\n",
        "\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "def find_mobilenet_layer(arch, target_layer_name):\n",
        "    \"\"\"Find mobilenet layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "        Args:\n",
        "            - **arch - **: default torchvision googlenet models\n",
        "            - **target_layer_name (str) - **: the name of layer with its hierarchical information. please refer to usages below.\n",
        "                target_layer_name = 'features'\n",
        "\n",
        "        Return:\n",
        "            target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "    if target_layer_name is None:\n",
        "        target_layer_name = 'features'\n",
        "\n",
        "    hierarchy = target_layer_name.split('_')\n",
        "    target_layer = arch._modules[hierarchy[0]]\n",
        "\n",
        "    if len(hierarchy) >= 2:\n",
        "        target_layer = target_layer._modules[hierarchy[1]]\n",
        "\n",
        "    if len(hierarchy) == 3:\n",
        "        target_layer = target_layer._modules[hierarchy[2]]\n",
        "\n",
        "    elif len(hierarchy) == 4:\n",
        "        target_layer = target_layer._modules[hierarchy[2] + '_' + hierarchy[3]]\n",
        "\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "def find_shufflenet_layer(arch, target_layer_name):\n",
        "    \"\"\"Find mobilenet layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "        Args:\n",
        "            - **arch - **: default torchvision googlenet models\n",
        "            - **target_layer_name (str) - **: the name of layer with its hierarchical information. please refer to usages below.\n",
        "                target_layer_name = 'conv5'\n",
        "\n",
        "        Return:\n",
        "            target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "    if target_layer_name is None:\n",
        "        target_layer_name = 'features'\n",
        "\n",
        "    hierarchy = target_layer_name.split('_')\n",
        "    target_layer = arch._modules[hierarchy[0]]\n",
        "\n",
        "    if len(hierarchy) >= 2:\n",
        "        target_layer = target_layer._modules[hierarchy[1]]\n",
        "\n",
        "    if len(hierarchy) == 3:\n",
        "        target_layer = target_layer._modules[hierarchy[2]]\n",
        "\n",
        "    elif len(hierarchy) == 4:\n",
        "        target_layer = target_layer._modules[hierarchy[2] + '_' + hierarchy[3]]\n",
        "\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "def find_layer(arch, target_layer_name):\n",
        "    \"\"\"Find target layer to calculate CAM.\n",
        "\n",
        "        : Args:\n",
        "            - **arch - **: Self-defined architecture.\n",
        "            - **target_layer_name - ** (str): Name of target class.\n",
        "\n",
        "        : Return:\n",
        "            - **target_layer - **: Found layer. This layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "\n",
        "    if target_layer_name.split('_') not in arch._modules.keys():\n",
        "        raise Exception(\"Invalid target layer name.\")\n",
        "    target_layer = arch._modules[target_layer_name]\n",
        "    return target_layer\n",
        "'''\n",
        "Part of code borrows from https://github.com/1Konny/gradcam_plus_plus-pytorch\n",
        "'''\n",
        "\n",
        "import torch\n",
        "\n",
        "class BaseCAM(object):\n",
        "    \"\"\" Base class for Class activation mapping.\n",
        "        : Args\n",
        "            - **model_dict -** : Dict. Has format as dict(type='vgg', arch=torchvision.models.vgg16(pretrained=True),\n",
        "            layer_name='features',input_size=(224, 224)).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_dict):\n",
        "        model_type = model_dict['type']\n",
        "        layer_name = model_dict['layer_name']\n",
        "        \n",
        "        self.model_arch = model_dict['arch']\n",
        "        self.model_arch.eval()\n",
        "        #if torch.cuda.is_available():\n",
        "        #  self.model_arch.cuda()\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            #if torch.cuda.is_available():\n",
        "            #  self.gradients['value'] = grad_output[0].cuda()\n",
        "            \n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            \n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            #if torch.cuda.is_available():\n",
        "            #  self.activations['value'] = output.cuda()\n",
        "            self.activations['value'] = output\n",
        "            \n",
        "            return None\n",
        "\n",
        "        if 'vgg' in model_type.lower():\n",
        "            self.target_layer = find_vgg_layer(self.model_arch, layer_name)\n",
        "        elif 'resnet' in model_type.lower():\n",
        "            self.target_layer = find_resnet_layer(self.model_arch, layer_name)\n",
        "        elif 'densenet' in model_type.lower():\n",
        "            self.target_layer = find_densenet_layer(self.model_arch, layer_name)\n",
        "        elif 'alexnet' in model_type.lower():\n",
        "            self.target_layer = find_alexnet_layer(self.model_arch, layer_name)\n",
        "        elif 'squeezenet' in model_type.lower():\n",
        "            self.target_layer = find_squeezenet_layer(self.model_arch, layer_name)\n",
        "        elif 'googlenet' in model_type.lower():\n",
        "            self.target_layer = find_googlenet_layer(self.model_arch, layer_name)\n",
        "        elif 'shufflenet' in model_type.lower():\n",
        "            self.target_layer = find_shufflenet_layer(self.model_arch, layer_name)\n",
        "        elif 'mobilenet' in model_type.lower():\n",
        "            self.target_layer = find_mobilenet_layer(self.model_arch, layer_name)\n",
        "        else:\n",
        "            self.target_layer = find_layer(self.model_arch, layer_name)\n",
        "\n",
        "        self.target_layer.register_forward_hook(forward_hook)\n",
        "        self.target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "    def forward(self, input, class_idx=None, retain_graph=False):\n",
        "        return None\n",
        "\n",
        "    def __call__(self, input, class_idx=None, retain_graph=False):\n",
        "        return self.forward(input, class_idx, retain_graph)\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GradCAM(BaseCAM):\n",
        "    \"\"\"\n",
        "        GradCAM, inherit from BaseCAM\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_dict):\n",
        "        super().__init__(model_dict)\n",
        "\n",
        "    def forward(self, input_, class_idx=None, retain_graph=False):\n",
        "        \"\"\"Generates GradCAM result.\n",
        "\n",
        "        # Arguments\n",
        "            input_: torch.Tensor. Preprocessed image with shape (1, C, H, W).\n",
        "            class_idx: int. Index of target class. Defaults to be index of predicted class.\n",
        "\n",
        "        # Return\n",
        "            Result of GradCAM (torch.Tensor) with shape (1, H, W).\n",
        "        \"\"\"\n",
        "\n",
        "        b, c, h, w = input_.size()\n",
        "        logit = self.model_arch(input_)\n",
        "\n",
        "        if class_idx is None:\n",
        "            score = logit[:, logit.max(1)[-1]].squeeze()\n",
        "        else:\n",
        "            score = logit[:, class_idx].squeeze()\n",
        "\n",
        "        self.model_arch.zero_grad()\n",
        "        score.backward(retain_graph=retain_graph)\n",
        "\n",
        "        gradients = self.gradients['value']\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = gradients.size()\n",
        "\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "\n",
        "        saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "        saliency_map = F.relu(saliency_map)\n",
        "        saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "        saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "        saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "\n",
        "        return saliency_map\n",
        "\n",
        "\n",
        "class ScoreCAM(BaseCAM):\n",
        "\n",
        "    \"\"\"\n",
        "        ScoreCAM, inherit from BaseCAM\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_dict):\n",
        "        super().__init__(model_dict)\n",
        "\n",
        "    def forward(self, input, class_idx=None, retain_graph=False):\n",
        "        b, c, h, w = input.size()\n",
        "        \n",
        "        # predication on raw input\n",
        "        logit = self.model_arch(input)\n",
        "        \n",
        "        if class_idx is None:\n",
        "            predicted_class = logit.max(1)[-1]\n",
        "            score = logit[:, logit.max(1)[-1]].squeeze()\n",
        "        else:\n",
        "            predicted_class = torch.LongTensor([class_idx])\n",
        "            score = logit[:, class_idx].squeeze()\n",
        "        \n",
        "        logit = F.softmax(logit)\n",
        "\n",
        "        #if torch.cuda.is_available():\n",
        "        #  predicted_class= predicted_class.cuda()\n",
        "        #  score = score.cuda()\n",
        "        #  logit = logit.cuda()\n",
        "\n",
        "        self.model_arch.zero_grad()\n",
        "        score.backward(retain_graph=retain_graph)\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = activations.size()\n",
        "        \n",
        "        score_saliency_map = torch.zeros((1, 1, h, w))\n",
        "\n",
        "        #if torch.cuda.is_available():\n",
        "        #  activations = activations.cuda()\n",
        "        #  score_saliency_map = score_saliency_map.cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "          for i in range(k):\n",
        "\n",
        "              # upsampling\n",
        "              saliency_map = torch.unsqueeze(activations[:, i, :, :], 1)\n",
        "              saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "              \n",
        "              if saliency_map.max() == saliency_map.min():\n",
        "                continue\n",
        "              \n",
        "              # normalize to 0-1\n",
        "              norm_saliency_map = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min())\n",
        "\n",
        "              # how much increase if keeping the highlighted region\n",
        "              # predication on masked input\n",
        "              output = self.model_arch(input * norm_saliency_map)\n",
        "              output = F.softmax(output)\n",
        "              score = output[0][predicted_class]\n",
        "\n",
        "              score_saliency_map +=  score * saliency_map\n",
        "                \n",
        "        score_saliency_map = F.relu(score_saliency_map)\n",
        "        score_saliency_map_min, score_saliency_map_max = score_saliency_map.min(), score_saliency_map.max()\n",
        "\n",
        "        if score_saliency_map_min == score_saliency_map_max:\n",
        "            return None\n",
        "\n",
        "        score_saliency_map = (score_saliency_map - score_saliency_map_min).div(score_saliency_map_max - score_saliency_map_min).data\n",
        "\n",
        "        return score_saliency_map\n",
        "\n",
        "    def __call__(self, input, class_idx=None, retain_graph=False):\n",
        "        return self.forward(input, class_idx, retain_graph)\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class ScoreCAM1(BaseCAM):\n",
        "\n",
        "    \"\"\"\n",
        "        ScoreCAM, inherit from BaseCAM\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_dict):\n",
        "        super().__init__(model_dict)\n",
        "\n",
        "    def forward(self, input, class_idx=None, retain_graph=False):\n",
        "        b, c, h, w = input.size()\n",
        "        \n",
        "        # predication on raw input\n",
        "        logit = self.model_arch(input)\n",
        "        \n",
        "        if class_idx is None:\n",
        "            predicted_class = logit.max(1)[-1]\n",
        "            score = logit[:, logit.max(1)[-1]].squeeze()\n",
        "        else:\n",
        "            predicted_class = torch.LongTensor([class_idx])\n",
        "            score = logit[:, class_idx].squeeze()\n",
        "        \n",
        "        logit = F.softmax(logit)\n",
        "\n",
        "        #if torch.cuda.is_available():\n",
        "        #  predicted_class= predicted_class.cuda()\n",
        "        #  score = score.cuda()\n",
        "        #  logit = logit.cuda()\n",
        "\n",
        "        self.model_arch.zero_grad()\n",
        "        score.backward(retain_graph=retain_graph)\n",
        "        activations = self.activations['value']\n",
        "        b1, k, u, v = activations.size()\n",
        "        gradients = self.gradients['value']\n",
        "        b2, k1, u1, v1 = gradients.size()\n",
        "\n",
        "        score_saliency_map = torch.zeros((1, 1, h, w))\n",
        "\n",
        "        #if torch.cuda.is_available():\n",
        "        #  activations = activations.cuda()\n",
        "        #  score_saliency_map = score_saliency_map.cuda()\n",
        "\n",
        "        print(activations.size())\n",
        "        print(gradients.size())\n",
        "\n",
        "        smooth = torch.zeros((1,k,u,v))\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "\n",
        "        mean = 0\n",
        "        param_sigma_multiplier = 4\n",
        "        \n",
        "        with torch.no_grad():\n",
        "          for i in range(k):\n",
        "\n",
        "              # upsampling\n",
        "              saliency_map1 = torch.unsqueeze(activations[:, i, :, :], 1)\n",
        "              \n",
        "              #noise = Variable(smooth.data.new((h,w).normal_(mean, sigma**2)))\n",
        "\n",
        "              sigma = param_sigma_multiplier / (torch.max(gradients) - torch.min(gradients)).item()              \n",
        "              noise = torch.empty((k,u,v)).normal_(mean=mean,std=sigma)\n",
        "              noisy_img = smooth + noise\n",
        "\n",
        "              #print(noisy_img.shape)\n",
        "\n",
        "              gradients = noisy_img + gradients\n",
        "              saliency_map2 = torch.unsqueeze(gradients[:, i, :, :], 1)\n",
        "              saliency_map3 = (weights * activations).sum(1, keepdim=True)\n",
        "              saliency_map = torch.mul(saliency_map1, saliency_map2)\n",
        "              saliency_map = torch.mul(saliency_map, saliency_map3)\n",
        "              \n",
        "              saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "              \n",
        "              if saliency_map.max() == saliency_map.min():\n",
        "                continue\n",
        "              \n",
        "              # normalize to 0-1\n",
        "              norm_saliency_map = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min())\n",
        "\n",
        "              # how much increase if keeping the highlighted region\n",
        "              # predication on masked input\n",
        "              output = self.model_arch(input * norm_saliency_map)\n",
        "              output = F.softmax(output)\n",
        "              score = output[0][predicted_class]\n",
        "\n",
        "              score_saliency_map +=  score * saliency_map\n",
        "                \n",
        "        score_saliency_map = F.relu(score_saliency_map)\n",
        "        score_saliency_map_min, score_saliency_map_max = score_saliency_map.min(), score_saliency_map.max()\n",
        "\n",
        "        if score_saliency_map_min == score_saliency_map_max:\n",
        "            return None\n",
        "\n",
        "        score_saliency_map = (score_saliency_map - score_saliency_map_min).div(score_saliency_map_max - score_saliency_map_min).data\n",
        "\n",
        "        return score_saliency_map\n",
        "\n",
        "    def __call__(self, input, class_idx=None, retain_graph=False):\n",
        "        return self.forward(input, class_idx, retain_graph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_B44BatzxL_d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "listscore = []\n",
        "avg_drop = []\n",
        "avg_drop_sc = []\n",
        "count1 = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkawSsSL4JbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "from PIL import Image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iuQm_lP1_TV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_activation_map(image_path, \\\n",
        "                       model_path='/gdrive/My Drive/mobilenet-rocket-chess-2.pt', \\\n",
        "                       model_type='mobilenet', \\\n",
        "                       layer_name='features', \\\n",
        "                       target_class=2):\n",
        "  '''\n",
        "  # model_path -\n",
        "   VGG16 CHESS = '/gdrive/My Drive/vgg16-rocket-chess_2.pt'\n",
        "   MOBILENET CHESS = '/gdrive/My Drive/mobilenet-rocket-chess-2.pt'\n",
        "   PNEUMONIA DENSENET =  '/gdrive/My Drive/densenet-rocket-3.pt'\n",
        "   PNEUMONIA GOOGLENET =  '/gdrive/My Drive/googlenet-rocket-1.pt'\n",
        "\n",
        "\n",
        "  # model_type - \n",
        "   VGG16 = 'vgg16' or 'vgg'(whichever works, preferably 1st option)\n",
        "   Mobilenet = 'mobilenet'\n",
        "   Densenet = 'densenet'\n",
        "   Googlenet = 'googlenet'\n",
        "\n",
        "  # layer_name - \n",
        "   VGG16 - 'features_29'\n",
        "   Mobilenet - 'features'\n",
        "   Densenet - 'features'\n",
        "   Googlenet - 'inception5b'\n",
        "\n",
        "  #target_class \n",
        "  Chess =   0 - Bishop\n",
        "            1 - King\n",
        "            2 - Knight\n",
        "            3 - Pawn\n",
        "            4 - Queen\n",
        "            5 - Rook\n",
        "\n",
        "  Pneumonia = 0 - Normal\n",
        "              1 - Pneumonia\n",
        "\n",
        "\n",
        "  '''\n",
        "\n",
        "  from torchvision import transforms, datasets, models\n",
        "  import torch\n",
        "  from torch import optim, cuda\n",
        "  from torch.utils.data import DataLoader, sampler\n",
        "  import torch.nn as nn\n",
        "\n",
        "  import warnings\n",
        "  warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "  # Data science tools\n",
        "  import numpy as np\n",
        "  import pandas as pd\n",
        "  import os\n",
        "\n",
        "  # Image manipulations\n",
        "  from PIL import Image\n",
        "  # Useful for examining network\n",
        "  from torchsummary import summary\n",
        "  # Timing utility\n",
        "  from timeit import default_timer as timer\n",
        "\n",
        "  # Visualizations\n",
        "  import matplotlib.pyplot as plt\n",
        "  %matplotlib inline\n",
        "  plt.rcParams['font.size'] = 14\n",
        "\n",
        "  model = torch.load(model_path)\n",
        "\n",
        "  X = model.eval()\n",
        "\n",
        "  x_model_dict = dict(type=model_type, arch=X, layer_name=layer_name, input_size=(224, 224))\n",
        "\n",
        "  x_scorecam = ScoreCAM(x_model_dict)\n",
        "\n",
        "  input_image = load_image(image_path)\n",
        "  print('Image Path-->{}'.format(image_path))\n",
        "  input_ = apply_transforms(input_image)\n",
        "  #print(input_)\n",
        "  original_image =  Image.open(image_path)\n",
        "  prep_img = preprocess_image(original_image)\n",
        "  \n",
        "  predicted_score = X(input_).max(1)[0]\n",
        "  listscore.append(predicted_score)\n",
        "  #predicted_class = X(input_).max(1)[-1]\n",
        "  #predicted_class = 1+predicted_class\n",
        "  \n",
        "  print(X(input_).max(1))\n",
        "  \n",
        "  scorecam_map = x_scorecam(input_)\n",
        "  \n",
        "  #scorecam_image = basic_visualize(input_.cpu(), \\\n",
        "  #                                 scorecam_map.type(torch.FloatTensor).cpu())\n",
        "  \n",
        "  param_n = 50\n",
        "  param_sigma_multiplier = 4\n",
        "\n",
        "  GBP = Backprop(model, guided=True)\n",
        "  \n",
        "  smooth_grad = generate_smooth_grad(GBP,  # ^This parameter\n",
        "                                    prep_img,\n",
        "                                    target_class,\n",
        "                                    param_n,\n",
        "                                    param_sigma_multiplier)\n",
        "  print(smooth_grad)\n",
        "\n",
        "  grayscale_smooth_grad = convert_to_grayscale(smooth_grad)\n",
        "  #print(smooth_grad.shape)\n",
        "  #smooth_grad = torch.FloatTensor(smooth_grad)\n",
        "\n",
        "  #smooth_grad.unsqueeze_(0)\n",
        "\n",
        "  save_image(grayscale_smooth_grad, \"./My Drive/grayscalesmooth.jpg\")\n",
        "\n",
        "  input_im = load_image(\"./My Drive/grayscalesmooth.jpg\")\n",
        "  input_gray = apply_transforms(input_im)\n",
        "\n",
        "  x_scorecam1 = ScoreCAM(x_model_dict)\n",
        "\n",
        "  scorecam_map1 = x_scorecam1(input_gray)\n",
        "  scorecam_image = basic_visualize(input_.cpu(), \\\n",
        "                                   scorecam_map1.type(torch.FloatTensor).cpu())    \n",
        "  #print(scorecam_map)\n",
        "  #print(scorecam_map1)\n",
        "  \n",
        "  mask_intensity = 50\n",
        "  print(\"ScoreCAM completed\")\n",
        "\n",
        "  seg = scorecam_map1[0][0].numpy().astype(np.float32)\n",
        "  #plt.imshow(seg)\n",
        "  img = Image.fromarray(np.uint8(seg * 255) , 'L')\n",
        "  #plt.imshow(img)\n",
        "  img.save('./My Drive/saved_img.jpg')\n",
        "  arr = cv2.imread('./My Drive/saved_img.jpg',cv2.IMREAD_GRAYSCALE)\n",
        "  #print(arr)\n",
        "  mask =np.where(arr>mask_intensity,1,0)\n",
        "\n",
        "  global count1\n",
        "  Yci = listscore[count1]\n",
        "  Yci = torch.exp(Yci)\n",
        "  #Yci = 1+Yci\n",
        "  print(Yci)\n",
        "  img = Image.fromarray(np.uint8(mask * 255) , 'L')\n",
        "  img.save('./My Drive/saved_mask.jpg')\n",
        "  \n",
        "  im = load_image('./My Drive/saved_mask.jpg')\n",
        "  #print(im.shape)\n",
        "  m = apply_transforms(im)\n",
        "\n",
        "  Oci = X(m).max(1)[0]\n",
        "  Oci = torch.exp(Oci)\n",
        "  #Oci = 1+Oci\n",
        "  print(Oci)\n",
        "  print(Yci - Oci)\n",
        "  avg_drop.append(((max(0, Yci - Oci))/(Yci)))\n",
        "  print(((max(0, Yci - Oci))/(Yci)))\n",
        "\n",
        "\n",
        "  seg = scorecam_map[0][0].numpy().astype(np.float32)\n",
        "  #plt.imshow(seg)\n",
        "  img = Image.fromarray(np.uint8(seg * 255) , 'L')\n",
        "  #plt.imshow(img)\n",
        "  img.save('./My Drive/saved_img.jpg')\n",
        "  arr = cv2.imread('./My Drive/saved_img.jpg',cv2.IMREAD_GRAYSCALE)\n",
        "  #print(arr)\n",
        "  mask =np.where(arr>mask_intensity,1,0)\n",
        "  img = Image.fromarray(np.uint8(mask * 255) , 'L')\n",
        "  img.save('./My Drive/saved_mask.jpg')\n",
        "  im = load_image('./My Drive/saved_mask.jpg')\n",
        "  #print(im.shape)\n",
        "  m = apply_transforms(im)\n",
        "\n",
        "  Oci = X(m).max(1)[0]\n",
        "  Oci = torch.exp(Oci)\n",
        "  #Oci = 1+Oci\n",
        "  print(Oci)\n",
        "  print(Yci - Oci)\n",
        "  avg_drop_sc.append(((max(0, Yci - Oci))/(Yci)))\n",
        "  print(((max(0, Yci - Oci))/(Yci)))\n",
        "  \n",
        "  count1 += 1\n",
        "\n",
        "  return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5KbA7N3tJgP",
        "colab_type": "code",
        "outputId": "35bc2c7c-a34c-4732-f30b-18d905fa6cc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "f = os.listdir('/gdrive/My Drive/Chessman-image-dataset/Chess_test/Knight/')\n",
        "for i in f:\n",
        "  get_activation_map('/gdrive/My Drive/Chessman-image-dataset/Chess_test/Knight/'+i)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image Path-->/gdrive/My Drive/Chessman-image-dataset/Chess_test/Knight/00000005.png\n",
            "could not transform PIL_img to a PIL Image object. Please check input.\n",
            "torch.return_types.max(\n",
            "values=tensor([-0.0013], grad_fn=<MaxBackward0>),\n",
            "indices=tensor([2]))\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:741: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:775: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[[ 1.68532832e-06  6.77245762e-05 -4.13316255e-05 ... -9.35025979e-06\n",
            "   -1.11664860e-04 -1.26717538e-04]\n",
            "  [-3.36205866e-05  6.66879816e-06 -5.04326262e-05 ... -5.09848446e-05\n",
            "   -8.21579527e-05 -1.00126015e-04]\n",
            "  [ 6.13229768e-05 -4.75681433e-05 -6.05247042e-06 ... -6.62888959e-05\n",
            "    6.42786035e-06 -2.38788221e-05]\n",
            "  ...\n",
            "  [ 2.65389378e-05 -2.24400521e-05  3.62889119e-05 ... -1.76518993e-05\n",
            "    2.41396995e-05  2.45892699e-05]\n",
            "  [ 1.85803801e-06 -1.20437413e-06 -2.86477106e-05 ...  1.09573628e-05\n",
            "    3.41910141e-06  1.74277369e-05]\n",
            "  [ 3.76371492e-06  3.26927844e-05 -1.78626226e-05 ...  1.73395360e-05\n",
            "    7.56011519e-06  1.06859021e-05]]\n",
            "\n",
            " [[-2.30652746e-06  1.32241854e-04 -3.76928435e-05 ...  1.15034208e-04\n",
            "    8.46665353e-05 -4.57603950e-06]\n",
            "  [-4.51501785e-05  7.07172323e-06 -7.45305605e-05 ... -2.60079722e-05\n",
            "    6.71353051e-05  7.31118489e-05]\n",
            "  [ 1.32512562e-04 -8.16471130e-05 -2.54880078e-05 ... -8.22165143e-05\n",
            "   -2.07538856e-05 -1.98379369e-05]\n",
            "  ...\n",
            "  [ 4.23938455e-05 -5.48819406e-05 -2.89334450e-05 ... -7.23491795e-05\n",
            "   -2.41445750e-05 -1.62529212e-05]\n",
            "  [-2.22664079e-06 -4.60845511e-05 -1.12652248e-04 ... -5.61048277e-05\n",
            "   -5.65931899e-05 -2.28757691e-05]\n",
            "  [ 1.53415356e-06  2.89135729e-05 -5.94482943e-05 ... -1.13084225e-05\n",
            "   -2.25848332e-05 -1.59917388e-05]]\n",
            "\n",
            " [[-4.58847731e-06  3.37962178e-05 -1.22866069e-05 ...  1.67879555e-05\n",
            "    3.25233326e-05 -1.16265146e-06]\n",
            "  [-2.12881458e-05  4.50977823e-06 -7.28108571e-06 ... -3.37872538e-05\n",
            "    4.78789490e-05  6.09256700e-05]\n",
            "  [ 4.16853325e-05 -4.81931551e-06  1.21288444e-05 ... -1.19795371e-05\n",
            "    6.60834310e-06  1.22045819e-05]\n",
            "  ...\n",
            "  [ 1.09690614e-05 -9.14875651e-06  8.92235781e-06 ...  6.66442444e-06\n",
            "    2.46387627e-05  1.99148571e-05]\n",
            "  [-6.54790609e-06 -1.23072299e-06 -6.56543882e-06 ...  5.02727344e-06\n",
            "   -7.54881068e-06  1.86828969e-06]\n",
            "  [-3.49847280e-06  1.06564991e-05 -2.66486255e-06 ...  6.97853218e-06\n",
            "   -3.80183570e-06 -3.51260474e-06]]]\n",
            "hello 0\n",
            "hello 1\n",
            "hello 2\n",
            "ScoreCAM completed\n",
            "tensor([0.9987], grad_fn=<ExpBackward>)\n",
            "tensor([0.4647], grad_fn=<ExpBackward>)\n",
            "tensor([0.5340], grad_fn=<SubBackward0>)\n",
            "tensor([0.5347], grad_fn=<DivBackward0>)\n",
            "tensor([0.4968], grad_fn=<ExpBackward>)\n",
            "tensor([0.5019], grad_fn=<SubBackward0>)\n",
            "tensor([0.5026], grad_fn=<DivBackward0>)\n",
            "Image Path-->/gdrive/My Drive/Chessman-image-dataset/Chess_test/Knight/00000002.jpg\n",
            "could not transform PIL_img to a PIL Image object. Please check input.\n",
            "torch.return_types.max(\n",
            "values=tensor([-0.0011], grad_fn=<MaxBackward0>),\n",
            "indices=tensor([2]))\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:741: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[[-1.03139235e-04  6.21855259e-05 -6.71379361e-05 ...  7.64026400e-05\n",
            "    4.23400290e-05 -1.33176437e-04]\n",
            "  [-1.38468491e-04  7.23656733e-05 -2.40797456e-05 ... -8.99107382e-05\n",
            "    1.01437103e-04 -7.49857305e-05]\n",
            "  [-2.94312742e-05 -9.44641605e-05  9.88370273e-05 ... -1.37998434e-04\n",
            "    7.90878199e-06  2.82807136e-05]\n",
            "  ...\n",
            "  [-3.80646088e-07 -5.49978018e-05 -2.06061639e-05 ...  1.16493320e-07\n",
            "    2.16011493e-05  4.27026395e-05]\n",
            "  [-1.74359779e-05 -3.54626798e-05 -2.57426570e-05 ... -3.27910576e-05\n",
            "   -2.12535379e-05 -6.03665656e-06]\n",
            "  [-1.67823373e-05 -4.34502057e-06  1.15824852e-05 ...  2.68284441e-05\n",
            "    1.55042985e-05  1.67086930e-05]]\n",
            "\n",
            " [[-1.95713378e-04  2.26259995e-04  3.73105519e-05 ...  1.79719161e-04\n",
            "    1.61842741e-04 -1.54702105e-04]\n",
            "  [-2.05731634e-04  1.77645832e-04 -1.91400177e-06 ... -1.47181470e-04\n",
            "    1.93661898e-04 -6.02621678e-05]\n",
            "  [ 5.79049811e-09 -1.62521973e-04  5.31275244e-05 ... -1.66113134e-04\n",
            "    7.33488123e-06  8.26212484e-05]\n",
            "  ...\n",
            "  [-8.76660924e-07 -3.61499353e-05  6.06974121e-05 ... -6.26676716e-05\n",
            "   -3.82777932e-05  5.76811035e-07]\n",
            "  [-2.83992849e-07 -7.97668996e-06  9.90968663e-06 ... -5.31979464e-05\n",
            "   -3.70242517e-05 -1.10569107e-05]\n",
            "  [-1.19249104e-06  2.46469281e-05  4.83443076e-05 ...  6.20615017e-05\n",
            "    3.59469885e-05  3.48309288e-05]]\n",
            "\n",
            " [[-6.62756804e-05  4.70174942e-05  5.28086536e-05 ...  3.33846244e-05\n",
            "    3.38004692e-05 -4.98786848e-05]\n",
            "  [-5.56397205e-05  4.53672186e-05  5.22745959e-05 ... -8.18259269e-05\n",
            "    4.14316822e-05 -3.20702419e-05]\n",
            "  [ 4.28154692e-05  2.97972211e-05  7.16349622e-05 ... -6.71568373e-05\n",
            "   -3.72393406e-05 -1.89898477e-06]\n",
            "  ...\n",
            "  [ 5.84747933e-06 -7.87958270e-07 -5.48989396e-06 ... -2.45076045e-05\n",
            "   -3.25720129e-05 -1.75841153e-05]\n",
            "  [ 1.10261294e-05 -8.39572807e-06 -2.42706272e-05 ... -2.38684635e-05\n",
            "   -1.32430531e-05 -8.44649447e-06]\n",
            "  [ 3.99369048e-06  3.66616994e-06  6.09385897e-06 ...  1.49987824e-05\n",
            "    7.88168749e-06  7.25450111e-06]]]\n",
            "hello 0\n",
            "hello 1\n",
            "hello 2\n",
            "ScoreCAM completed\n",
            "tensor([0.9989], grad_fn=<ExpBackward>)\n",
            "tensor([0.3984], grad_fn=<ExpBackward>)\n",
            "tensor([0.6005], grad_fn=<SubBackward0>)\n",
            "tensor([0.6012], grad_fn=<DivBackward0>)\n",
            "tensor([0.8794], grad_fn=<ExpBackward>)\n",
            "tensor([0.1194], grad_fn=<SubBackward0>)\n",
            "tensor([0.1196], grad_fn=<DivBackward0>)\n",
            "Image Path-->/gdrive/My Drive/Chessman-image-dataset/Chess_test/Knight/00000010.JPG\n",
            "could not transform PIL_img to a PIL Image object. Please check input.\n",
            "torch.return_types.max(\n",
            "values=tensor([-0.4251], grad_fn=<MaxBackward0>),\n",
            "indices=tensor([2]))\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:775: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:741: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:775: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[[ 8.43397109e-06 -4.16239724e-06  2.78539490e-06 ... -2.01837160e-05\n",
            "    1.03129842e-05  1.00988359e-05]\n",
            "  [ 2.82576936e-05 -2.51558144e-06  4.36269632e-05 ...  5.96323609e-05\n",
            "    1.37423631e-05  3.15130455e-05]\n",
            "  [ 3.66920931e-05 -3.40533326e-05  9.47424211e-06 ...  1.18145673e-05\n",
            "    1.27808843e-05 -3.60618811e-06]\n",
            "  ...\n",
            "  [-7.48824386e-07  1.57362758e-05  3.54060810e-05 ...  7.13348854e-06\n",
            "    6.63750712e-05 -1.45434099e-05]\n",
            "  [ 1.16941030e-05  1.24089373e-05  1.10331574e-05 ...  5.67608234e-05\n",
            "    2.38790084e-05  3.06916703e-05]\n",
            "  [ 5.62659698e-06 -3.34838987e-06  2.59841327e-05 ...  5.91484644e-05\n",
            "    3.47750029e-05  2.02229852e-05]]\n",
            "\n",
            " [[ 3.04774335e-05 -7.35926093e-06 -4.84861247e-05 ... -6.42085169e-05\n",
            "   -3.26222973e-05  1.31210720e-06]\n",
            "  [ 3.99759738e-05 -6.61283149e-06  2.13729427e-05 ...  4.72786603e-05\n",
            "   -2.17956793e-05  1.97753357e-05]\n",
            "  [ 4.75889654e-06 -1.74253974e-04 -8.38411320e-05 ...  6.62448932e-06\n",
            "    3.85560072e-05  1.26766495e-05]\n",
            "  ...\n",
            "  [ 1.83996104e-06 -2.44391710e-05 -3.89228575e-05 ... -8.99580680e-05\n",
            "   -1.51540129e-05 -1.07518826e-04]\n",
            "  [-6.14811433e-07 -3.39886663e-05 -3.14498087e-05 ... -3.68028507e-05\n",
            "   -8.57280567e-05 -6.44337572e-05]\n",
            "  [-2.66213465e-06 -2.88805016e-05  8.12407234e-06 ...  8.30293458e-06\n",
            "   -8.31053010e-06 -3.25198285e-05]]\n",
            "\n",
            " [[ 9.93063673e-06 -5.05216653e-06 -1.68376951e-05 ... -1.11678126e-05\n",
            "   -1.11407344e-05  8.54649406e-07]\n",
            "  [ 1.34036457e-05  5.67469106e-07  1.26402534e-05 ...  2.92961719e-05\n",
            "   -9.15293756e-06  2.19082576e-07]\n",
            "  [ 7.83617725e-07 -5.03636152e-05 -2.62442045e-05 ...  2.14621192e-05\n",
            "    2.42038490e-05  1.37480884e-05]\n",
            "  ...\n",
            "  [ 1.24683662e-05 -3.33246338e-06 -1.05700677e-05 ...  8.48052849e-06\n",
            "    3.23371077e-05  1.25763426e-05]\n",
            "  [ 5.75148151e-06 -3.30582639e-06 -1.05537218e-05 ...  3.47833382e-05\n",
            "    2.37502367e-05  1.66165095e-05]\n",
            "  [ 2.56522733e-06 -2.14581698e-06  6.17364421e-07 ...  2.34984024e-05\n",
            "    1.79729983e-05  8.12876620e-06]]]\n",
            "hello 0\n",
            "hello 1\n",
            "hello 2\n",
            "ScoreCAM completed\n",
            "tensor([0.6537], grad_fn=<ExpBackward>)\n",
            "tensor([0.4368], grad_fn=<ExpBackward>)\n",
            "tensor([0.2169], grad_fn=<SubBackward0>)\n",
            "tensor([0.3318], grad_fn=<DivBackward0>)\n",
            "tensor([0.6177], grad_fn=<ExpBackward>)\n",
            "tensor([0.0360], grad_fn=<SubBackward0>)\n",
            "tensor([0.0551], grad_fn=<DivBackward0>)\n",
            "Image Path-->/gdrive/My Drive/Chessman-image-dataset/Chess_test/Knight/00000001.jpg\n",
            "could not transform PIL_img to a PIL Image object. Please check input.\n",
            "torch.return_types.max(\n",
            "values=tensor([-0.0057], grad_fn=<MaxBackward0>),\n",
            "indices=tensor([2]))\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:741: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:775: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[[ 1.71412118e-04  1.88690890e-05 -1.34467287e-04 ... -2.24242974e-04\n",
            "   -1.42632043e-04 -1.20201614e-04]\n",
            "  [-2.26728339e-05 -5.29319495e-04 -9.77729447e-06 ...  8.39545857e-06\n",
            "   -1.82246026e-04 -2.70760190e-04]\n",
            "  [ 1.00043975e-05 -3.97871658e-04  3.33116762e-04 ...  7.99344853e-05\n",
            "   -2.37457491e-04 -1.10365013e-04]\n",
            "  ...\n",
            "  [-4.57331212e-05 -4.29162569e-05  5.20758424e-05 ... -8.77904147e-05\n",
            "    1.39405287e-05  1.42142363e-05]\n",
            "  [-1.98011077e-05 -8.78313789e-06  1.53498910e-05 ... -3.14315269e-05\n",
            "   -1.88528118e-05 -2.16889312e-07]\n",
            "  [-1.19645474e-05  6.30560215e-06 -5.43038407e-05 ... -9.52062488e-06\n",
            "    7.70418323e-06  5.28952398e-06]]\n",
            "\n",
            " [[ 2.55155563e-04  1.47397164e-04  1.20380633e-04 ... -1.13191893e-04\n",
            "    1.99013390e-04  4.45089955e-05]\n",
            "  [-2.30614468e-05 -5.47478981e-04  2.53102072e-04 ...  1.90243926e-04\n",
            "    1.55429617e-04 -1.62622165e-04]\n",
            "  [ 2.14091688e-04 -5.76729551e-04  5.90045266e-04 ...  1.23790950e-04\n",
            "   -1.14733651e-04 -4.59899567e-05]\n",
            "  ...\n",
            "  [-4.13954630e-05 -2.25705188e-05  9.74353775e-05 ... -4.72145341e-05\n",
            "    7.59956054e-05  3.03460145e-05]\n",
            "  [-2.96971411e-05  1.01828296e-05  2.95717362e-05 ...  3.68244294e-05\n",
            "   -1.50744314e-05  1.49660930e-05]\n",
            "  [-1.29921327e-05  1.29043940e-05 -9.60031990e-05 ...  3.84097151e-05\n",
            "    2.96497671e-05  2.02400051e-05]]\n",
            "\n",
            " [[ 6.45773858e-05  1.13349967e-05  8.30589328e-05 ...  3.26864817e-05\n",
            "    1.80702060e-04  1.05776498e-04]\n",
            "  [-5.61934058e-05 -1.38507113e-04  5.95361227e-05 ...  6.51942240e-05\n",
            "    1.53622152e-04  6.89192116e-05]\n",
            "  [-1.86603726e-05 -1.96169652e-04  2.03437600e-04 ...  1.71464344e-05\n",
            "    3.52507900e-05  3.69571894e-05]\n",
            "  ...\n",
            "  [ 1.72564993e-06  1.51953474e-05  2.86596222e-05 ...  3.47466208e-06\n",
            "    1.00714783e-05 -2.02361890e-05]\n",
            "  [-3.35509685e-06 -1.63407705e-06  7.75312423e-06 ...  1.14541221e-05\n",
            "   -1.17492245e-05 -1.27392297e-06]\n",
            "  [-6.57310011e-08  9.52500384e-06 -2.84868572e-05 ...  5.50190220e-06\n",
            "    4.73049731e-06  4.27742692e-06]]]\n",
            "hello 0\n",
            "hello 1\n",
            "hello 2\n",
            "ScoreCAM completed\n",
            "tensor([0.9943], grad_fn=<ExpBackward>)\n",
            "tensor([0.3052], grad_fn=<ExpBackward>)\n",
            "tensor([0.6891], grad_fn=<SubBackward0>)\n",
            "tensor([0.6931], grad_fn=<DivBackward0>)\n",
            "tensor([0.3484], grad_fn=<ExpBackward>)\n",
            "tensor([0.6459], grad_fn=<SubBackward0>)\n",
            "tensor([0.6496], grad_fn=<DivBackward0>)\n",
            "Image Path-->/gdrive/My Drive/Chessman-image-dataset/Chess_test/Knight/00000019.jpg\n",
            "could not transform PIL_img to a PIL Image object. Please check input.\n",
            "torch.return_types.max(\n",
            "values=tensor([-0.0172], grad_fn=<MaxBackward0>),\n",
            "indices=tensor([2]))\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:741: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:775: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[[ 5.11550158e-05 -9.49670468e-05 -1.25665337e-03 ... -2.45639067e-04\n",
            "   -3.21853012e-04  5.09868329e-05]\n",
            "  [ 1.70803741e-04  4.76443619e-04 -1.51740253e-03 ... -1.17080826e-04\n",
            "   -2.19041836e-04  4.23925649e-05]\n",
            "  [ 2.24705935e-04 -4.99133617e-04 -3.21029499e-04 ...  1.62459929e-04\n",
            "   -3.93775152e-05  1.20200552e-04]\n",
            "  ...\n",
            "  [ 1.77239440e-05 -1.62930274e-05 -6.66950177e-05 ...  3.42871761e-05\n",
            "    1.17185153e-04  6.53508445e-05]\n",
            "  [ 4.47176350e-05 -7.91582162e-06 -4.96503152e-05 ...  2.86017777e-06\n",
            "   -5.49991615e-05  5.44471014e-05]\n",
            "  [ 4.39914037e-05  1.44058676e-05 -1.12816296e-05 ...  6.25496171e-05\n",
            "   -3.34684970e-05 -1.31835078e-05]]\n",
            "\n",
            " [[-5.65824844e-05  3.15416791e-04 -4.82629128e-04 ... -1.55089051e-04\n",
            "   -2.06883345e-04  3.42496149e-04]\n",
            "  [-1.08851744e-04  1.05381206e-03 -7.67267048e-04 ... -6.57847803e-05\n",
            "   -9.05569829e-05  2.99809501e-04]\n",
            "  [-4.30493243e-04 -8.71860459e-04  6.54050633e-04 ...  1.32434070e-04\n",
            "   -9.99806169e-05  1.94027871e-04]\n",
            "  ...\n",
            "  [ 2.05789856e-05  1.31105748e-05  1.90990546e-05 ... -1.61879919e-04\n",
            "    1.34366583e-04  7.43111735e-05]\n",
            "  [ 5.10644028e-05  3.04883393e-05 -3.98741104e-05 ... -2.25382508e-05\n",
            "   -7.42584094e-05  1.02197183e-04]\n",
            "  [ 4.17887233e-05  2.01789616e-05  5.64665883e-06 ...  1.46032386e-04\n",
            "   -5.01997117e-05  2.07476574e-05]]\n",
            "\n",
            " [[-4.89735976e-07  1.78378150e-04  4.51019816e-04 ... -1.03048394e-04\n",
            "   -7.20376614e-05  4.21524374e-05]\n",
            "  [-7.07355794e-05  3.79311293e-04  2.66162120e-04 ... -6.34863181e-05\n",
            "    1.45777944e-05  1.65357552e-04]\n",
            "  [-3.38094458e-04 -2.19301954e-04  4.66059186e-04 ... -4.89456905e-05\n",
            "    4.35583433e-05  1.03343977e-04]\n",
            "  ...\n",
            "  [-1.11993635e-06 -4.35610069e-06 -3.32627329e-06 ... -1.44152865e-04\n",
            "   -3.37998685e-05 -5.49534615e-05]\n",
            "  [ 5.50672412e-06 -1.09601906e-05 -4.44408227e-05 ... -8.92510265e-05\n",
            "   -9.51754674e-05 -3.76990298e-05]\n",
            "  [ 2.77581130e-06 -2.28055730e-05 -1.45541516e-05 ...  2.88315979e-05\n",
            "   -4.62692743e-05 -1.50792743e-05]]]\n",
            "hello 0\n",
            "hello 1\n",
            "hello 2\n",
            "ScoreCAM completed\n",
            "tensor([0.9830], grad_fn=<ExpBackward>)\n",
            "tensor([0.6261], grad_fn=<ExpBackward>)\n",
            "tensor([0.3568], grad_fn=<SubBackward0>)\n",
            "tensor([0.3630], grad_fn=<DivBackward0>)\n",
            "tensor([0.6642], grad_fn=<ExpBackward>)\n",
            "tensor([0.3187], grad_fn=<SubBackward0>)\n",
            "tensor([0.3243], grad_fn=<DivBackward0>)\n",
            "Image Path-->/gdrive/My Drive/Chessman-image-dataset/Chess_test/Knight/00000009.jpg\n",
            "could not transform PIL_img to a PIL Image object. Please check input.\n",
            "torch.return_types.max(\n",
            "values=tensor([-0.0182], grad_fn=<MaxBackward0>),\n",
            "indices=tensor([2]))\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:741: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:775: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[[ 1.55471591e-05 -2.87712459e-05 -3.36042466e-05 ...  6.64435625e-04\n",
            "    1.30625041e-04  3.16853300e-04]\n",
            "  [ 1.29044708e-05 -3.28130694e-05 -6.68402470e-06 ...  8.65186006e-04\n",
            "   -1.41108045e-04  1.33057963e-05]\n",
            "  [-1.54910830e-05 -5.58503787e-06  2.67279241e-05 ...  1.56315975e-04\n",
            "   -4.20065336e-04 -3.45223001e-06]\n",
            "  ...\n",
            "  [ 1.50669494e-05  2.04634704e-04  1.62209310e-04 ... -3.44567141e-05\n",
            "   -8.70927004e-06  1.60281139e-05]\n",
            "  [ 1.61205116e-05 -3.84120503e-07  6.66516740e-05 ... -2.30727543e-06\n",
            "   -1.34516810e-05 -1.37888489e-05]\n",
            "  [ 3.30676907e-05  1.40160962e-05 -1.69697276e-05 ... -2.95120873e-05\n",
            "   -1.27489178e-05  9.38717858e-07]]\n",
            "\n",
            " [[ 3.74968071e-05 -2.15906696e-05 -4.64033335e-05 ...  2.89742835e-04\n",
            "   -1.85937546e-04 -5.72147267e-05]\n",
            "  [ 1.58739742e-05 -3.55528039e-05 -2.09887093e-05 ...  4.15695086e-04\n",
            "   -3.72835174e-04 -1.14747491e-04]\n",
            "  [-2.99832737e-05 -2.71025300e-05  2.52261572e-05 ... -4.61817458e-04\n",
            "   -4.81435433e-04  2.92652324e-04]\n",
            "  ...\n",
            "  [-7.00735301e-05  1.88112184e-04  7.68268155e-05 ... -2.53355852e-05\n",
            "   -1.06506399e-05  3.72516410e-05]\n",
            "  [-6.11233758e-05 -1.07365046e-04 -6.22591469e-05 ...  4.84967744e-05\n",
            "    2.18415027e-05  1.70696713e-05]\n",
            "  [-2.02490180e-05 -1.79278315e-05 -1.37233902e-04 ... -3.30270850e-06\n",
            "    7.16426177e-06  2.37205415e-05]]\n",
            "\n",
            " [[ 3.13545554e-05  1.44628691e-05  8.06150958e-06 ...  5.18292002e-05\n",
            "   -1.35743795e-04 -7.74900150e-05]\n",
            "  [ 2.35711737e-05 -1.28975575e-06  1.09057035e-05 ...  5.18911844e-05\n",
            "   -1.13117099e-04  5.25886239e-05]\n",
            "  [ 3.32772557e-06  1.60258845e-05  1.81003218e-05 ... -6.75992668e-05\n",
            "    8.70880205e-05  2.79806964e-04]\n",
            "  ...\n",
            "  [-4.23785113e-05  3.16177518e-05  3.19154793e-05 ... -2.08723336e-05\n",
            "   -2.78222049e-05 -7.91839382e-06]\n",
            "  [-2.77644955e-05 -1.82398758e-05  7.92301493e-06 ...  6.47863373e-06\n",
            "   -2.55109306e-06 -1.08893924e-06]\n",
            "  [-1.20981783e-05  7.33271008e-06 -5.37539972e-05 ...  2.44986091e-06\n",
            "   -7.47499944e-07  4.03218786e-06]]]\n",
            "hello 0\n",
            "hello 1\n",
            "hello 2\n",
            "ScoreCAM completed\n",
            "tensor([0.9820], grad_fn=<ExpBackward>)\n",
            "tensor([0.5578], grad_fn=<ExpBackward>)\n",
            "tensor([0.4242], grad_fn=<SubBackward0>)\n",
            "tensor([0.4319], grad_fn=<DivBackward0>)\n",
            "tensor([0.7613], grad_fn=<ExpBackward>)\n",
            "tensor([0.2207], grad_fn=<SubBackward0>)\n",
            "tensor([0.2248], grad_fn=<DivBackward0>)\n",
            "Image Path-->/gdrive/My Drive/Chessman-image-dataset/Chess_test/Knight/00000006.jpg\n",
            "could not transform PIL_img to a PIL Image object. Please check input.\n",
            "torch.return_types.max(\n",
            "values=tensor([-0.0454], grad_fn=<MaxBackward0>),\n",
            "indices=tensor([2]))\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:741: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:775: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[[ 1.18135344e-04  7.63468444e-05 -1.90175939e-04 ... -1.35877561e-04\n",
            "   -3.02708661e-05 -2.25869976e-04]\n",
            "  [ 4.82496107e-05  9.17133316e-05 -1.68858171e-04 ... -3.19239348e-04\n",
            "    9.11930762e-05 -9.61707346e-05]\n",
            "  [ 2.94358982e-05 -5.96504938e-05 -2.40144227e-04 ...  4.58918465e-05\n",
            "   -1.02629941e-04 -1.75054336e-05]\n",
            "  ...\n",
            "  [-5.59552805e-06  3.44987144e-06  9.38973157e-06 ... -1.46940164e-05\n",
            "    7.38684321e-06  1.49648124e-05]\n",
            "  [ 7.89466081e-06  5.81672066e-06 -2.53221090e-05 ... -4.61787218e-05\n",
            "    1.48890074e-05  5.47487580e-07]\n",
            "  [ 9.89132444e-06  1.59444241e-05 -6.28144131e-06 ... -1.29139272e-05\n",
            "    2.11670669e-05 -5.96372294e-06]]\n",
            "\n",
            " [[ 7.83343054e-05  4.65918845e-05 -9.97270271e-05 ...  1.15841739e-04\n",
            "    4.67425063e-04  6.92765648e-05]\n",
            "  [-9.48630460e-05  4.01352439e-05  2.79076793e-05 ... -1.58691052e-04\n",
            "    4.88602668e-04  1.50578646e-04]\n",
            "  [-1.09503921e-04 -6.01230422e-05  4.61468659e-05 ...  3.93000897e-05\n",
            "   -1.77168045e-04  1.74843613e-05]\n",
            "  ...\n",
            "  [ 9.96928575e-07  6.66031905e-06  1.99772837e-05 ... -1.12409389e-05\n",
            "   -9.74957948e-06  4.33488167e-06]\n",
            "  [ 8.08123732e-06  1.27692334e-05 -4.34733322e-05 ... -7.79789826e-05\n",
            "    2.01781909e-05  4.28545725e-06]\n",
            "  [ 8.55471706e-06  1.19815650e-05 -8.82116146e-06 ... -3.71020986e-06\n",
            "    3.41991591e-05 -6.61571161e-06]]\n",
            "\n",
            " [[ 8.86037829e-06  2.93227797e-05  6.35020249e-05 ...  1.24305850e-04\n",
            "    2.98433863e-04  1.72755904e-04]\n",
            "  [-1.21043622e-05  2.70095933e-05  1.29001699e-04 ...  2.56767496e-06\n",
            "    2.57772878e-04  1.50796147e-04]\n",
            "  [ 2.79799104e-05  1.23527730e-04  2.66359802e-04 ...  6.55180030e-05\n",
            "    4.11356427e-05  8.00469331e-05]\n",
            "  ...\n",
            "  [-2.28270248e-06 -1.01764849e-05 -1.74385274e-05 ... -1.36428396e-06\n",
            "   -9.20395367e-06  6.54216274e-08]\n",
            "  [-1.44661914e-06 -5.05166245e-06 -2.77303625e-05 ... -1.11334817e-05\n",
            "    1.21379562e-05  4.92854393e-06]\n",
            "  [-2.00086788e-08 -1.25855452e-05 -6.67066546e-06 ...  9.72519978e-06\n",
            "    1.20146584e-05 -3.31993186e-06]]]\n",
            "hello 0\n",
            "hello 1\n",
            "hello 2\n",
            "ScoreCAM completed\n",
            "tensor([0.9556], grad_fn=<ExpBackward>)\n",
            "tensor([0.3854], grad_fn=<ExpBackward>)\n",
            "tensor([0.5702], grad_fn=<SubBackward0>)\n",
            "tensor([0.5967], grad_fn=<DivBackward0>)\n",
            "tensor([0.7376], grad_fn=<ExpBackward>)\n",
            "tensor([0.2181], grad_fn=<SubBackward0>)\n",
            "tensor([0.2282], grad_fn=<DivBackward0>)\n",
            "Image Path-->/gdrive/My Drive/Chessman-image-dataset/Chess_test/Knight/00000013.jpg\n",
            "could not transform PIL_img to a PIL Image object. Please check input.\n",
            "torch.return_types.max(\n",
            "values=tensor([-0.0004], grad_fn=<MaxBackward0>),\n",
            "indices=tensor([2]))\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:741: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:775: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[[ 2.08014343e-05  1.33746001e-05 -3.45242256e-05 ... -8.01031664e-05\n",
            "   -6.97518140e-05 -3.45854089e-05]\n",
            "  [-1.63180940e-05 -1.49491127e-05  3.00847925e-06 ... -6.60793856e-06\n",
            "   -7.45008048e-05 -7.05367792e-05]\n",
            "  [-6.30759750e-06  2.01437157e-06  2.19204836e-05 ...  4.94374475e-05\n",
            "   -9.07181203e-05 -8.36178544e-06]\n",
            "  ...\n",
            "  [-1.29535189e-05 -2.03871378e-05  6.30894024e-05 ... -7.94447493e-06\n",
            "   -1.29079982e-05 -1.62340072e-05]\n",
            "  [-6.17485377e-06  1.56471494e-05  2.64937710e-05 ... -8.87089176e-06\n",
            "    2.08059535e-05  8.99801147e-06]\n",
            "  [-5.34030900e-06 -3.88604007e-06  1.47526723e-05 ...  1.38341368e-05\n",
            "    1.09429145e-05 -3.82781436e-06]]\n",
            "\n",
            " [[ 3.10597336e-05  1.58877485e-05 -5.19472035e-05 ... -1.02462731e-04\n",
            "    5.17476350e-05  4.21284558e-05]\n",
            "  [-2.39244569e-05 -1.80995697e-05  6.82884641e-05 ...  7.16254860e-05\n",
            "    4.64421464e-05  3.28907184e-05]\n",
            "  [-9.45585663e-06  2.64643016e-05  1.04124695e-04 ...  7.17314985e-05\n",
            "   -1.51494872e-04 -8.92909593e-06]\n",
            "  ...\n",
            "  [-1.42422947e-05 -5.11350995e-05 -2.73247040e-05 ...  4.19383124e-05\n",
            "    2.89779459e-05  1.25395809e-05]\n",
            "  [ 7.47981016e-06 -1.06986170e-05 -1.13952125e-05 ...  8.89665156e-06\n",
            "    4.05518711e-05  1.51497277e-05]\n",
            "  [ 6.19292259e-06  9.48419038e-06 -1.10622589e-05 ...  1.03528053e-05\n",
            "    1.33571704e-05  6.30541588e-06]]\n",
            "\n",
            " [[ 3.61888815e-06 -5.98682673e-06 -2.75552692e-05 ... -2.01301090e-05\n",
            "    5.04708989e-05  1.91710657e-05]\n",
            "  [-1.00879534e-05 -1.86110928e-05 -9.06876521e-07 ...  4.96230647e-05\n",
            "    4.46907384e-05  3.98971839e-05]\n",
            "  [-4.28595813e-06  1.96162058e-06  3.23498761e-05 ...  1.11665227e-05\n",
            "   -5.09895105e-05 -9.83128557e-06]\n",
            "  ...\n",
            "  [-7.30535132e-06  1.19956722e-05  7.26345694e-06 ...  6.38256141e-06\n",
            "   -8.89099087e-06 -1.70048338e-05]\n",
            "  [-1.27351697e-06  9.91064240e-06  3.08371289e-05 ... -1.69835612e-05\n",
            "   -1.63947174e-05 -1.88771635e-05]\n",
            "  [ 2.52795202e-06  2.29024747e-05  1.34097529e-05 ... -1.33409945e-05\n",
            "   -1.25084398e-05 -2.46691197e-06]]]\n",
            "hello 0\n",
            "hello 1\n",
            "hello 2\n",
            "ScoreCAM completed\n",
            "tensor([0.9996], grad_fn=<ExpBackward>)\n",
            "tensor([0.5269], grad_fn=<ExpBackward>)\n",
            "tensor([0.4728], grad_fn=<SubBackward0>)\n",
            "tensor([0.4729], grad_fn=<DivBackward0>)\n",
            "tensor([0.6441], grad_fn=<ExpBackward>)\n",
            "tensor([0.3555], grad_fn=<SubBackward0>)\n",
            "tensor([0.3556], grad_fn=<DivBackward0>)\n",
            "Image Path-->/gdrive/My Drive/Chessman-image-dataset/Chess_test/Knight/00000008.jpg\n",
            "could not transform PIL_img to a PIL Image object. Please check input.\n",
            "torch.return_types.max(\n",
            "values=tensor([-0.2210], grad_fn=<MaxBackward0>),\n",
            "indices=tensor([2]))\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:741: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:775: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[[-4.49568871e-05  1.93712616e-05 -1.59864419e-05 ...  5.24332142e-05\n",
            "    6.79864036e-05  2.67860456e-05]\n",
            "  [-4.42451565e-05  6.53906586e-05 -1.58186318e-06 ... -2.61967652e-06\n",
            "    3.88123817e-05  4.12846822e-05]\n",
            "  [ 9.66084655e-05  2.01583607e-06  1.33276414e-04 ...  5.24939410e-05\n",
            "   -3.17566795e-05  3.60242149e-06]\n",
            "  ...\n",
            "  [-5.11380238e-05 -9.23364796e-05 -5.18074306e-05 ...  3.62016098e-05\n",
            "    3.05392826e-05  2.20290315e-05]\n",
            "  [-2.53986055e-05 -6.46035071e-06 -1.07664685e-05 ...  2.82002310e-05\n",
            "    6.11834810e-06  1.52548053e-05]\n",
            "  [-1.84543664e-05 -9.63135739e-06 -6.78158540e-06 ...  9.09788127e-06\n",
            "    8.69380659e-06  2.47427728e-05]]\n",
            "\n",
            " [[-3.49089527e-05  7.64506636e-05 -8.57537799e-05 ...  7.45509169e-06\n",
            "    5.18244039e-05 -5.92568656e-06]\n",
            "  [-3.89868161e-05  1.10412883e-04 -1.56666972e-04 ... -3.09383264e-05\n",
            "   -1.10088114e-05  9.50048154e-06]\n",
            "  [ 1.25612142e-04 -2.14571133e-04 -1.44189494e-04 ...  1.09981503e-04\n",
            "   -6.83539361e-05 -7.61765288e-06]\n",
            "  ...\n",
            "  [ 4.24736226e-05  1.70755514e-05 -3.18968669e-05 ... -2.23059068e-05\n",
            "   -3.75306327e-05 -3.87366954e-05]\n",
            "  [ 3.18215531e-05  9.79821105e-05  3.09780706e-05 ... -5.53842960e-05\n",
            "   -6.44995086e-05 -3.70476884e-05]\n",
            "  [ 1.39456824e-05  3.05552827e-05  1.06003101e-05 ... -2.73433607e-05\n",
            "   -1.85068627e-05 -8.90360097e-06]]\n",
            "\n",
            " [[ 4.24193568e-06  3.00981756e-05 -3.23019456e-05 ... -1.47919869e-05\n",
            "   -2.17124051e-05 -2.70818966e-05]\n",
            "  [-1.18193647e-05  4.10055090e-05 -6.24335976e-05 ... -6.15779310e-06\n",
            "   -2.15509045e-05 -1.94959622e-05]\n",
            "  [ 5.28538716e-06 -8.89108144e-05 -1.40686333e-04 ...  3.42136272e-05\n",
            "   -1.83665077e-05 -6.34431781e-07]\n",
            "  ...\n",
            "  [ 1.35098654e-05  3.21143446e-05 -1.33136928e-05 ...  1.17264420e-05\n",
            "    2.23409268e-05  2.68059084e-05]\n",
            "  [-1.54783670e-05 -7.77366571e-06 -2.31057056e-05 ...  1.78520265e-05\n",
            "    2.14836840e-05  1.94361270e-05]\n",
            "  [-7.29612890e-06 -8.75244790e-06 -1.45092059e-05 ...  8.63326190e-06\n",
            "    1.13093690e-05  6.67554326e-06]]]\n",
            "hello 0\n",
            "hello 1\n",
            "hello 2\n",
            "ScoreCAM completed\n",
            "tensor([0.8017], grad_fn=<ExpBackward>)\n",
            "tensor([0.2899], grad_fn=<ExpBackward>)\n",
            "tensor([0.5118], grad_fn=<SubBackward0>)\n",
            "tensor([0.6383], grad_fn=<DivBackward0>)\n",
            "tensor([0.3562], grad_fn=<ExpBackward>)\n",
            "tensor([0.4455], grad_fn=<SubBackward0>)\n",
            "tensor([0.5557], grad_fn=<DivBackward0>)\n",
            "Image Path-->/gdrive/My Drive/Chessman-image-dataset/Chess_test/Knight/00000000.jpg\n",
            "could not transform PIL_img to a PIL Image object. Please check input.\n",
            "torch.return_types.max(\n",
            "values=tensor([-0.3496], grad_fn=<MaxBackward0>),\n",
            "indices=tensor([2]))\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:741: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:775: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[[-6.66900203e-04 -1.24840848e-04 -5.16348407e-04 ... -6.36519445e-05\n",
            "    6.91275671e-05  1.33497799e-04]\n",
            "  [-4.63351235e-05  1.51412740e-03 -1.19355388e-03 ...  1.86025840e-05\n",
            "    1.04816593e-04  1.76691860e-04]\n",
            "  [ 1.37908086e-04 -5.00510819e-04 -1.23870604e-03 ... -8.70442390e-05\n",
            "    1.21064410e-04  1.88691611e-05]\n",
            "  ...\n",
            "  [ 1.64509937e-05  3.36443144e-05 -1.45334762e-05 ...  5.01186121e-05\n",
            "   -1.81404948e-04 -6.70307176e-05]\n",
            "  [ 2.02612136e-05  4.90400009e-05  6.76241238e-06 ...  1.30809448e-05\n",
            "   -1.61029063e-04 -1.08858775e-04]\n",
            "  [-4.70221159e-06 -6.38309226e-06  1.53107452e-05 ... -1.21646794e-05\n",
            "   -7.75969075e-05  2.77279702e-06]]\n",
            "\n",
            " [[-1.50082216e-03  2.63443440e-04  5.36622331e-04 ...  2.11962573e-04\n",
            "   -1.61370300e-04  8.71202350e-05]\n",
            "  [-4.87284772e-04  2.31093213e-03 -4.67684641e-04 ...  3.77282724e-04\n",
            "   -4.16084751e-05  1.57506019e-04]\n",
            "  [-8.15845579e-04 -1.20243214e-03 -1.76654488e-03 ... -5.14287595e-05\n",
            "    6.70839474e-05 -2.87240557e-05]\n",
            "  ...\n",
            "  [-1.99433719e-05  1.07034796e-05 -3.84688750e-05 ...  1.58356801e-04\n",
            "   -8.66129715e-05  1.19269891e-04]\n",
            "  [ 3.57060344e-06  2.81682028e-05 -4.70314128e-05 ...  9.23809037e-05\n",
            "   -1.03776511e-04  2.57216278e-05]\n",
            "  [-3.42173874e-05 -5.24922088e-05 -5.73373865e-06 ...  1.25849142e-05\n",
            "    2.53075175e-05  8.81281681e-05]]\n",
            "\n",
            " [[-5.62152527e-04 -1.32937375e-04  3.01104710e-04 ...  3.45415832e-05\n",
            "   -1.84236579e-04 -6.15090178e-05]\n",
            "  [-2.70190388e-04  3.88253555e-04  1.37338061e-04 ...  1.35354958e-04\n",
            "   -1.34276124e-04 -3.74925137e-05]\n",
            "  [-6.00551069e-04 -6.45085871e-04 -5.58188893e-04 ... -5.72298886e-05\n",
            "   -2.91659497e-05 -1.84860535e-05]\n",
            "  ...\n",
            "  [-1.25623168e-05 -1.10436440e-05  1.60503970e-05 ...  8.16098694e-05\n",
            "    7.09094480e-05  1.24520771e-04]\n",
            "  [-4.35207388e-06  1.89282093e-05  3.81267513e-06 ...  1.12533420e-04\n",
            "    7.44643947e-05  1.08362185e-04]\n",
            "  [-1.56962022e-05 -1.33203249e-05  4.27963503e-06 ...  1.90367003e-05\n",
            "    6.25619106e-05  5.27501153e-05]]]\n",
            "hello 0\n",
            "hello 1\n",
            "hello 2\n",
            "ScoreCAM completed\n",
            "tensor([0.7050], grad_fn=<ExpBackward>)\n",
            "tensor([0.4825], grad_fn=<ExpBackward>)\n",
            "tensor([0.2225], grad_fn=<SubBackward0>)\n",
            "tensor([0.3155], grad_fn=<DivBackward0>)\n",
            "tensor([0.6027], grad_fn=<ExpBackward>)\n",
            "tensor([0.1023], grad_fn=<SubBackward0>)\n",
            "tensor([0.1451], grad_fn=<DivBackward0>)\n",
            "Image Path-->/gdrive/My Drive/Chessman-image-dataset/Chess_test/Knight/00000018.jpg\n",
            "could not transform PIL_img to a PIL Image object. Please check input.\n",
            "torch.return_types.max(\n",
            "values=tensor([-0.4982], grad_fn=<MaxBackward0>),\n",
            "indices=tensor([2]))\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:741: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:775: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[[ 9.55885183e-05 -1.84483989e-05 -1.29772853e-04 ... -1.22627290e-05\n",
            "    3.16328648e-05  8.74914415e-05]\n",
            "  [ 4.28675488e-05  3.60486396e-04 -4.01407387e-05 ...  1.54926572e-04\n",
            "    4.37448267e-06  1.32783856e-04]\n",
            "  [ 4.63611726e-05  1.26318010e-04  1.60583034e-04 ...  9.09201987e-05\n",
            "   -2.42949487e-05  1.73885946e-05]\n",
            "  ...\n",
            "  [ 5.74436388e-06  1.74277788e-05  3.81771009e-05 ...  1.44790160e-06\n",
            "    6.86573156e-06  1.36930821e-05]\n",
            "  [ 8.97726160e-06  7.18678697e-06  3.37492255e-05 ...  1.08968385e-05\n",
            "    1.59979844e-05 -9.94794304e-06]\n",
            "  [ 6.64675725e-06 -4.55104338e-06  2.25211680e-05 ... -5.16194850e-06\n",
            "   -3.42150161e-06  4.12839698e-06]]\n",
            "\n",
            " [[ 1.52147422e-04  2.04391102e-05 -1.15948142e-04 ... -1.87507120e-05\n",
            "   -2.00143196e-04  9.82741127e-06]\n",
            "  [ 3.42901587e-05  4.17526811e-04  5.88991679e-08 ...  1.32475868e-04\n",
            "   -2.18993220e-04  2.64362025e-06]\n",
            "  [-2.43384973e-05 -2.17525708e-05  1.12055214e-04 ...  6.28179079e-05\n",
            "   -1.20669277e-04 -2.65797251e-05]\n",
            "  ...\n",
            "  [-1.38283800e-05 -1.27039384e-06  1.99717446e-05 ... -2.14429223e-05\n",
            "    6.18114951e-06  2.88413779e-06]\n",
            "  [-1.05339289e-05 -3.94711550e-05  6.43666892e-06 ...  1.30121270e-05\n",
            "    3.36307986e-05 -1.74170523e-05]\n",
            "  [-8.93454126e-06 -3.52776516e-05  7.63764721e-06 ... -1.24934409e-05\n",
            "   -9.49639478e-06  1.17320695e-05]]\n",
            "\n",
            " [[ 6.66693412e-05  2.69545522e-05  5.67353982e-05 ...  1.21661392e-05\n",
            "   -7.51912035e-05 -7.20442185e-07]\n",
            "  [ 2.57211411e-05  1.91475935e-04 -1.04351947e-05 ...  5.52699901e-05\n",
            "   -7.35892728e-05 -3.52642732e-05]\n",
            "  [-7.38825765e-07 -1.07631069e-04 -2.47615646e-05 ...  4.04358236e-05\n",
            "    2.33879918e-05  3.79300909e-05]\n",
            "  ...\n",
            "  [ 1.06090418e-06  6.46507950e-06  2.06401572e-05 ... -6.50284463e-06\n",
            "   -8.32111051e-06 -7.24057783e-06]\n",
            "  [-6.41467122e-07  6.73409959e-06  2.73890048e-05 ... -2.94691155e-06\n",
            "    4.39750147e-06 -9.80786979e-06]\n",
            "  [-2.34245308e-06 -3.35199642e-06  1.17275747e-05 ... -1.01268955e-05\n",
            "   -6.23789267e-06  3.38801707e-06]]]\n",
            "hello 0\n",
            "hello 1\n",
            "hello 2\n",
            "ScoreCAM completed\n",
            "tensor([0.6076], grad_fn=<ExpBackward>)\n",
            "tensor([0.6844], grad_fn=<ExpBackward>)\n",
            "tensor([-0.0768], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<MulBackward0>)\n",
            "tensor([0.8336], grad_fn=<ExpBackward>)\n",
            "tensor([-0.2259], grad_fn=<SubBackward0>)\n",
            "tensor([0.], grad_fn=<MulBackward0>)\n",
            "Image Path-->/gdrive/My Drive/Chessman-image-dataset/Chess_test/Knight/00000021.jpg\n",
            "could not transform PIL_img to a PIL Image object. Please check input.\n",
            "torch.return_types.max(\n",
            "values=tensor([-0.0025], grad_fn=<MaxBackward0>),\n",
            "indices=tensor([2]))\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:741: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:775: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[[ 2.24838825e-05  6.73388422e-06  4.47468273e-05 ...  2.22202670e-05\n",
            "    5.66312065e-05  6.58362033e-05]\n",
            "  [ 1.91879109e-05 -1.15650501e-04  2.44622910e-05 ...  3.56187019e-05\n",
            "    3.01271328e-06 -8.31047073e-07]\n",
            "  [ 4.26445855e-05  9.83992498e-05 -9.17381607e-05 ...  6.79689739e-05\n",
            "    1.90380961e-05 -1.43762166e-05]\n",
            "  ...\n",
            "  [-1.34625798e-05 -1.61653268e-05  1.40037807e-05 ...  2.99409265e-06\n",
            "    2.16403254e-05  4.15389240e-05]\n",
            "  [-7.07584783e-06  1.18244044e-05  4.01170971e-05 ...  1.47280423e-05\n",
            "    4.66564670e-06 -1.32389483e-06]\n",
            "  [-1.22196076e-05 -1.41223206e-05  7.32988818e-06 ...  8.82327848e-06\n",
            "    3.54947173e-06  3.52648785e-07]]\n",
            "\n",
            " [[ 2.54423497e-05  7.00338837e-05  2.19288766e-04 ... -2.13220832e-06\n",
            "    2.91352649e-05  4.99544293e-05]\n",
            "  [ 2.18884437e-06 -1.67495999e-04  9.20608733e-05 ... -1.33302272e-05\n",
            "   -7.81310629e-05 -7.15776812e-05]\n",
            "  [ 4.10940964e-05  3.54430219e-05 -2.65632905e-04 ...  2.40411889e-05\n",
            "   -8.29908531e-07 -5.41278860e-05]\n",
            "  ...\n",
            "  [-1.81896193e-05 -3.52557516e-05  2.83412309e-05 ...  5.61447116e-06\n",
            "   -3.95689672e-05 -9.21387458e-06]\n",
            "  [-4.63524484e-06  1.17794890e-05  3.93517967e-05 ... -8.63702502e-06\n",
            "   -3.65925580e-05 -4.21408145e-05]\n",
            "  [-1.41404662e-05 -3.61117302e-05 -1.99641194e-05 ... -2.19002482e-06\n",
            "   -1.82328792e-05 -2.09811237e-05]]\n",
            "\n",
            " [[-1.37773808e-05 -3.04993312e-05  3.95718496e-05 ...  2.01279705e-06\n",
            "    1.37690455e-06  3.33478907e-06]\n",
            "  [-1.98667916e-05 -5.96898934e-05  1.98117131e-05 ...  6.49468100e-06\n",
            "   -2.10411008e-05 -7.43013341e-06]\n",
            "  [-2.94062844e-05 -3.15175904e-05 -1.14425104e-04 ...  1.21387187e-05\n",
            "    2.89286836e-06 -1.47548970e-05]\n",
            "  ...\n",
            "  [-4.01084195e-06 -7.44964927e-06  1.85077218e-05 ...  4.28498071e-05\n",
            "    3.35768098e-05  2.14063213e-06]\n",
            "  [-1.36264411e-06 -2.51825317e-06 -7.20912241e-06 ...  3.76280351e-05\n",
            "    2.58459826e-05  1.77454879e-05]\n",
            "  [-2.28095247e-06 -8.92837299e-06 -1.89343654e-05 ...  1.99580728e-05\n",
            "    1.23553618e-05  1.02446380e-05]]]\n",
            "hello 0\n",
            "hello 1\n",
            "hello 2\n",
            "ScoreCAM completed\n",
            "tensor([0.9975], grad_fn=<ExpBackward>)\n",
            "tensor([0.7506], grad_fn=<ExpBackward>)\n",
            "tensor([0.2469], grad_fn=<SubBackward0>)\n",
            "tensor([0.2475], grad_fn=<DivBackward0>)\n",
            "tensor([0.3970], grad_fn=<ExpBackward>)\n",
            "tensor([0.6005], grad_fn=<SubBackward0>)\n",
            "tensor([0.6020], grad_fn=<DivBackward0>)\n",
            "Image Path-->/gdrive/My Drive/Chessman-image-dataset/Chess_test/Knight/00000023.jpg\n",
            "could not transform PIL_img to a PIL Image object. Please check input.\n",
            "torch.return_types.max(\n",
            "values=tensor([-0.2183], grad_fn=<MaxBackward0>),\n",
            "indices=tensor([2]))\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:741: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:775: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[[-1.39193994e-05 -4.58851922e-05  1.42220606e-05 ... -3.61561147e-05\n",
            "   -5.62541140e-05  2.21844809e-05]\n",
            "  [ 5.77605446e-06 -9.27400310e-05 -5.22577902e-05 ... -5.48829045e-05\n",
            "   -1.35133760e-04  6.86090672e-05]\n",
            "  [-5.67897223e-08  2.34906632e-05  3.86086130e-05 ... -1.12297237e-04\n",
            "    1.67605281e-05 -3.03255022e-05]\n",
            "  ...\n",
            "  [ 1.97862368e-05 -1.18296384e-05  2.47596693e-05 ... -1.83311757e-05\n",
            "    9.63744824e-07  1.68844091e-05]\n",
            "  [-1.54717721e-05 -2.23030359e-05 -3.57580953e-06 ...  5.65137423e-06\n",
            "    1.22928550e-05  1.03067060e-05]\n",
            "  [-1.63670327e-05 -1.69499777e-05  3.77414224e-06 ...  1.10553624e-05\n",
            "    9.85545106e-06 -3.05872818e-07]]\n",
            "\n",
            " [[-4.20910725e-05 -1.13649573e-04  3.19438870e-05 ...  2.46356707e-05\n",
            "   -2.77032051e-05  1.67579651e-04]\n",
            "  [-1.46049540e-06 -1.40131172e-04 -7.15288473e-05 ...  2.18049181e-05\n",
            "   -1.50506068e-04  1.44186663e-04]\n",
            "  [ 8.18945118e-06  9.81281791e-05  1.38346581e-04 ... -3.12337186e-05\n",
            "    1.83725860e-04  2.33202754e-05]\n",
            "  ...\n",
            "  [-9.32307448e-07 -3.64582334e-05  6.65598037e-05 ... -2.55413493e-05\n",
            "   -1.94912450e-05  6.98400836e-06]\n",
            "  [ 9.96424002e-06  5.69458026e-05  9.56736878e-05 ... -7.29506253e-06\n",
            "   -1.52165885e-07 -2.96811282e-06]\n",
            "  [-5.50735975e-06  1.57650036e-05  5.01092058e-05 ...  1.19955582e-05\n",
            "    1.05370797e-06 -9.85577935e-06]]\n",
            "\n",
            " [[-1.03130215e-05 -2.51195580e-05  1.30179338e-05 ... -3.57451383e-06\n",
            "    9.13950498e-07  5.78881707e-05]\n",
            "  [-2.72473320e-06 -2.15160800e-05 -1.88645907e-05 ...  4.79650125e-05\n",
            "   -2.53279158e-05  7.69827573e-06]\n",
            "  [-1.28790998e-07  1.16121164e-05  3.58864549e-05 ...  7.05081038e-06\n",
            "    1.06694158e-04  3.78233311e-05]\n",
            "  ...\n",
            "  [-2.07016664e-05 -4.26558452e-05 -4.63213865e-05 ...  6.35178061e-06\n",
            "   -4.31645720e-06  1.50985055e-06]\n",
            "  [-7.94026011e-06 -2.87226704e-05 -3.09656188e-05 ...  2.63855181e-06\n",
            "    2.55817460e-06  1.15457631e-06]\n",
            "  [-3.92299029e-06 -1.27523462e-05 -8.79646570e-06 ...  5.78811916e-06\n",
            "    1.81870331e-06  3.92963339e-07]]]\n",
            "hello 0\n",
            "hello 1\n",
            "hello 2\n",
            "ScoreCAM completed\n",
            "tensor([0.8039], grad_fn=<ExpBackward>)\n",
            "tensor([0.7393], grad_fn=<ExpBackward>)\n",
            "tensor([0.0645], grad_fn=<SubBackward0>)\n",
            "tensor([0.0803], grad_fn=<DivBackward0>)\n",
            "tensor([0.3283], grad_fn=<ExpBackward>)\n",
            "tensor([0.4756], grad_fn=<SubBackward0>)\n",
            "tensor([0.5916], grad_fn=<DivBackward0>)\n",
            "Image Path-->/gdrive/My Drive/Chessman-image-dataset/Chess_test/Knight/00000022.jpg\n",
            "could not transform PIL_img to a PIL Image object. Please check input.\n",
            "torch.return_types.max(\n",
            "values=tensor([-0.0176], grad_fn=<MaxBackward0>),\n",
            "indices=tensor([2]))\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:741: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:775: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[[-5.49426768e-07 -1.54227130e-04 -1.81048661e-04 ...  2.59289201e-04\n",
            "   -1.53723480e-04  5.97946625e-05]\n",
            "  [ 2.79018469e-05  2.75562890e-05 -9.17771086e-05 ...  4.80107814e-04\n",
            "   -2.16837656e-04 -1.41777075e-05]\n",
            "  [ 1.79571249e-04  1.99117959e-04  1.64482519e-04 ... -2.27016560e-05\n",
            "   -2.01959796e-04  6.16186578e-05]\n",
            "  ...\n",
            "  [-2.24199193e-05 -1.17380582e-04 -1.05444808e-04 ...  5.95517340e-06\n",
            "   -3.06416885e-05 -1.11594200e-04]\n",
            "  [-7.01362407e-05 -1.60157308e-05  3.08230240e-05 ... -4.39728843e-05\n",
            "   -2.65516038e-05 -4.25423821e-05]\n",
            "  [-4.71605686e-05 -1.11379195e-04  5.31659741e-05 ... -2.06430629e-05\n",
            "   -4.40878153e-06  3.67780449e-06]]\n",
            "\n",
            " [[ 4.55379440e-05 -9.37283132e-05  7.72689842e-05 ...  4.85441461e-05\n",
            "   -1.06206257e-04  1.87319051e-04]\n",
            "  [-8.61007720e-09  4.87099355e-05  3.60665168e-05 ...  6.29673824e-04\n",
            "   -1.45835220e-05  3.16634625e-04]\n",
            "  [ 7.88995437e-05  1.01247653e-04 -9.59434826e-05 ...  5.04921041e-04\n",
            "   -1.48767503e-04  2.89769731e-04]\n",
            "  ...\n",
            "  [ 1.99597608e-05 -1.06192753e-04 -1.09720947e-04 ...  2.19592755e-05\n",
            "    1.81093765e-05 -1.12218820e-04]\n",
            "  [ 4.14762209e-06  1.21847354e-04  4.50242171e-05 ... -8.24068300e-05\n",
            "    1.65831181e-05 -1.92150404e-05]\n",
            "  [ 7.06909399e-06  1.93994597e-05  4.84547624e-05 ... -7.90740829e-05\n",
            "    2.49086437e-05  5.87292667e-05]]\n",
            "\n",
            " [[ 9.58377146e-06  6.52522431e-07  1.38649624e-04 ... -1.96744874e-04\n",
            "   -5.04750293e-05 -2.18106154e-05]\n",
            "  [-2.60748179e-05  2.90666195e-05  1.07379537e-04 ... -1.02421530e-04\n",
            "   -9.03315283e-05 -8.46114825e-06]\n",
            "  [-2.86784209e-05 -3.05014662e-06 -1.09651955e-04 ... -1.08080162e-04\n",
            "   -2.60896925e-04 -9.53685585e-05]\n",
            "  ...\n",
            "  [ 1.13146077e-05 -1.32374163e-05  4.12230752e-05 ...  4.09711269e-06\n",
            "   -2.74158246e-05 -6.93253521e-05]\n",
            "  [ 1.29934098e-05  5.82990050e-05  4.13771719e-05 ... -6.42026495e-05\n",
            "   -3.45596112e-05 -4.39904677e-05]\n",
            "  [ 1.72048633e-05  7.28034927e-05  2.48801592e-05 ... -3.92622128e-05\n",
            "   -1.47417304e-05  7.27758044e-06]]]\n",
            "hello 0\n",
            "hello 1\n",
            "hello 2\n",
            "ScoreCAM completed\n",
            "tensor([0.9825], grad_fn=<ExpBackward>)\n",
            "tensor([0.5018], grad_fn=<ExpBackward>)\n",
            "tensor([0.4807], grad_fn=<SubBackward0>)\n",
            "tensor([0.4893], grad_fn=<DivBackward0>)\n",
            "tensor([0.8907], grad_fn=<ExpBackward>)\n",
            "tensor([0.0918], grad_fn=<SubBackward0>)\n",
            "tensor([0.0934], grad_fn=<DivBackward0>)\n",
            "Image Path-->/gdrive/My Drive/Chessman-image-dataset/Chess_test/Knight/00000017.jpg\n",
            "could not transform PIL_img to a PIL Image object. Please check input.\n",
            "torch.return_types.max(\n",
            "values=tensor([-0.0078], grad_fn=<MaxBackward0>),\n",
            "indices=tensor([2]))\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:741: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:775: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[[ 5.91052976e-05  1.27369529e-04  9.96129215e-05 ... -1.11715104e-04\n",
            "   -4.28468222e-05 -1.07411705e-04]\n",
            "  [ 8.04447755e-05  4.88319434e-06 -9.22745466e-05 ...  4.62389737e-04\n",
            "   -9.93668381e-05 -2.34345142e-04]\n",
            "  [ 5.32996142e-05 -1.59914456e-04 -1.94912944e-04 ... -1.16643971e-04\n",
            "   -1.53126772e-04 -5.64467488e-05]\n",
            "  ...\n",
            "  [ 2.07898626e-05  9.82978102e-05 -4.64318460e-05 ... -2.18414068e-04\n",
            "    1.21167572e-04 -2.24524084e-05]\n",
            "  [ 1.46120333e-05 -1.36888295e-05  1.34603016e-05 ... -5.22382418e-05\n",
            "   -1.05980169e-04 -1.42914732e-05]\n",
            "  [ 1.52881001e-05  5.13127656e-06  5.20853931e-05 ...  3.87536641e-05\n",
            "   -1.12844724e-05  5.50821982e-05]]\n",
            "\n",
            " [[ 4.62501822e-05  1.05858520e-04  1.00491159e-04 ... -7.67830992e-05\n",
            "    2.04404648e-04  4.91265533e-05]\n",
            "  [ 1.05372677e-04 -6.62111072e-05 -2.30391603e-04 ...  6.53737411e-04\n",
            "    1.18922545e-04 -6.11209869e-05]\n",
            "  [ 4.31013480e-05 -2.58854795e-04 -1.83159728e-04 ... -8.68303142e-05\n",
            "   -1.22706685e-04  9.43920761e-05]\n",
            "  ...\n",
            "  [ 5.07582154e-07  1.16311973e-04 -5.61680703e-06 ... -2.65531801e-04\n",
            "    2.74366215e-04  9.75148752e-06]\n",
            "  [-1.39688305e-05 -3.38965468e-05  7.74772046e-05 ... -1.03164022e-05\n",
            "   -1.93351004e-04 -3.25330719e-05]\n",
            "  [-7.65799312e-06 -1.22165773e-05  9.98500455e-05 ...  7.84880016e-05\n",
            "   -3.36152618e-05  1.35257430e-04]]\n",
            "\n",
            " [[ 7.27051578e-06  3.63474735e-05  9.48760193e-06 ... -1.21147642e-04\n",
            "   -1.92100997e-05 -3.28925857e-05]\n",
            "  [ 1.34724402e-05  1.61726028e-06 -8.19662679e-05 ...  5.69894444e-05\n",
            "   -1.28835309e-04 -1.06348339e-04]\n",
            "  [ 8.58764804e-06 -8.22001975e-05 -6.94284961e-05 ... -2.23415233e-04\n",
            "   -2.21328866e-04 -5.88347623e-05]\n",
            "  ...\n",
            "  [-1.65292691e-05  9.35912249e-06 -5.31435385e-05 ... -9.42355301e-05\n",
            "    4.10220027e-05 -4.80262097e-05]\n",
            "  [-1.02543936e-05 -5.36547648e-05 -3.72369960e-05 ... -3.40153556e-05\n",
            "   -1.09102596e-04 -5.74232265e-05]\n",
            "  [-6.57020253e-06 -1.37469487e-05  1.07781356e-05 ... -2.90969969e-06\n",
            "   -3.33702192e-05  2.39465921e-05]]]\n",
            "hello 0\n",
            "hello 1\n",
            "hello 2\n",
            "ScoreCAM completed\n",
            "tensor([0.9922], grad_fn=<ExpBackward>)\n",
            "tensor([0.3812], grad_fn=<ExpBackward>)\n",
            "tensor([0.6111], grad_fn=<SubBackward0>)\n",
            "tensor([0.6158], grad_fn=<DivBackward0>)\n",
            "tensor([0.7949], grad_fn=<ExpBackward>)\n",
            "tensor([0.1973], grad_fn=<SubBackward0>)\n",
            "tensor([0.1988], grad_fn=<DivBackward0>)\n",
            "Image Path-->/gdrive/My Drive/Chessman-image-dataset/Chess_test/Knight/00000012.jpg\n",
            "could not transform PIL_img to a PIL Image object. Please check input.\n",
            "torch.return_types.max(\n",
            "values=tensor([-0.0435], grad_fn=<MaxBackward0>),\n",
            "indices=tensor([2]))\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:741: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:775: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[[-1.32420138e-04 -1.03187263e-04  2.52680108e-04 ... -2.62152450e-05\n",
            "   -1.04847420e-04 -1.76808191e-05]\n",
            "  [-1.79182962e-04  1.75851379e-04  1.98169034e-04 ...  8.59639980e-05\n",
            "   -8.86823237e-05  7.64240697e-05]\n",
            "  [-2.32683755e-04 -4.27643768e-04  3.40323672e-04 ...  1.55762937e-04\n",
            "    1.12714563e-04  7.81216193e-05]\n",
            "  ...\n",
            "  [ 1.73047278e-05 -4.00424115e-04  5.73578477e-05 ...  3.87572288e-05\n",
            "    2.46669282e-05  3.16678174e-05]\n",
            "  [ 3.63652036e-05 -1.50811649e-05 -4.84314281e-05 ...  5.30574034e-07\n",
            "   -2.55853590e-05 -1.67587923e-05]\n",
            "  [ 4.97776200e-07 -2.08376208e-06 -2.01532012e-05 ... -2.65355664e-05\n",
            "    2.76139181e-06  6.92092173e-06]]\n",
            "\n",
            " [[-1.75170023e-04 -1.20314499e-04  2.68577281e-04 ... -1.55840581e-05\n",
            "   -1.30305225e-04  2.43744301e-05]\n",
            "  [-8.81555490e-05  4.30381261e-04  2.20440067e-04 ...  1.86444838e-04\n",
            "   -1.52490288e-04  1.31204799e-04]\n",
            "  [-1.80626325e-04 -5.38647287e-04  5.75431325e-04 ...  1.86093543e-04\n",
            "   -2.13963841e-05 -5.46267349e-05]\n",
            "  ...\n",
            "  [-1.51095691e-05 -5.37528619e-04  1.12609938e-04 ...  1.12199746e-04\n",
            "    2.87098368e-05  3.08417086e-05]\n",
            "  [ 2.16516480e-05  6.82333205e-05 -9.54484381e-05 ...  1.41387444e-05\n",
            "   -8.58525338e-06  5.40155452e-07]\n",
            "  [-2.56737787e-05  4.98981774e-05  7.19700474e-07 ... -1.33803312e-05\n",
            "    2.95588304e-05  2.89133261e-05]]\n",
            "\n",
            " [[-5.14377654e-05 -6.16472308e-05  6.11201301e-05 ... -1.92632573e-05\n",
            "   -2.55636289e-05  1.77441759e-05]\n",
            "  [-2.30834214e-05  1.51387025e-04 -1.55643956e-05 ...  6.35099970e-05\n",
            "   -3.90561554e-05  4.05391073e-05]\n",
            "  [-1.30392471e-05 -1.69239584e-04  1.12582892e-04 ...  1.90972316e-05\n",
            "   -9.73503105e-05 -7.48725142e-05]\n",
            "  ...\n",
            "  [-2.86480587e-06 -2.56628729e-04  7.08616432e-05 ...  5.71638951e-05\n",
            "   -1.92286796e-05 -1.31057017e-05]\n",
            "  [ 1.09683874e-05  1.27680888e-04  1.09681790e-05 ... -5.79143467e-06\n",
            "   -6.50923233e-06  1.78550690e-07]\n",
            "  [-1.25579187e-05 -2.12698127e-05 -7.82744121e-05 ...  7.53399509e-06\n",
            "    2.14358838e-05  6.01196080e-06]]]\n",
            "hello 0\n",
            "hello 1\n",
            "hello 2\n",
            "ScoreCAM completed\n",
            "tensor([0.9574], grad_fn=<ExpBackward>)\n",
            "tensor([0.6533], grad_fn=<ExpBackward>)\n",
            "tensor([0.3042], grad_fn=<SubBackward0>)\n",
            "tensor([0.3177], grad_fn=<DivBackward0>)\n",
            "tensor([0.4266], grad_fn=<ExpBackward>)\n",
            "tensor([0.5308], grad_fn=<SubBackward0>)\n",
            "tensor([0.5544], grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAABzCAYAAAB0IYW8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOy925IjR5au963lHgAy68Bjk91kNzm9e2Zv2TaZ6Tn0InoCXehSZnoPvYCudK2nkEkzs6cP7CPZzWNVsaoyExHua+liuUcEkEiQXU1Ws2XlZjAgASQQCI/f/38dXdydV+PVeDX+cYb+vQ/g1Xg1Xo2/brwC7avxavyDjVegfTVejX+w8Qq0r8ar8Q82XoH21Xg1/sFGPvfi//iL//lu17IIvh3wIeGbjA0J3yiWlbpRPAt1I1gWbGB1DzbE47oBz2CDYxuw7NgAvjHIjgyGbio5V4ahMqRKNaVUpVbFqmKmWBG8KlSBKkgVqCBVkApSBADPjqd+79Bukh1JhiZHk5FzJSVjSJWkzn7KjFNmGjN1VNgnZBS03dIo6B7SCDpC2jtpD2l00uTo5OjopNHQ0dCpIvuK7icYJ2Q/4eMI+z1+s8f2ezjj1f+/7P+QF5zvk+N//Z/+d+eurxPwIeNJIScsK2TBkmJZ8CRYBk+CJ7D5Hjy3x5l273gGT44loM+DVn710b/z+ed/RtRIWnFXkIxIxk2YilMnR8lkzYgp4gLmXGwuePvNt/npj3+KCJDAFVw9aCk5tMeSDFFQNVIyVJ2khooz1UQpSi0JKwqTQgEtgkyCFkEn0AJSiHmdQIuj1dvzHn8XQ4ohkyFThVKQqeJlgqng04RP093zLPC//Z//y8l5PgvaV+PV+D5Gv0xLLTx58oTtRaYUAyRA14ZVo9QRIbEZdsiQGW8K+5s9QmKbt7iDAc+fPcfdERFiBfpO17Uf1HgF2lfj5Q8Hd2ecRr788gsef/0IT5U8DME8VihlwjzjKKVUynSNSkZ1wBHKVFBXcsrs93v2z655+s5THj54cGD0rSXE/19g/Aq0r8bfYThfP/2aTz79mPuv3ePR14+4ublBKGiqDAqIUKYJ84SbICQMsFpxA1zY70d0K1itiMFHH33Ew4cPefjmQ+4/vM9wsTkC6v8/EolegfbVeHnDwcwwjP3Nns8+/4yvngykrZCHjJuRE+CGiJBzRmSgVKcWJ2lCRbHi1GqIwL3Le2DOdDPx7Nkznj57xqdffsrF/Utee/Mhr7/1Bvcf3iOpHBzIPzJ8X4H21XgJIyBibvzud79n8ont/R2qif24x0tFspGzIxmGYSC7cLM3plIRNACMztJaRbh/7z7DkKljISWFYaCUwjiO7B9PPLt+xpePv+Lh6w/50btv8fC1+2gC+YeG7CvQvhrf+/DZmBTg+vqaz776HNkmyMJms8G1ooOz3Qr4yGajXOzuMU7GF18+YZric9wKVkAQLu/d497lBbUUrDNzSiRVBs2YCq5OmQqffPwJf/nsL/z4vXf42Yfvc3Gx+YcG7lnQejoTxhWJm2q7F1wiBICEiz/c7uAqJx8jq/e0x2ibZA03vQCqjoqT1GOVbffuDu6IAu5h67TPEYvjiA/ox7x8Lwf38Rki8dkikMRRoX2vIeLt1kMJ0o43/naV5Ten5WYmiPWQR4SjvAqSBM+K1ATZEMt4rZAKkod4/JKGq9xt7kk7idLPZ8wzR+cx5k/meTx47M7V/gbdZN54600++fIv1HEKt281JHtciG7stsL9y0sQIZNISZmmYFsENEOWzLDJFK+Ig6YUh5cCioZxPY3c3FxDcra7LWOd+OOf/sTz66f8/D/9jDfffNh+XrvGxFExRHS5bsTb7wVZz/PRtYuCu6yui3gfIogGJsQ0wouecDOoiqSEVzs637ce3BrnmXa7ufMlF4kY7TZid7ZJLf6qWIvP1jvis/1vH1qMtt374Hh22Bg6VFI28lDZDoXtUMhq5CSUmhglNYA5JoopmAiuEVvzdkG5CCJxRfo6NjjHaHt8tpKSk3Nt32NsciG1/62mMTEuVO/hNaUCMe3LBHdAe4qLzDKkBCkrngTNQhoUzTFxmhPkhOSE5IzvR3iJoKV7bU8MF4Gk+KB4ijh8LEDaYq6C9fhsJuYhS3stbtfTnv/73/8f0m4gX24Y7m/CrZRsPv8iE1kr9y62YHvS4JTiqBaGwTCLCTVXEMe0MlXHquPFqVPFJsPMqBiViiTYbbfs7m+R0bjZjzz5+gt+9ctn/OyDn/DT995lkxNDqnST19xoXBCxYmgLQTA8LZwkbZ5poJUaAFb1OF/JURXIEeNFFVLcJDXATgUxO3Xaz46zoLXdmZdF5oSKOugBWG0Q6iANqA2s8+NVQsUMXsdaQgWDoYORcmWzqWxyAHaXC9tUmCwxqZHUmKqimiglUauDKK4BYqrg6LxaQiRVzMH2bEhyNBuqRs42J1TkZGxSJHNkjZNqQ6zicW3nAKsAolhnIPVIMlDQxqxpdDS38zI5KTs6CDYqKQdwfVB0SDBkJAVwzyVXfNfDh3OJcXI7oSIHUCOxoj+OZArTNr8ZUMeSx0W6S3z59Cv8mjmhRcURq4jWAOzlwDYXbLrB6hZIbFLBa6WIY5YQFAduxueU0fHqeJFQWSX4yZOjSZEM+/Ga8fE1qKFiqMI4PuePv/8N18++5Bc//ylvvH5Jbiw9pBrzTADVmkzzxr6GhDIrMrOtqaDF2wImkWjRz08RNCmaDCmCTAopIRrAvXueX5BpbftNoFVsowHSjc5seguwwzKRnVVnsDaGPZUBtcmFXQPsRZ7YpYnRMqMmkmaSJpI6kziTBvPWJm+6PKO0e+IiOpcBldXYNKbd5sKglU2q7TIJCbT8/oStZaHqgSz2xjyeBR1Bs8cFPgPZ47XGuj6kYN6syP5lgzbFLzz1lRIgDaBKy4JqCupE9tOBaZBhtEJR4ycf/oRHv3yMY5GVlCzUR6ooExcb4XLjbFKh2g3jNKJ5xyZXzAyxShXHLGNWQoiIIiqQ2u/I8RM0CWmjIIaLo9qsOHU2gzJkJWvl68ef88v/9oj333uHD3/6LkO+iM9xQTJMLUfDmjT3dqM2uZxAJ0ESkAQv8R1epS3ejvRrQeO4JDk61ZDM04upqW8AbbrzNRcJwGbBNg2oKzlcV1L4GLCdaWc5PBgy2CyHh6GwzZVtLtwbRi7yxGUe2abCvlZGTSFhNbOXWLFVjUkTIinkkyRMDJeQy8AshyV5pLClYNecjJwqm1wZ1oDVyibFPxuCrUArEh9b1QOwsqTwBQM5OgnaJLJOQhqD7S0JKTcmboDQFI9lSOiQkPryQGudaU98pcsCWG/HfJCu2Jm2y2H1JW0xOb/7/e/5/NHn5N2GvEtIit+qKqRUEK8M6tzbwkZu2EiBNEF1rFY2mvAc6YFCwsQpIZfaoqkHARzVRMoKWhF1ctYwowh/RFbj9Yf3UJ8o04SXa/7y8e8Zn33Gu+99yP3X32FIAAkfnNquHdNu14cpJhrpsa4efgoFSQFMCnh1TIVU+nnpktljkSmhsjie59lp94JMW8+ClpDDQ7dfV/bqDOAjsA6r/OLhthwOdq2zHN41sF7miXt5z1YLG82MlgO0YiSxmXV19pU4pUvXduHhstivGgvEWg7nVNk2SbxNhY0WdqmQtaLNrlWapBMniTO2hcJSwpLjk86sJFMDcAYfFxbSLGgGHwPM3T5MuUnmZLEKv0Tnpg16pyPKpcneHIriEKQyA9R1yTGec7wTXL52wdVn11h5hmZHvCIYSRzxkU2q7AZno072iWxT2JgZxlrRFIay5kQxKBZzXEWoQgC3eY5c496kz5mFs5KY86zOkIWLjZI1YUPGpoL4yM2zG377q+c8eOMrfvzeBwwXr4HkMFnVKJJwVawQPou2QEkVpACJZrvG75YSC/ba3p0X9CIB3jMOQLkbs98E2jP/KZ1N1yx7xtm0Zteh2ZQNsDlXcrYD+/ViBuzI/TxyLwVoJ09c1yEARHj8tLFtB6xK8yaKUwkHFRDS5IT9OqSQwRsNdt9oYaOVixTOkbQCrbTv2mvYwqrhMKmq1KRYCYeND4JPgo9xkWteAKvtgteZdSVW4Rx2kQ7ykpn2nPc45K+t2VYXOTwDtYF1tmmTM/rI/Tfu8+P33+bTz/88O/u2gzMkQ7yyzZWLbGQmso8MPrF1IycluaLmkDOpwhhBCkSgulBNqBaefxPAHLwVBCRhGJTNRtlulM0gJClgI1auyDslbxXyAGWilkJFePb4M3799VPeevc93nz7JwzDDjWNa6mmcHymVqCiYUt7cigNsCUKVVTBayx0Unp0YSWZG/Dl2A/1LXItz4N2cw60zTs83JbDdzmbQg4bDI4OFU1+Sw5fDNMM2Htp5F7ecz/FbacTNzaQCKBmrWQdmlRewNvZUKQrhbj/NvbrLk0zYLcaTDuI3VokkmZSC0NNapSs4RArUYHkDbw1CzI2T+uqGqazbuoXe2qPk4SdZC8RtLl5Re+waTtoT8rhXr0z27KOZWfyiX/9b//Ks+eP0QTD4OTs5FTJFDITQ5rYJWOnlY3s2enIhRS2UplkCIdNC8UkSSTLjOZIjcW4ugTzqoYFVHUOCapW3B2rgpLZDAPbQVs12A02Qd4mdlvQTWbcj+zHkU3KMBiPPv8DX335GW+8/WNee+Nd8nCBisfCXBXTQ/BaIuylFBVAroKW7lFugO6SeaIxst6tqF6Yac+Cdg3UQ3adnU3DHc6mwSLH9IwcvkjTDNgH6Yb76YadTGx1IIkxWGWQYMHUwToDy+cwT7vucCcYVu+2X3d5YqOFrVYu0shWC1st7CXPYI2FwthoZdDKZIl9yow1UVJlyolalVIUGxJ11HYhrxxVJ8CrK+dNSkTZ2UsaltuEngQt1O5kWgN2lsOhomhgjXBaZB1JKlTfIxhDcoamYLIWMiO7bGxTZasjWyYudeRCJzZSGKlI2pDFyX1uG9MmFUY3DCfZ4miqVTCHrKAiAfZUsVIo+2s2ZDYZ8Im63zOZstsJmwHuXSaeirMv12w2G+5f7riZnMef/ZEvPv+UN99+l7fefo+UtlhXVSaxQFsoLFSwKmHvhtl9WzK3EJGV5hw7PuffJ9N6l0GDUDffQg4fOZtyPu0dXsvhDthLjfudTNx4MO0gAVoVJ0udAQscAXZ5vI6/DmoH9usmBbtutMwsu9OJrZT5uwapXNuGQYx9c1KNNc/gnWpiMqXURKnKWBJTStQcdq+nZvOmRTKnZhd2lk3JWzD+myfvuxqWz+QH9USZdpyn5XADawOvZGOjws//03v88j8eQ7NTN8nY5mDaxMhltnaORy61sNWJC50YmBgwkjuJ8F2oGKkaSXpprFLF0KpoclJVSgFESWKoClkFFSFJRT2A62OT4lSyOtO1kSZnu0u8thu4mhxhT85bLrcD97aJp9cTjz/7A8+efMV7P/2Qe/dfx1KimmJJKaYUjcc+aTid9FAyawkw0xJPtCVgfOdMa3fnVrSVVqJ4/YhhPYNtbsvhNCzsOuQ6g/UiT+zydCCHL9PI/XTDA73hQbrmnu5vgVabbO1yeXYYtSt+DVh3OYi/nrJfO4AvdVxAqxOTpwCtVrIZG81sLTeWTewsmHa0TDGdATzWxHUamLIxJadmxVOiJkFaooUnJ43hfZzlceYly+O7X3NZwld2xLCz0ykvySqawytfyhW7jfPO2/d5+viaTfZQJzKR/YaNTlymwlYmdhqgvdCRnU4MFEavJA9Ho5ohNBPFgp0SiSqKIi2JrvtblVavT27AzUIkMVhh8IkhObtsXCRDqWgZKdeODlsuhy0me5RrVKK44cFG2STlenzGp3/8D3783oe88daPMBLFlGrKXjO1OkXBioOGl5nKnCWlpYV+OnATYYefGC/uiPoGpr1lv667TwwBVjlIligzu25SnW3XXSozWC/SNMvhB3rNPR15oNc81Bt2UnjuAwlv0thIeExsXQCrsj15zJ1dB63sUmGTyoH9utWQ5R2slzoySMVcuJGBwQtbKey1AdYyk8d9SYnJdQbu2IC8SZXraeAmGWPKlFkqe6zMPYbXnVMpwMvLlMdnHFHhPT6yX9fdJ1KYPR2wkoycCp98/Ef+/PFvSVLbwhjneisjG/axODbABlhjsbzUwobKtVfUneyOsvgUIntCyAhT5A9GSqsYJgkQEpCowdIBLfCJLCODGNsEl4OxTc4gTjKDOmHjiPnIsL1k8ERSwWwPnsgpI2o8u/qaz/54xev3E/fuP6BYolqEIPclh6dZU4QCk0IRTFs6Y0SiGtMSNvDxPP+t8vibmfYO+3WzeIfTcBh77Y6mbS6zs6nbr5c6zoC9p/sZrJe654FM7MQYPICqtFUYm5kVaFWXJ47XZQXYqXmLywqwcbtsq/1WJ+7pnkEq1YXBK5MnJsnsPDFpir89BbO2x8UCvMXCyz1nVqWBfarsc2ZKAfJgq+ZtHg8zql6mPPbM3aDVu+3XkMPWAOukVCN0p8ZP3nnA9ZPMdHPNRqbGqMZGRzYetusujeykg3WMhVIqG4FkMc/XOKKGuINaJD6YMDV2NTLVK8US0lNN1TGvoAWXClKbPW1kCQbXqaAelUUXObKyxlKpJRRcysLWBXJFxhuqJfbjDZtaMb/m6Wcf8c7D/4ynzOiZXMJfsi85HJMpUaZE1eZlVo085RJptT2H+XvwHp95UQ4Be2y/6uZ27vAuFy6H8YBdL9K0sl1HHqRrLnU/g/Wh7HmgE5cCO1EGryj7OLF2CNbwMu5OX3wNtN07vE1ltmG7FO4Xzk4mLjW81TuZMJTRE5NnqguTB8NWdAar+fK4tsd7y1xME7u0ZZu2XKUNV8lWrJupyeMiW7HuyVDA9zhmeXyHI8pmD/ex/drCduqk3EJoORbGzYPMP73/Oh///qtwNKXKTiZ23MRiqXs2OnFPJ3ZSuNCJC6nsRNiKomqIlXDW1GByr23BVuPGDMyoDFQymURlwIHclNhAJUvE3zfJ2SQnew2bWQuDFYYa+eU7NXYD7EullIreFDQVtpKY6g1WnK0bQ4IqEzdf/Ynp6we88fbbFM9spTKkzKAD+5rZl/BiT5NimkIq68K6UoiU2rtbRN05zjPtcOZFXTmcZvt1lezf7NcO2Mt1KGdm2LHZr3sedJCmGy6lg3bigRTuqXBfBraSGXwkUUg+hjSeWdepLaG8Jp0fm0vcIwtgNfKYL3TxEB+CduSejux0ZCcT1ZWdBHANpbpSkRnEAeqMucxANldubJi/a9M81EOqDGngWo19MiY9zbovFbR359CEI6o7nIbFO0zuyf4R986pMqTwxiff86vf/CvPHn/KvTSxbU6+rUxsfWRDSOKtTFzKFN56MXYiXEhiIxmloFJJFEQ9bFI11In4LBF/rxJAqAamiiPNs2/zbZuMrTqbFHJ52zzSA8aGAG+mgiRyVqZa8Vrw5yMpZzbVkGpsBEpKmDn4nsef/Ac/e2sLecdzqeQaTsostshlSZTqFDnNureEoRzcnRzfbT3tmTQeu8NGs3Z4tSVmV488z9rSBk2gumNiFCqVJUZXiVsA6fTn95Ir/DApXo+WOP0WKElizD3DXEEKCaW2hcMkAN3tYBULyazKlBLFNW6m1Nzuk6Gm1OSR0tYcPD/o5rZn5tkdVJXXXnuN8eoRWipQqdYURTvzUa3T3EtumIDhGE5l+bu60JIQ8XZNRBLN4dWtCuq9AqffCG+zLI6dpMyvKRKJON5iuha5wCpKdaOWkVpKOL4cRDTmWSOpo1w9pl4/4fK1RPE4troijaoW+dquiBrqscD0zpDtFByd23Yez5z+s6CVc/nM1n68rise4t5EkbDB58ykAM8we3jjI5Z83mDFcDnMQFSNVVUnqhU2Urlx56klnnvmqe14ahc8ty1XtuXKNtzYwN4i1bG4MlqieAoQmcZxuJJdUU+oO+rO3gaSOupGInPjTvIFyLWBfs205m1yXtBp1IqD/u7jm9arubyxSvu7Px85172KMOY6ITg/+vHPMHf+/PtfUcTwtEVMqO5U2WE64tZyeUUxn+hFdQPG5PDclZHMlW+4YcNIZpINIxuqbNo7B1wSJhmXHMAmCKzi1JYVVzBwj9arakhwOUKO3+bgONUC0C66lGEiuCiiCXHHzdFONt+yhPK7nOezoNUzx9MLg6XEShbnX5ZyNY1SuchMynNe8Bq01YWa2o9PCxhqA/DYHD4VYZLCIMaNZ65s4GvbceVbntuWp3XHlW3Z28CVbdhbDruihie3mDbmS6g5SmLsSRkNtMmNGxvCLsbmvzvjddB2sMbjBbB2RI0qHnHGOc3yDDJeZqLxiaF2fmWXQAC9ODyaDQjUliIaXQgQUQTHTfjT7z/mi0//TPIBEwlQamajcS6rEbm8puxImCrmIyaVrDCi7Elc1YGRDZNs2TNQGDAZmNhQPVFIVHIYSg1owcgFa+LFgGIe1yxEHqbGcbtEpewg2tqxSgOwtusXQFHVYHdJiBvVoi3ObrdZEnoAOTvP38l0vTjT9oYGiGAS/WalsJQwTVFnWtt790cXpjU5sQaAuTT2DbtwbUNOzZN74wNXFmB9bpvGsMGyV7UB1nILv0TsdGpGW2mAVXHUUgTtW5x3Ikr79g2o2kINWEjn9XH2cQzY6uc1bZ/cH9yw89eTVuY6Um8F4N6K/qlhwkBq5XKOS2Zz8TqSH3F9NZFVqWmgpwdb857ikaBQSAE0ESoT2YyJgb0PXHti7wNFBia2XJsyeaRnFDLmoaSK53mOXBxrVUCVmHc01l8TwaVSRRApqAQj95m0Jq99BmwLpXr77Y2N3Z2UEjl3CC1nUFaJPt/HOM+05e7Xuh6PlK2QySptAiVWKm8nDZjbevSEhwW0cSuWolpoJT9ri9CNmphIc3JFl8PPG1j3NnBVN1zZhus6cF2HFi9NM9NCgKaIzzJ538CsYgeMq24zeGEMd+5qHLMqHAI2YVR0Tv44Buq3Au5LxLbWM1/XFmERopVpl8kSubQNCs2fErnCVeH1N99hv5/4/W+fNaBXrDq7lixvFAyharNvXRtrJjKFyTOjN+DWxKRD5FF54moSikecNli7mVGxegR8VBGL1Me9C7USITVxSC0VI6Wo4hFFmQKyKmw2W/KQEM0BU3eqVcpU2E8VN8PcuX7+lKdPn/LGWxdzMcnBqXuRBfpb/MvfaNMGaFtFVEjkuZdQ2EIuGjWQ03Lhh4Q5ZNvIeZFYNbWFTzQzpcTomcnzAWi7FL5ZSeIO1nHOUErzpAIU8TaRi0xO4uxbzCMkraHmB/bti4zO3vPf3zChcnd+w/c/zjCts6gqrcyL8mLfdsaNx7UkPMGjR1/zm4/+iDDgJhRzdmlArbfsCcAVEtULRqJqxshkn9hbYvSBysCemM9JBvZVGU2YKozmFDcmDyVUghIxDXOkaijA1BaTyVP0FvAC6lQzXC0ksgqxnwjYVGGsuI8QVh45J1LKDINSSolFXpW54LaNl+E/PA/aM0wrygzOMHGkOZxajWNv/NYcU/He2xK5913qTqlu565joFNKcyphxD+HA7D22wzausjiqaZZxnaGPZbJijfwtuML8pgfHyds3MWUx+9LLQHkm/7v7z2+yQyybs/Svba+LMx1gbuLYkTjgd3FQx68/jaPvvoUrLLNG6ZaAaWKUNaef004GZOCaSb5htFhahlnIwPFE/uqVDJVEpMIBWVqAC5mVIvjipBLtHwpApp7TlUwbZVMEUEk/j8cUD0GrCRVBKGUETPjtdceRnO6mxvcYwF2hzwkLi4umh58efP8DY6oM6792Xu8MO1s3yKoeNgP8Um3wlHBtMG6tgJtyNnILCppwpLMrNtL8/aeubGBYsq+eYv3zX6dAVsXadyZtsvzY5msrgt4V/Ztl8prOaxiEZk5MUkVnZ9PLdwDR+9dd1n4gYD4XJ5zMO1i5xzbt4o3m7ZbhYKgbIYL3n//Q75++pRp7+xbn6eQxlCrtkymwjZVcKFKwltc9aY4Nxax8FjMw46dTCkmmKWwW61HHsKSFIQ3336d1x5csEmQWszWyg2b5KhNUEe8XEO9gTqGVHbBLEJ1bhrFBnlHubni6nrPe++9x2ZIfPbZZ9zcXCPuTOPI148fcXH/NeBwntclGN+1ffs3Mq03oHZ3f5RE0QDbCWv2KPd/XjOs2cquVUoKz3FxZWq3rRamlNj7kvO7t2HO9d3XFt6pmdEirbCYzsCdQduAepdM1qMwUPLFEZVaLW31RCLijbNH2O0kANde4x8iWPsQ4+7MHAGrPqumbt9S28JMm+fcZXLEXYvCb3/7J66elwam6M80VqO2a2NwKCKzR37CQqGmKAfcl8JYm4kTdTmzrA6GjXraaXJGc9yFzSbz4T/9gjffuE/SOic6ZPrjSNhQH/FyQ7l5xnj1hOtnX3N99azFZkdqUXJK6OaCfRn5w58+5p233+TDDz/g66+f8PXjx5SpMI775jmOGPHLmOcXD/lYy8I6AOxi12q08Jo9cJ1xu3MqQEsLPrcuBL7YOz0JoTNu8cRew7m0bzHYycJu7Qn6xdqEHrHs7KCQxqxHMvnsUFBXTGyOzRlLwoZKOJ3WwE1yWk6fCvu8kLPiOx7nQOvSva6H9m28djjPy251ikjmrbd+zDhWHj/5gpyVas7eC5ssYB6JKFoxVzzVOMcVJCdcEzXVSCusidKktORti5n2A4zulqlAKc44Vv7wp0948PBfSJpxaSkbKpgYptpaE2WG7Zbh/n2y/IjkI9N4w9Mnj/nD73/Ls+dXTDYxZEFTxq3y+ZeP2O9v+K//9b/wwU/f4/rqGffv3Z/9AX0uT83py4vTljOySYNhVZZVuIfvVJpMklhjDZpHuXmVvZV2AdYcE92+raZsGuBK0ubOV/aWW7uZBuQOak/zajyDtH3O4lVcMa3ogRd7/q1H9m1nW4zIcEJQj1pNeobVCri3zh0t3sshYJf7vz9Y+9DIIDg5upyV2qIDdf1/XQ57GI/xH/MC/ebrb/P06VMePXkUC/M0kXOK02aQrMSirXNOWeTm5oQOWxgEKzfsp0olwkNeoyVrzlvSptmoU42EiSSYO59+9iU/++l75Dcehm2qGhdnq+VjvwYAACAASURBVF8Va9dCa0wfQnDL5iLz9u6Sm7Fw9dvfgFemEr22VQIq77zzLhcXlwwqPLh3GSEg2rXCcl3p0f13OV48TmuxqvoauDCDV9ePRWZp7ACuuHnYtKnbE3GrZjPrVlMmqxRTNloZU2qJEqt0wCOw2gxYwfpzrcePmUa0QhxqC/fcYd/Oo63oxsIq6srcS2QGMOBRpP2PNiSShU6/5m3hFaAeLsKwsDAwe5Qh5nm/H/n0k8/BElOtqOicJ+zmmEQ7GDOhqke2FA5J2eoGNJG2G2y8Ypq8+RYySqaYIDqQN4p5YapTs0mdMjmPHj3j/v17yJDIPfWw9ZwyhSqOehy8rFy+gvD2u+/xl08/4/r5c0RLnJsUDrjYRjfdcqq+zPE3MW0wV7dpW2pXD0hDk5M+P56BG5SLN/DiLfzTgGYNaCXZAt5UGS3N+Z2llWEVCyuitv+r68+Y7WZpeQBR1gUJUj1p365PSXdMmcjcJcOIx5PlA+CecvV3mbzcz0YC8MOQxhCLs3CabaPDYTgXaRJ4BnEbSrSMbZ/W7pxt3vH+T37Gn//yMc+uniE5Me0NXCLdUYTqhaqKecQ+TYBJsKTkvMFE2GwzN+MVxQwXISFsthtS2gDKZms4E+4jpUyUWvjVrz/i8eNH/OIXH/DGa/cRUTCLzKawy+YFWa1V5LiQBNJmxwc//2f+9Iffsr++Ionx4PISs4lnT7+mlsIwpNXCdcSufX7nE/wSHVHnbdpuvyws2zOjDlj2FHCd5oxyMA9Q5Wgd6u3vmozqlWrClIzBlEHTDMZu/3ag+urvJZQEZiHHe5J4/CS7Zd+O828tB44pbCC3E7F2MkRRwAJY69K5jSShJL6NHP57k/M5mxZfHCyzc1Gideh6nAJu0sR7P3qPq+dXXD27waeWraSKqlA9cpEs1TbnLVtKIjtqiyKS0JzZXiSm59dMxVsr1YntNrEZMpqUYdC2OAulQimVTz/9gmfPvuZf/vlD3v/Ju8ggTNaRWhFrPZFFwBKp5XMKzhtvvs1rDx9gZSSJsRkSagVxazv4LUCtLreA+32O86CdzjBtNAlo9myc6NgPZXFMdO/x4piKyXVv+avWPmfFup7CC+hdLidrMThhUm/A5ACkzvK3+6nHfV1Z+LDbt/NInHZMaQFjZtt5dW6TNBCOqdRsiXXYZ/6IEx7Fk0D9O6FXy937tUZ70gbc6rPn12an0+q9PYbbP9fh2ZNnfPmXLxATipWImdbI463VccJscW2J+SygNWl71CKkIbHZKdPVPsrwXDCvuFc2w4CokIcNSEImpZQ95oVnV3v+2398xH4/8vMP3me7iSIAMV0uUGgGeqTe5haHzsOWzWaD0ntsD2QMaWmQyiGrrcM836eKOm/TnutT5DJX8IjKnC0zpza2gLxIf9xS4OaY3jLp3j6vtw4xt6grXYWG3GW+4L8NSOewUv9scdxna3v+jHxkhJoLx/uHmxhDlx3LR2AnUDZQIoVxBdz+vmNZvDhDnL5LoKsvhRcva3jLvT0lj13mWXIV1BaHY3Oat6qYxtgIS4qVcG93ybtvv8unX3xGNYtKn9FQHcK34a2Lg0WiUtjWkYuM0q4bBRfycMlmk7m+volCAHPcJsw0wCWJlGos0kCpsajf7I3ffPRHpmniFz//GZe7jIkzIAfAdXFSO/5M35jC5mb39J/nzGbTumumz/Pc39rnueGEPs/9SV5ons/n6J0rWBAPL5xJ2EQNvGIrT2MHcKsS0ZZ8MQPXvYGVnkE2sy7uWDLc08y6qg20xwCFWyBd7OZVrCIJtF3R0hGhdlk9pNs2QdUIuleVqJmlLnaurvohIyjabGGdq5XWY92bWedFL26uBBCSc2A0ft/jbGGKz+dTmm0jLefS62Lfypq1mvpChO2w48Offsj11Q3lySNKKdGpsIKQwBz3iklsARn7JUnLqIuPym2jKnPYDDtqEW72e6wIkznuBbfMkIdIWey5xRItTs0nplL5wx//ws3NyH/+53/iwf0d8yrVjt1aOxvTuG6SxjWVxRD1qIttfg1pIajunDxewPvWqOsm+lHM7/NWoa5+J2rPhYhePCNKlkyoMO06eHu8toG49hAQbRLahDY27Izr3p6e5TK4JSQtrGtq8488BdD+/GIzs5wUceY0hzWDp+X+rmG0GB8yy+VZJvdVoz0XiRcRu11XBN06txL73mpn2xm4UQT/MutsdXY03B792b7gurT9djtJtXk+UIrCAei+/OxLHj96DAheQSVhFVQT1SqQQCOGSi8DbKEkFUc2oPS8YWez2VGrMO5HrILVipcJtsqwyY1xh4CWRB9qswDuZ58/YpxG/vnnH/D2W6+xsndwE1yMZMxymdYSRjp9t3lWb2AVOVlAMp+KNs+lRTCaaIiLXeWM2/7uC+AbQj5n5LE0D3IlOsv1A1kzbANxnzz630i0AoAV0y6s670bofqBrSt9E9E1g7bPOABoB/H6b5FZesfGA98+tdu8RseBPlbA7aBNrXxvIs3VPdF+Rg4qgA53QGiMq9bCIR7nwSQS8V/SEDtj0xKLci8Q6QtyD+N1hu1ipsvkeZ7VeXj/AW8+fJOvHj8ioSGR95W0G6Jrh0fYzM2pjX0iFVaYIrJPzvE90bkis82CTTBNBatQzdjbiLgwbHJ4JlQYJD4nogqFYoVHj57xb/tf8U8f/JSfvv8ODL2kz+h+Ko6AK744mtSF0mqHjXVJ6XKNrBRw+zsWZ6sriby+ho/Gufl/cZtW2tYVLddglsrmB5J5CQnFHC6x37bKNM95sN1KLmsDrwXAvbUrab+o3XMA3OXx6j3970bqB9K7J3o0prXu5GqPyfGaJWHZeq+NBtxEpIb1Lhe99raz7bE8jvesJLJahD/U0eSxz676nZP5vYxzNm2fZwE0Fj6xUEvhdeUgDCTCzMZ9PLx8wM8/+JDnz57h+yhzo6kz9YS5480H4g2YcRqkxdGj2kqI68yBJIntsMUrlKlgVqkVRkbEN6RNmo9JtcvthJtSbOLp85Ff/vp3PH32nF/8/H3uX+7ICQaD6QRwVZ3YeyvmRsSXQheEg21Q21jv/STasKKRiGEqkXz/Xdu0ciZOK60zhUl3SHUdv/w9y+TmWZ7BS5fCcRF0STxrrgbaGbz9b70DrH2sQCtrYENcEAdv63Wg3Rg7wbwlYwc27gJcxRvTRq7yZFFksGbb47EOB3RbR4W2kVeUMEpboF4maGMhPf2FIj5XaXldXIcuMjPrLJPn9KLV/3uQ1ddfPqbcTM2G9djmsTiKRgsYrCm1Nr0eHmqDlnQBWVMcRHN+ZEns8oZ9jU3QzJ0yGiKFgUzaRDuZMOOCcU0jVu9V2E+Fjz/5lOfPr/iXf/6At994CEnDe6ytBtugaEVcGXCqaZgxK7Y9dc76WEcLVD2awgkL2949K3e+8sJM685sWHem1donbSWT1yzbgQy3GTK1FawznzaZmJoEa13aWf3vLWCeAHM/f95N6ZXN/K2Ae2uUmIhWgzunPDYHxeSJg04XrQKlj3m/oWavJTWSRBMwae01Q1m8PKNW3O80rWiLcUhE4kTaCZncwXp02C5hnf7o7Xcwg9/+4Xcdkfhk6JBRi7I51RymiEVNrLnNAK7iDIM0GR1GtYqQZRNzWiZqNcyiAWA/GM0KmlFqdPknnIkmEXoqdeLR46f8v//6Sz784Cd88P677DYJqLFxFspgEPtshQc4EXXfnW0h4vRrtl1a0S3+i+5MNYs8AT+z1eU5Bv4G7/EZphWgdNuMhUnXMrm2eK34zMjdy+5rRm0bTnn3oDZZLEqzZ+V257ojUB7R6K3nRZhP6gHGvWXIpGZ0f1tbN3FnSZ951IzeNVRsdlBIc1DETm9hAtQeBnpZ42xor527KguryJFM5nCx7vYtEKezwkXesRt2sQ2kC7VWjCiBS7aSlz3EZKE4LCK3+FgxEkk1Ft3W2ia5ILKBBJMVrBomTpWKAJmE5gAqZETCKywqoW5q9JO6vpn49W/+yLOnz/nFf/oZD++3XSrUQj22FEhVb9trNnl8dp59dqbFeVt8GaJEctGdfqgXtGm13B0L8CY5Apzt8QmZvMRvV+1oYAEsjQEbm4qCef9RBHiNBug7gDof0Oq19evzdzXjtjPu6uWWaMhdwFXxsyV9uXmLDTkI9RyHAg5DPj5vl1ktdqc3i53S72S+72GI+exoOR4u4M3mjok8LZODbo/s2/kL4i6jPLi4x9fPn6HulLEiOZjLWt8lodnE0BqMhXxyM0wqKafwJJvN05tQNroBEYqUYFyMbt0ldFFrLX0yasHb72lZc1Mt/PmzL7i6ueK/+5d/4u03HkSBiTFnTiXCuZhoHVeaGXRqnmV16yE+E2KTNQ+T6NZpPz5pJ8Y3MO3doBWJlEMpi+dTLPwzs9usM2xd5FSP8YkybznRWbfn3qtLA/EReA+Y9sTvPHr+1uOWwLFccu25lt9zDrjHdspxg7hiaWbbIVXq0Uk/7vt8AFy12DfGY9e8iCW/PHl8dp6bApodiF0mN1T2eUVkLtfs8+zdt9CY5e0330JV+bd/+7fY5V0EnxxNilr0OhaPvXi8hVgMwz3i9dXb3jypmRze3gdkUUi55RWDF8exKLz3RBoSkqUbl/3HoTK09FkAwZh48vVz/vXff81/+cUH/OTdNyPu60ryWFw72ya1W2A9nueuNGeJ7OCiTZH5kXxc/d8Zp8YLh3wiThsNujFHii+2q9G8x6vyvJV9K92mXN1mtl2B1/tmvF02n/p9d4GUQ9Z16b9W4o2zVHYosTJEhBWOgavilN65QZxJOmA1AOt6wLadaaNofonXatuycQbs2q5VbReExWZXLxG00jzAJ68TAa1h94seyWRtC3dLX+y55WsfRnRtbGJHhPFqHw3hKiRVbF/Z7DbYssUchoX3WdoC1sqQzAwTI+XYLc+awy4kqJBRkAwa7WeshriuRPVPMkXaXpnRxEFnh6pq9GuO8GHh6nrk33/5W3Dj/R+/RZLovrlm2+417gkXfaEWCZu1d0kJUXrItqrBtnKGUe8a55n2jDzuTCmNWUU8oiBr73HtDqlD+3aOqR4AlFjFe/B59RraPJUr8jsp+Y+fW7OxNj5NMAOXFXBjCT4J3HXa4dTlTmPa4nbAtr2P1eBzacKtw+yAXQfeO9uGnf1y5TH1XEqUE50NDa/hfLklk6WdvyP71rtE7gu1wDtvvc0Xn3/BF19+iRVrLO6RHunSKuAkyF+A2nbF2xjighHJGJp6NpXPa7CiDBI1taGioFZr76mY1bBlsyIpgCse0kE1N9KLYgZzuBkL//Hr35Kz8N67b4Rd6zHnSZoZ1FJW/dQ8szikVMID3tkW7Uk+dxq1d07JN3iPz0zmKoreZTKiSOkVNNAL4XvBQPcod5dggPI2eDtA53JV4bYj6tbx3PEbuve4ye7ZElplY+HEPkSNHmap3DIHpBUWRHpmdK0vEit3adVCxZWBVVtYjlaZNubyvNYGpbqSNY4qacvFTS9ZHp+J0wY4W8fftUymM+ripJg7WaxCP7MC1EDuQOYib1EXqG2LjxrzrO0aSChYbRd0LyIw3JVq0eEihYspOir2KiUhCge8l/oRx43jk0UoLXUnGlEWq6BJQ6qm1CR/QsRwq1zvC7/6ze+53G148/V7rUClxfMJkjo1z+uqH1VvHT4tYr4S59z1xDxLv3tBeXx2BdY46bR2lSbavMlRDdKzonoYqMsmOWbRFXhn4MrK5u2P7fa5+ascrF3RsQJuWQG3S7kSQA0x0LoFtkSRKspUF8dCFaVoInvU/U6WyBJF17WtOMchnziGtSMqCrTNhbyKCd+199H3Mlrr0ZPXyZwE4ItMljg3aq3+tSVZ6GqefWUOhSuhRxmEn/3kfb747Av2ZcSmimdrcrczsrQkCwDDpwC8u+EKVQqDZFJWhBTApdm37UP65tMa+D/w1JobWGxG3R2U0kLAmtrP1SgLdHOePbvh1x/9gf/hv/8X8k6plqhqVFl0WS8HXQ9paioa38d8e1NXoJHhdcc0n5v986A9p9Fadge+SBQ8vJDOij1T/xxB3MMx0eSMrK4VMZ+BSwNpl1bd+/utQHrXezrgJRYSa7ZWtLLwpQH3+nEN4BlKnUNb4YQo4iTVmW1HySTxaI9jlUnSHLedehf83nSut8yxVbNtX3Xc8HPr7Pcwzs6zt3jk4gcIH8Qyz4uX/nCeHWZpPLNu+/9NHtjf7CNeWQlbs0ni1FP8rJlS6lhpwHanTELVShYN9eS0hJQAhbnFXNFtbpDaGLfQTCTHCoGAOguDsFE1nE5Iwjzqur/86gmffvYlH/z0XVLy2LLao8+VN6U1+zN6v21k6aiyVmHtmE+ZTvOUnJmu72bXvDnNKdLFPMzDmSn7BrrLfZyh/jpt5UaW6+Kv+hXf4j0zs7dFQZoGDudxKIbIr5XuUF4SPHpMzqLSR9v9VNOBF3iSdJA4EbsY5LljZDxO3NQhmm+3vsyTaTQvq611TtWXK4+/7ZDl3mUBZZ+3vgGb63INQJtnlvdd3rvkxz/5Mc9+8xG9e/+QcoCuvUl7D2UA94i/ajig6mRMMpFUSKm12/MWFvJwVqHQGzLEWtLK/8RX4amQ5kaQjeYmLs2pEr2MU3MyMVX++OfPeeedH5G2qbG4z2xavHcBTXMX0anvJdUW5t4WyVo7pd6g4fS5fkGb9q8afZKaXHKRGaTzxK5DPHIHYFf3d45vsF9Pvb/bx8yAlKYM2uc1Xec97zcMljgek6Uw29qOCbV5lU2X8M9sr/rsJe6AjUbqS2/mfV16M081OkjWGoC1Hypo++jzs16kDwBNLNp9Plne3xMwNrstKSesTAhtcytaWivezC9F3Oiefa9RVukCtRSsZLKmKAhobGvNwI163JWDzFuJfW2v9fhvO2xF8BIlRuaxe3yS2KoyKYgJj59c8fjZNdvNfYRotytN+nbA7o+UVG/n21sEz6CtEvHhOy70F5fH3zD8YNKWx/Mk9uePWHYBrRys1OeGnDC5boH0BGgPEjGcOdS02NQsGYzrxx2wjW2t+eo720rr8jjRAuk1zQy7t2jRmcS5rrEzeGfYqW1XMtZg2bICbCmxi5zX0wnof6/Rd4tYz3M8z+l57sy7em0N6m6Z9Kot6zIY6DHqJBpglGYqNVD2XQKsGGUq5NR2tPPa1JN3nTtfD2FmdWqN12Ifov5E82JY7IPsVsNLngOsilIE9pPx1aNnvPnaa2iCyb15lFn6bXu08+3MurTylQZYDcC6BvPfMc/fG2hPftMskbpkOnVbwAqHzHqSZZcF8/bz668/8/rcNLHL4rZ6hs6hgXSxq72vEk06ews0xyoZkqw7pbpMHi2hzXnXvcTHgN3XzGTKWPIBYGsDrBUJW/qlGrUvMNYK6lgldZbt89zGmnmNVvlCV4J9RZW5DFDmf2r2agOticX2p1Mh58yQFRWleC+ZDDru/hGaD7k3bnBWphJh84Z07m6lOFCrbedHh6xR/vf10+dMFZII4eeO33AM2Kmx6lQTZsGyZjKrKKtBAreZqJ2rM7D920E7p4fBgVRaA1fkELB9UtcHefT3t5XHB0C963F/74pdZ4fXLI2X5xcGXgOWmW2lZYCZ6SyTp8a0Y+uP2yfzGLD7mig1MZa2zWNRaomJ9aJ4kXCE/YCYFmgg7WbMylF2ANyV0uqL9/E8t8cXFxdsNtuIo1ok+vcEj/BpLk7L/o9u1uKbYGbUWhn3U0jbxqwiIannaho8QNwX4XmefV6YvS/Y9BT/njTRP7eZK1q5ui5MJba5xCKhEbgF2KmmmW27HDZLsTjU8By3Ta/+6qn4Dm1ame+7PYvILZZdO6gO//+Ox33cxap3APX49eNsqy6FRZrt00Da5bCspPF8YZrgRL5pra0TgYVDKn76YQ+bbtOeAmypSikpVvMqWNEIN1WJToc/VKadQXhoGp20ZU8Jhgaaq6trxnGi1pZv3JMkwvM037dI+Qw4s8h+otmnRpk301ov4GJgc8lni8s2pSXEsa+jF301EcKzKxIZV9VAk8TCQNutrwrD3BguchGKpTOAbWZPB2xn2R5fPnWavzdHVPcOhn99mbi+8q68xLNXsYM3ztE3jp5Js/47vnT1Jr/7tfm5FbPOpa4NvL6yabuzapFPjW07qBt4TRaZ3Ds7TsenR5x9ybMkPgZsrYoVxRtYqYKUuP2gQDurodUCdkoa93nt83yHahLg4WsPGIaBUgo9U2HODnKPbond/9D/qdmsqg14reF5KZXcABq9t6V5hBvWe2qW9/XF6V1B4/uWxaL90NlrHVK2YczBLPYQWlqxylxP3SXxKcBam9/4IIkwU/0WADgxXhy0vSB9tSIcTN4MXFmeUyKv9lseqxyDcf6iw/tjsJ4EL8ydA+kMcBT+QQgZ5GfYVlrGT5PJ5867NDu327B3ArYcAlaKfNf9rV98zG12TiDvYIGO1w8W6hMjCM0ptTBOY6yJ2jKSGkDnnGVfsDq/1hh3VrrmWHHqXDUTCRhKFM/3woXukF6v4evVIMRxcwB6NJKhLwwaYDVd8sqrpXZGonywrGzYOwHbQVtp3V3uXpzPZf99R3HazrYdoHKLcQ9ux/N/h8SdEzCOX/u2YJ2fbw6mLoeaVJq9xatQzxwWWtu7rORfl4JNJvcyL7PbTAuEt/jbAHYKsGqJqqgfFNOuxqyqOGRSh1kaH4T0Dv7Z253z6NFX87lzujd3eZ+bN+9qsGr/IhEi4aMvEq2QoE1ZFK63+W7E3ERvLMimjanb93ZWniV5y2zydvG56AxcR1AdItnGpenvkG3lrwFsM4FONon/FoT2t4G256mtbJo55qrH9uxhYkWc8XYny+P5mP8Kdj0H1vlxGCqHHuT+vWtZLO2A2vulsevMxKvfavWQTjLcYt6xhXVqS5q4BdgVu2oRpNBycV9MOn0vY5bFnVsO53ntdJydkqeYtp3vUipTKZQytQ29uo1Cp+LGfcySFmTtsF4tAO1Dm2MpSZfHSxLcnLEnUaHjLUqAEs302qpTKfP1EEpMQBVr3VnMlJS3iOTYzUADrMDiHZ6dTkeArRI+lA7Ylm99l+w8N/vnQXs2Q/+u/1nJ5LUNdBSjBQ4B1/6e2XUF5PXrwJIQcfT4JFh99dKRzXogi9fJFsds21bg+fn2UBOzfbtcIj1nNkbPcupeYq9yGrBTqzsuxHNn0r6/89ETwv8adl8xaY/bHtu6p+bZgavra7748ktSykTPYlpSRTAgvf2Nr7+MRRI3e1UUqHYQGzaArN2lBHir2mlzK9Gmpl+bUX3THV+NYecE6khlTj3Gi7Lb3QcStXWwzG3a503jVl7ik4Btcyv97+N5/t6Zto2FYYVDe/YQqAdOqH6AnenWn8cJWfwtADt3XzhiX3GOAHjMrCtZfJZtuzRuhy59JzdgtbbravUMwB57ieXAhp0BO0kDLT8cm3Y9VvN8YM/2BW61QB9MaH+fOdM08cknnzCV0nKEu/3Iat2QVnO9sODB6Cw756d3UNrSMzoKZqNbirdKreaZEg9Ie2NwIXai7xlVPSbvRCFBOKoSKgP37z+IpuVNwveDM2/pp3d4iQ8AW2NHjrO7Up6Zhr8ZtAd2zfEqO09i70SxvD7bl7JgN2TPmR9yCrBH7HqXbO57ks4Li6/eM4eBjth25V2Wbvc2Gzd+d7iYO3C793NuQAfNhg0bx+sqrLOWxEeA1cLLZdpvMU7Nc/97frzyZXSn3Sw33fnyiy/49NPPePL4SbxdlVpLxGgbCx6qr3WJeKf1DuSQwyIa6YcqbYOw9jzgOb67dwN1Z7Zb+1y7t/iwNbt6df3JakVSUXa7Sy4vH8TWqRjrvYvChm1tc9aAPbJh14DtjHtynEHti4O2r7jt8aEkvh2fvRWvbcCY0xPXrGtrKbQ6/rtk87lx4r3zguF3PHfr1md89XhdbEDbUc5YNfoKIM8xuipz4kRfdWfAdqC2e53+yt/4vQ65/bCT39qXcRyf7YuzgFXn+vqaj373O/b7kSTRfbFME9bLP3V9CbSJmGUpNPQtc9YWe/Oo/qEGaB1FalT5zEkqcnTgzf7t32VeKa0pXOjupe1v5JULSTOvv/5GbPJlETtWZe7UEz2uNFi6J04Ys5d46Vi6Amzh9jzLwd3JcR606Q6/PeCq0VsnK54lHifBUzjVrHVQtPacpzaZaQXaHjddAUacAwndX1sz6+nn5NZz8f8+g92TYP1Yjo8pgSW/dZyuREvTeeMkn5lE1oqin5eVV22JA6505NGC0LtX9mOfV9+XCdp5f5fbL7lKFI6rclCtlYhQyNwSSObzFumgce5qNT7+88dcX90wWUGHkFy9RtpWxm4oLWfe6XA+1WvbhVabK/P/hCjyw/celBIG6/mqa4l7LCYFo3qhWMWooQ56t8akpCGRs7K92PHGm28wb2tzij0OWJzDefZlnqHNs9/+iPnvF2Vaz3eDFtXYYydpA6ZgWQ5Aaplbjy37oX3JWsbKgaS923Y9fX/ytX6ihHlBOQarJz8Eagdv8tgYTNvV0Xa2W8C7tJCVdqDHrS9nSXbw5NHicuL2Um3ac90fe3ZbalVbaQFuz26zvritlJQlmMrIkydP+OTzvzCNIzqEgSgSO+fVFMkRXWot5lGEe7Qxa39+3vrGl/cuQF/mgJZmeRCyoxUFNF+FaXx39SgAdLVmBguSBM3RlkaTkofMw4f3uXd5iRxM1DJOzjOcvJ7PzrOs3nPHOA/a463l1mPNsllnlvW0ZlfBO3B1AS7rtcBXx7f61XfFbrubfAEldwO2TVrPqlqz58K0h+yKLoD1tAKo0nJYiUR3XfrXygq8d57LA1AuLLH+vevby2RaV737+3TFsjNwO1gDuCRZnVuPhVGcP3zyJz799NPwDCdhshK+AasUn6iptqKBOBNdFjtxDnr6ohCOn+73mTluNc/BAwdknwAAGzJJREFUzMviepi27A1ULfYrANY2FOsLTrxbkpCyIElJWUmDkjeJN954nZTkSFndcdLW88zpee6PT/7vN4zz8vgM0wZIdQZoZ1lLguWF0WzNbiv5+aIjQOongTnLzAbkY5m5zsqyBsj1xTazq/b7Q7AirTKlAVf6c0es+1eNc4z7skbq0uf2OGDZg5j7IpFP+i0S5N0G2WQwi3WveWhv9iNFDEkt9DKvaGvbc0klhP+vvWttbuPYsQfooSS/7ciPJPdW9tb+/x+1mxsnfsSO7Ui2ONMN7AcA3T1DcuRrO9pKFdvFIkVaItk9pw9wgAa05vyjsufiZ0d61LJSdOvqdrZFciJO634tGVCJCZSA5Ef9eFCkBAwD4+zsFPfu3/XkMF/n+Jyfu1B6zf1/MFZBK6vmMVWWlaHzW+O2fG5QZ12dFQpf2u663Jb6/xem0QyU1PkHOnutB3GEJWZgJcxMY2PZDqjexS7AaX5slN9spTHRgTVYd2dUnzushM6P3Wc2rVX9/8bDsocO7DbUmcUzRvVNMNEOWOG3H/7xPU5un+LX589xeXkJEDDJhEzFmpsB7qaG8kg1UV59Yw4zWNSEvmqZebiHBA3E7iMrtZK7BuBgf7iFBEi0YSGAkoKTqdmcLP6eEvuNcO/uHZydDM3JVlQLa3cyu6nr1rn+HP+nbjiLv/G1QtRaz1YQzczgyrI9ww5tIetiD2g+YrexAqg74s6n7icnRApxP8y79AXbRkcCite6nTlM8x6sjVX9PkzhpDMzuAlPa2BdzF8vSiynb2kqx03MzLtJnzbyhvcOwn6W7f3ZFP8P3VlaIA2Mp8/O8fD8Pt68eYvffvsN4/uxrqfOnDreNTsbqmGpjU6X6B57JiEBNakqjhHa9HsKa91s7XcSsb1lrcjo93FLhJQIJ5sBDx/eq90I7FzuYv5qaGjf5GK2zkDvAu3ZKr/WPF4VogiVZXcU2V4xHkx8ks5PNHCgA+jKz9QxHezLq5AtkJ+YiL4y6rtuVWBlkd8Z5lLHrPHczBRm9bq4fouP1dHiPrAeZNk6oZ2q2H3VpTBx0+rx6uYMNJZdhu06MSpcC/E5rHNKhM0w4OmPT/Hw8QP89uI3vHr1ElefrhCdegC0yEynJkPIkxjgm7VVitBIbPANmfyaiJOR1J06A3vd6lp4wfKWIxpASR2oUa+bkJJVU+WBcHb7DLfv3l6s8wGWrRO2u1EvWfZa7WJlSdZBu7IDt4Wijmkby+4qtM68Qywm6iSG2FOZLPzGntk46JLa2UQh33Gp7bxq4KV6mgI1gN0utgMiU9eRvQpMLDNQ2n03t93i7Xu9TdjicbdoOwJUT0A3MXhNPcZns2wwrSRYF0QG4MoRk+Jsc4L/+u9/4uH5Pfz623NcXFwg5+xCUe/zoK5jKMDtXlDdjNl626/XuLGzvZ8iANCYNhjV1tviscQtNpsSYRgYt25t8PTpdzg5GVoTLdA1gF087kx8Wq7rvnWm7ncPjGuYdg201Fh2Bs65LyvVZ1RfSG0LSg0kVdwBKlDYTdO4J7JqEZF5Yjeu4NXuvGIUpkYwMNBY1X2bPoyzD6zRyQ64ZmeN+T5kIfWm09Kv6R/Hdbu2A/8Fwzbn/W+o9Bks21swS6ul33y91eT5kwe4d/8Mv799jd/fvMH26iNEApkAIL62Nnf2WD0v2E3jMDlFEWVb6q/HZuwpqEti4JrqSKCkSJVprYjb6ekG3333COePz3H3zt1WFsdm5PA87lvn/nH7eoc35q81j2XNbOp23qoYD+Rs2vm0zq5xw2Cm5wwowAwkdrPyldY6w9tCktbawLkk6xheeBfAxZm3NCYGMGdV3yT6z8G+FS4/x2xOD/kuB17b99yOIOGP7e11Rjg3MeygyoHvFczVKcazJBVulhXC9elcjz40BgDkbT7v3B1w++4zPHlyF2/fvMab168wjiOiipsIoQggmiCCGYBV/PictAR/M5cbTVEFLrrNIzYOPz/rVpyKxWqHYcD9Bw/w008/4d7de+Bo1LVA0t513jN/tASrPzbm9ScPHBhYc1i+WIjaUYtn7BqPO//RF5KSggapICEHR0pSgcosFagbFiS2FhoMKwo9lYSi2ct6GICt0p2VgdHUHY0KBvaZ2Btj7Zid6+O2adTvXJOmUTsA7N0s9wF7sYC9+17vl7ebGmvJFbOw2NwMnrkbnRAVmyMlS1gg+MYYVlOsPQlunZzh4d1noPwe7/64QJ5GaCnIAqjYaZ2Qgou/gU1PcuPEUhcBmKkc5rgCdSIFDlIBlWa1WQkaRSIgccKtsw3+9dM/cf/+XU+Y0QMFxW2yPqdi5rXrvP9Pr45rzOOV13iuGO9NV1yKT0lByRnV7wOsgwN1SAWJFJtUMLBgw3Z/wrnWGY66S3lgL/ZdarHvMlA7t8pWbzZKkkZstTX21R0TvBYfZzH/xpm2b/sg7lMBvvvH4pGZcNEhvDVKxu79wqdd+rY3qx6vvNazbLWu9oC4T/VkrYBlt5Cq5UICZptXhmLgAt4AP/34CDq+g0wEKYpxEoyimDKjqCAXQiY7+pZBUMoocMZVhvph9tbFrrnUQBdLZ095JHIBSs1/PWWcP7yDhw/veCdMS8jgenKMKoAjtNzWOSZrcb/vuVjf7vHe8aVC1Kp5TJ/Psjo4YAcBJ0UaClISu5FiSIIhFWxYDKxk9yecceL3GxIMXLwg9OC1gweMaV6pfyoJmRk5dYfOq22me33lAGqY4tHNjqkZPYoGXPKFihBb8sUTZ+L9TEvtfvYYM+BW0/iGfVo99IYzIeozWDZi2wwwF6TkYIVbVHWDFm+oLRgo4/TRKT6+YVxdCkqeMDIwFsE4eDF3gdVeKowsaht0ssaOZmHZMbzmQ6OyZU019Q07mmIlJgwDcLIhnJwMGBIwePxWNRIW496+Q4C35oTsTOaBdQZ2zeR9bPvVQtSaeUyYKcZ7WTb82Ih7JmPYYXCTNxVski2igbRUsJ6ljNOUccIZp36LNpKfygm2MmCbWsX+MRn7jqkrBN6BN3bgKmoB1U9mX8weqEN9TStYow+Lwi6gJXgZ1sN0Nk+Hfuh22R3V+KaZNqFdWMvXKtPugvcgy4YmkeZAZVIMHWg3VLBhu23HS5zoBRJfQocJIxRXSNiAMYIwsYE3kwF3IqslzGpHH1OKbvIeuuGAG3wjBtiTN1JyVwjAwIqEDCqKq48fUKYrnJ7dMh+V+jXvwKtY7cPzH63z2u8eGF9xNK/d61I2pX2fpu10Mbjb+WIhB28BObAzLskMtBBAeF6NySbWus8NaACL/p/Wi6czi7EO2GDdvtVHD8bqz6o3f1KrHq9A7c+iGq0fqJ6vtGNaqLcaU67HtbQ7Z3mTTu1nDFdb588dXuf+6ov/wnWeBQmuYbD1zLn88A7505/QcgXWAhJGgmDgZP6rAMrm2ypgPnMBLEHB1jv7AQMOZRhU3RvyUA3DWl4WL3wOcotQMq4AvH71Gj/+458gTrWdZb0pIO5LiHTXwCJvoFpLXSiqVqsQuOLt03cgI2ptrIKW8+ELp+60OQLV5tNSgh3w9rgXhTlVCJIJxSPgkcooaowu6iGkzncQ5fqzKGHitNPQ6qoMlW3DTDam5SpQGdN25hEbe/YALkLVhyVSFKXaJaBnV1Gyznb+uLKwtJ1ehaovLVMCRrbCbROBRwJPsEPvE5BG+M9aH6fxZjOi4uLfOzg2Fr/oiSDdcyCvY8gwBVe8ej4lWFNuINfS7TaXm1QslATbVN9fXuDnX3/Hx0+KpJatNIk3sRLrOjiV5E2urKjaVMhMZDXmVdgmCYU1Nme4lWQxWvsqCoA8HmuCYyEgqYKFkRV4/utLTJLw5MlTDJtTRC931XaTKIQg/h0EXkKIgYzuvLQ9ruelC0DZmq9zMXx9c5+WysqVo/ZhLD5H0KT2QWsmioGZMtW4njJ7Ia3UJgG+WyUHhnfIFiVkzsiJIbD6OxsRM4u7RlZ9MfDq09a6TKk1tAJqSKf3a4mWAKbq2xZ/PZi09mNZANV2Wlc1+4SPQsDEFaxUgWoF3CpgR3XgGmDTpDfq05KsvF9lLjWgsZrVwHbhCXmyQaGuaoX9P/Ecw1rRQ81UBeyEUBHBmw8XePH8Z1x82AIZYCQM0NoetNAGig0wDCAkkCSQsjXgZpvv5IAdmEEpYUhk/X04LDuFaoGqQKRASoFoBlT80ICCk4Hv8irj519+w+9vL/Dw0TkePHiE09PbABhNXIzED2dWAVAMsOyFDtgf17pfBaBJDawOWM575p12HuyMdaadrmfaqCxnAEYVKShH1gmAbIukxB4yLbaDaQBWfFIJkooBVu0EUbDtxAkbLpgkrbaLjHYbJkLRrKGVRryQFFKwN4lDlGyHlgbqAK1UoHKNF0rEC5eZWf4zjeSsSqAVwKYRSJMBlke9UfOYViyqAGCkhHKBZT1ZWSW7GBmABIgBEBuAByvFo2pd3JnhwAGm8Qrv3r7Gu7evMF59QhkB0gHJTdoijAJrG6mUQMMGnE6x2WwwqFlSXAipACUBpQCSFcwJw5DAcYxOFYqCUjKsiJyXmFEjDoFZVsmtOVKARZHzR1xcTHjx4g88ePAI54+f4uzkln/5JiDW1h4VpOQVSA4ANjfAWi2wld3ywPhipo2G0X2KGzvrcgXuHjM5EwQJJAIMZLud2i5c3PTUoVTTs7D198ycMXCahXyW3eeydIqx32uh2tBKXZCocVpNxiDR6TwEiy5WS8B+sC5TKfvq8QpLo8xzU7jdAzwuTOJJkUZ1EMu6lfONB3kS/r7rhNxqCv+1MHms09wJoQNmcrH4qbKCEiO64X38tMUfb1/jz/dvUfInSB5tAygJhA0SrGK/tYb05swqKNvsH0agSG712NyXYmEiLQToBESGE4eSbCVlDKwtPY6gAFlusuVqJST1ZB0CmArGcYuPl6/w5vUHPH3yDE/On4JpcDHJwRt1nypQ+8qauyZxAJazfPtqjOtMq1bRfaEs9sA9ZCYDgKpVldfUQimqnjKnhJLEQMxcfdpBZbUZcxRRq+zqoEVUcmeq5USUIn2SKphVLNFCJBnrSou5WqW9BVijkVLUIgo/rwpOe8DamcFLk5hHRRoFPMrKDvztB5coIbpn1CSU3iQ2s1PpGjMZqEdlMyk+/PkeL188x3Z76axX/HUGSbIGzgQwCSZJnTvihb9VUbRU8U983rWYIFjXgV2ejTTGGofWTkyLCo3u2wLebM9bbMIlLzXzfvr0Ef/+8xfkK8UPz35Agp31iyOg1PmvPcNWgC4Am7KAsh5e55WD2deA9nAuneUYK3jSGduGeXzQTPZSIHB2VWVP3QsG84SNUGb9cU6MgWUej93DrpEJpYVq5cOaKiY+GQHcLv84gNuzb2TxHEqRjIuECnWtHlBBy15lkSZbxF506oG7BCyP5WZ92tzCI8uh7BsQ9ckpxrLXmcn2F6286dt37/D811+w3X6ESoZSRgRO7E8RMrGJgeTr6myrrieoWtF3PzPgtYXhaYzYWWeCbzDsES0KBY1AYDeN/QA8yJI1yE1Wk6sBLXY4AQzVjJe/vMQtvoXHj84raFkay84K9ZUAqM4AXAG7gq9D6wFcax6v/FGJLBmz1a3aoK6byR6UBuCpc4ZcVa83m5qzX+OiIpV1k6u6wa6te/oBdpUGqLYD+6aisbioFkCwLzFQyI6FmVtE85TI/m+HshqlMj2EAzEziR2wvARsB9w0ij3vgOWxWFPUGxokUjfS3dcaYLmKLnq9mdwIDR+vrvDrzy9wcfkRU5kQB55DM2AiMDGITAgsLg6Wyrap6geh0FtVOTQhqIKWqtgYjB9VVy08aYtvFpdRMCmgBQAYJaJYoagHaXudmmka8fr5azy69QibxLtlUZ1VyYFaATsZy3LRCljO0oL8OxN/eL2+iml5CpOXm4qcD5jJwXDZ6777ZNQjV2I9UzRpC5s464avm1gt13iNXbtb34IBAGofmrh1Iks9WytuLrMtmBIqw2JnM2iy/izeWha7bue31vBODfMYYNO2gEYBjxk0lZv1acthM03ZLki4iFiZ9pCZDBfyPN9bSPH21RtcvL/AdjuhQGoiBrNlMVFUjGDbfCPDzPxZEwCN9cINoXpeWoW6GKhf6RbvgXi6IgBE/SkmeLEFizlSmHyetqjdBkCiflbXxClRI5Dx44hyNeH0ZDOLv9LUhXLCHK5hHgMsTwJkBecCZDksOH6peUzTSgl08XhXmMlxNIt2zeQ4KUJM9XiWeqlMKFpGzgrraiJksaN5n8uuPQO65lBjylFhoQpkoQQ6iLUzmXtzeOdv5wZYrs/ZIkYNY546/3VSB62LTqMgTdIAuy2gccJhJ/PbD8qtofPOa17HidgTP7h7vDCTAbvWtMSZUxOOri63yNsCyVoTI+LcrXLr/RrlSQ2PDFVTkpjDvuWaqqgMUOQCg+IvVO0CierGHC4QKmBhFoHC368AKkZ6YqxLJQBMrcAC2cGCs+EUGx3cJ+1ZNkBqOsEMtJOCSwfYSUC5fNE6r2dE5TXzGCZEJQKHmeyLsDSTTYkFkO2LC7Awa8hS4a5hXWbt2JWaMrzsl7IwV2tFRgcmVcBqFdJm4CUg5PyIS0IsYI4a4qJacLoXIOK5unjVHEYD69KHnYqBdTLA3jRoseIGGTsRxM+bWoqiA2FhJgOYiVWA+apnwxlO0xkkA0WsHYjEOVhPhWUY2EGEzSaB0gZMG6uNXEz70M5/pQL3Z1vc2GLK7DWLCZTsMdiY1jiiQKEQstCTQiBaMJUJKmKAzS5KCsBCYDA2nLBJG9zanOH83iOc0GBrLHPAVpAWdfXYwZsFlMX82GKApWlftfIYX8y0+fCLyVaHk028TmhmMjVFmSkU2s60glc+BFqq4T7WFYuXhsJc84A/k12byeoXVKSsOUhVbEFXwSv2eWrf2E7a7+X9uWLogA2pv2fYUZv/OjXA8piBAOxoF9CNjSJuJu55jRlKlkRvG+8BM9mV2XBFomUGM+PRg0e4uLhEIsY2jxjLhKzeDgRaTdJcxFpuqCKdGAlseGMqrxjjV6eTzCrSbMCrFf2JkZiQEluVRbJYrIn7BVkIWaRadFkyihQUEWuZWexASAIjUbLibpRwQgNOhhM8/u4xnj58DC4ErlECbcJjVpAD1oQnY9gALOUC5ALKGZhKF4JajC81j1HWzGNX3rz8JFNnJocLws2Uab4taj1bS5HBYdblTmEWzyGWz2PXmX/p+Z/qsbuosgB2Bl4DbySQRBtKZ9jobrcr8cfuGuax+6+5xWDTaAsYohMFYKcMbEfo1g+D39SQFbVaPbZd3Eymzkyup2dcjCqoJ2zCv4Uq7t+6ix+ffY8XL1+CP33EkAZMklFQkLVAIZ50oZBJUfIEfJoA2YKVwEgOpIQEO6hh9aNgbFvUhSMzlasVVV1hr8gIsTAjiYURSQHPI0BiDEjVz2ZhJDBYFMgKKRPOHtzFswfn2CBVdZjDNQpTuLQYbADYWNWAi5xBpQBThk5516JaEaBifDnTMkPZU/Q8XlvN5C5uW8WeHsBAl5iwh3UDxKEwh6kczLcQg3pGnT0O4Pr1T9KU4jXwInkIygvGRZYL5RZEr2waZvCSXeNxbxL3CvEkoDEbYKcM2k7QcTSW3W6BG2Rayr6r7X3Rzqoi066ZHOIitTgtE9UkIUIcNGc8uvMA+cGIP9+/B6QgMUCUzF8lq/ZvXdzV2VfNFFZCEQNl1mxlVCM2WhPwMW9m1cXj41C+ucS+ORMj0TyXAPCj9AIgC6QISgZ0KtBJcMIDLuQD/p3/Bz8++x73b98HKztotbGs+7NVIS5iglMw7FSgeQJygU6H3aA17F7j064wrXdBP2wmdyay5yjHweNIwLHbHtZN9lpVmBkzkehadpU5YKt67H7PEryS3DSM17XbaLhtAjuA3QfWqQdu8195csHJGZZCJd66OTxNwHYLHSfI1fZmmbasgTayyA6ZydTCn8VYrTIvbHnHcYtXv7/Cy9evcHWxxVhGA5FfM5TIxNwoU8oE0gjHkMdJgXqUrdZEdoHRP7pd6HEy2D5AHLGDm8kWETC2jW4DGv+ybQ46CTQDLAArIXnijKDgMr/Hzx8+4dn5M3z/5HsTokTn10EVnMSAWsQIMBdotk1ac4aOhzutrSka66Bd2+252q5AHCp1wccmUr1inrFW33Co9oLV7hbPpfnztc1k+FwOqtnvzm6dqd09X09T+K/Gc7WGMtnfr4eb1eK02v/+nvsax+vecxYGKN3rxXOKi7rFIEAUNCvFLIrisuVNClGzLuyLETHteFHnvq8Lsfa9EtV5CVHx0/YTfv7lf/H+4gO20SVPzDSVUsxETcbY7AXuPVsRDK6syrHuM3albt7b9VeTZoBqSYm6IMXeu4dQ7xVq37OgxqIZZkGydKynCimCMU94+fI1zu9/h2FzVo/b1WtC7PqvZ/skHmtbb5FuovaMFar9Jk2lj+M4Do0//niL9+8+YJQJuRRzfwhe9oURtY9F1fKH0aWUhlUVhxXUQdylD6IEGKjWhVKghnukE0IhMDcrQOoMzMxAgpn3ws1vVwtPRnhLVJGLpVGeDIRSBNjc/JweQXscf8Fo7GGkoigRC/ZsKTNbUY/YMWkT/qgpyiTwuC88QRhxTLd7H89ysr/u8V4Ye7OFfszVCbcHLV7rhyHM0jIznNgU7Ja+6ZENsejFZhgwDIOVfa2DgL2myrcfR9Aex186ElsDq3JVIEU9WtNSHyPRggigmhChng/MLSbrAiQJjBXdzQhAVbdEQ2R0YHrGXn8eWENZBgAEwKm9rxCI/DyueHNpYgw0YIOElFL1nz9D7P3m4wja4/j2gxrnWIpiwsnJCaiwtbvUguICgAI1TzjaeTB5WKcDVM2BDh/W0zzD/yXYc1x9V7iK7JuBv4dECV0mr57hopT7tiF0sWdqJSIkZiRl87HDtfcHxu43O46gPY6/dNy+dRtDShgzORAYkfVL8B6xLggBqOouUwAk2NkaPDM1YZK4CVOJCJrFmDYRMHhP+DgLzIQSsVoStCOi/p4uTYc5nMybBYd67UKYbyW4e+cObp3duunpBHAE7XH8BaMJooo7d27j/Mk5xpcvUKaCBM8lDxaFxXgjkV8Bq/gfbUtDkFICCZuK7OZyE6TczS3u1RZASwQTmujUmBbO4FwTgEDu3hJq1UZ7ierRQaggpQ1+/OFHPH342M7U3mBkLgbpTYYWjuM4juOrB/9/f4DjOI7j+M/GEbTHcRx/s3EE7XEcx99sHEF7HMfxNxtH0B7HcfzNxhG0x3Ecf7Pxf6Yn/UceyOKlAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 288x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIbBTo6JDssf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "4f058bc9-b8f7-45dd-c64b-fb2527567b7e"
      },
      "source": [
        "avg_drop"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([0.5347], grad_fn=<DivBackward0>),\n",
              " tensor([0.6012], grad_fn=<DivBackward0>),\n",
              " tensor([0.3318], grad_fn=<DivBackward0>),\n",
              " tensor([0.6931], grad_fn=<DivBackward0>),\n",
              " tensor([0.3630], grad_fn=<DivBackward0>),\n",
              " tensor([0.4319], grad_fn=<DivBackward0>),\n",
              " tensor([0.5967], grad_fn=<DivBackward0>),\n",
              " tensor([0.4729], grad_fn=<DivBackward0>),\n",
              " tensor([0.6383], grad_fn=<DivBackward0>),\n",
              " tensor([0.3155], grad_fn=<DivBackward0>),\n",
              " tensor([0.], grad_fn=<MulBackward0>),\n",
              " tensor([0.2475], grad_fn=<DivBackward0>),\n",
              " tensor([0.0803], grad_fn=<DivBackward0>),\n",
              " tensor([0.4893], grad_fn=<DivBackward0>),\n",
              " tensor([0.6158], grad_fn=<DivBackward0>),\n",
              " tensor([0.3177], grad_fn=<DivBackward0>)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2xI4lH2exwq",
        "colab_type": "code",
        "outputId": "3df77158-ed7d-48cf-9bd4-5333fe17486b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "avg_drop"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([0.5347], grad_fn=<DivBackward0>),\n",
              " tensor([0.6012], grad_fn=<DivBackward0>),\n",
              " tensor([0.3318], grad_fn=<DivBackward0>),\n",
              " tensor([0.6931], grad_fn=<DivBackward0>),\n",
              " tensor([0.3630], grad_fn=<DivBackward0>),\n",
              " tensor([0.4319], grad_fn=<DivBackward0>),\n",
              " tensor([0.5967], grad_fn=<DivBackward0>),\n",
              " tensor([0.4729], grad_fn=<DivBackward0>),\n",
              " tensor([0.6383], grad_fn=<DivBackward0>),\n",
              " tensor([0.3155], grad_fn=<DivBackward0>),\n",
              " tensor([0.], grad_fn=<MulBackward0>),\n",
              " tensor([0.2475], grad_fn=<DivBackward0>),\n",
              " tensor([0.0803], grad_fn=<DivBackward0>),\n",
              " tensor([0.4893], grad_fn=<DivBackward0>),\n",
              " tensor([0.6158], grad_fn=<DivBackward0>),\n",
              " tensor([0.3177], grad_fn=<DivBackward0>)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6cHnuDOe0Fm",
        "colab_type": "code",
        "outputId": "7a81370c-ca1f-4455-9983-3d34d4d02c22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sum(avg_drop) / len(avg_drop)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.4206], grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTgMMy0XA4WS",
        "colab_type": "code",
        "outputId": "82dd554a-3b6a-49c0-bcd9-2435fbb3068b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "avg_drop_sc"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([0.5026], grad_fn=<DivBackward0>),\n",
              " tensor([0.1196], grad_fn=<DivBackward0>),\n",
              " tensor([0.0551], grad_fn=<DivBackward0>),\n",
              " tensor([0.6496], grad_fn=<DivBackward0>),\n",
              " tensor([0.3243], grad_fn=<DivBackward0>),\n",
              " tensor([0.2248], grad_fn=<DivBackward0>),\n",
              " tensor([0.2282], grad_fn=<DivBackward0>),\n",
              " tensor([0.3556], grad_fn=<DivBackward0>),\n",
              " tensor([0.5557], grad_fn=<DivBackward0>),\n",
              " tensor([0.1451], grad_fn=<DivBackward0>),\n",
              " tensor([0.], grad_fn=<MulBackward0>),\n",
              " tensor([0.6020], grad_fn=<DivBackward0>),\n",
              " tensor([0.5916], grad_fn=<DivBackward0>),\n",
              " tensor([0.0934], grad_fn=<DivBackward0>),\n",
              " tensor([0.1988], grad_fn=<DivBackward0>),\n",
              " tensor([0.5544], grad_fn=<DivBackward0>)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCz0PPVEA8uf",
        "colab_type": "code",
        "outputId": "ff9ee8f6-96e6-41e5-f7b3-7c630caef086",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sum(avg_drop_sc) / len(avg_drop_sc)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.3251], grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMPqaZcQtYjZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}